{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:00.864106Z",
     "start_time": "2022-02-24T22:59:00.845101Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "datadirs = ''\n",
    "sys.path.insert(1, datadirs)\n",
    "savepath = datadirs+'save/'\n",
    "datapath = datadirs+'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:02.566136Z",
     "start_time": "2022-02-24T22:59:01.090158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:02.678161Z",
     "start_time": "2022-02-24T22:59:02.567136Z"
    }
   },
   "outputs": [],
   "source": [
    "import visdom\n",
    "# python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:02.694165Z",
     "start_time": "2022-02-24T22:59:02.679163Z"
    }
   },
   "outputs": [],
   "source": [
    "from pars import PARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:03.490349Z",
     "start_time": "2022-02-24T22:59:02.696165Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "from setup_net import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "headnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.1703, time: 18.6848\n",
      "reconstruction loss = 0.0681, similarity loss: 1.0902\n",
      "Epoch 1, loss = 0.1428, time: 18.2625\n",
      "reconstruction loss = 0.0673, similarity loss: 0.8222\n",
      "Epoch 2, loss = 0.1355, time: 18.4255\n",
      "reconstruction loss = 0.0701, similarity loss: 0.7243\n",
      "Epoch 3, loss = 0.1306, time: 18.3827\n",
      "reconstruction loss = 0.0695, similarity loss: 0.6806\n",
      "Epoch 4, loss = 0.1388, time: 18.5344\n",
      "reconstruction loss = 0.0664, similarity loss: 0.7904\n",
      "Epoch 5, loss = 0.1266, time: 18.5743\n",
      "reconstruction loss = 0.0665, similarity loss: 0.6675\n",
      "Epoch 6, loss = 0.1285, time: 17.2996\n",
      "reconstruction loss = 0.0660, similarity loss: 0.6915\n",
      "Epoch 7, loss = 0.1429, time: 17.3744\n",
      "reconstruction loss = 0.0675, similarity loss: 0.8221\n",
      "Epoch 8, loss = 0.1298, time: 17.3177\n",
      "reconstruction loss = 0.0685, similarity loss: 0.6814\n",
      "Epoch 9, loss = 0.1310, time: 17.2740\n",
      "reconstruction loss = 0.0686, similarity loss: 0.6933\n",
      "Epoch 10, loss = 0.1270, time: 17.2516\n",
      "reconstruction loss = 0.0676, similarity loss: 0.6619\n",
      "Epoch 11, loss = 0.1254, time: 17.1979\n",
      "reconstruction loss = 0.0687, similarity loss: 0.6357\n",
      "Epoch 12, loss = 0.1334, time: 17.6165\n",
      "reconstruction loss = 0.0656, similarity loss: 0.7436\n",
      "Epoch 13, loss = 0.1174, time: 17.8322\n",
      "reconstruction loss = 0.0662, similarity loss: 0.5779\n",
      "Epoch 14, loss = 0.1277, time: 17.3203\n",
      "reconstruction loss = 0.0676, similarity loss: 0.6683\n",
      "Epoch 15, loss = 0.1073, time: 17.2114\n",
      "reconstruction loss = 0.0677, similarity loss: 0.4640\n",
      "Epoch 16, loss = 0.1196, time: 17.1964\n",
      "reconstruction loss = 0.0660, similarity loss: 0.6014\n",
      "Epoch 17, loss = 0.1194, time: 17.2570\n",
      "reconstruction loss = 0.0652, similarity loss: 0.6078\n",
      "Epoch 18, loss = 0.1228, time: 17.3787\n",
      "reconstruction loss = 0.0654, similarity loss: 0.6396\n",
      "Epoch 19, loss = 0.1232, time: 17.4198\n",
      "reconstruction loss = 0.0672, similarity loss: 0.6264\n",
      "Epoch 20, loss = 0.1146, time: 17.3642\n",
      "reconstruction loss = 0.0660, similarity loss: 0.5521\n",
      "Epoch 21, loss = 0.1157, time: 17.3275\n",
      "reconstruction loss = 0.0667, similarity loss: 0.5575\n",
      "Epoch 22, loss = 0.1139, time: 17.3764\n",
      "reconstruction loss = 0.0683, similarity loss: 0.5239\n",
      "Epoch 23, loss = 0.1103, time: 17.2444\n",
      "reconstruction loss = 0.0685, similarity loss: 0.4862\n",
      "Epoch 24, loss = 0.1008, time: 17.2410\n",
      "reconstruction loss = 0.0666, similarity loss: 0.4083\n",
      "Epoch 25, loss = 0.0978, time: 17.3816\n",
      "reconstruction loss = 0.0675, similarity loss: 0.3705\n",
      "Epoch 26, loss = 0.1052, time: 17.1948\n",
      "reconstruction loss = 0.0678, similarity loss: 0.4415\n",
      "Epoch 27, loss = 0.0949, time: 17.7122\n",
      "reconstruction loss = 0.0684, similarity loss: 0.3333\n",
      "Epoch 28, loss = 0.0952, time: 17.8125\n",
      "reconstruction loss = 0.0667, similarity loss: 0.3515\n",
      "Epoch 29, loss = 0.0984, time: 18.6258\n",
      "reconstruction loss = 0.0658, similarity loss: 0.3927\n",
      "Epoch 30, loss = 0.0929, time: 17.6310\n",
      "reconstruction loss = 0.0649, similarity loss: 0.3447\n",
      "Epoch 31, loss = 0.0965, time: 17.5785\n",
      "reconstruction loss = 0.0674, similarity loss: 0.3591\n",
      "Epoch 32, loss = 0.0968, time: 17.3149\n",
      "reconstruction loss = 0.0675, similarity loss: 0.3604\n",
      "Epoch 33, loss = 0.0963, time: 17.2577\n",
      "reconstruction loss = 0.0667, similarity loss: 0.3627\n",
      "Epoch 34, loss = 0.0921, time: 17.4030\n",
      "reconstruction loss = 0.0672, similarity loss: 0.3162\n",
      "Epoch 35, loss = 0.0908, time: 17.4990\n",
      "reconstruction loss = 0.0633, similarity loss: 0.3378\n",
      "Epoch 36, loss = 0.0919, time: 17.3895\n",
      "reconstruction loss = 0.0657, similarity loss: 0.3279\n",
      "Epoch 37, loss = 0.0928, time: 17.2082\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3425\n",
      "Epoch 38, loss = 0.0907, time: 17.2901\n",
      "reconstruction loss = 0.0645, similarity loss: 0.3266\n",
      "Epoch 39, loss = 0.0904, time: 17.2880\n",
      "reconstruction loss = 0.0623, similarity loss: 0.3436\n",
      "Epoch 40, loss = 0.0904, time: 17.3264\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3187\n",
      "Epoch 41, loss = 0.0851, time: 17.2569\n",
      "reconstruction loss = 0.0610, similarity loss: 0.3021\n",
      "Epoch 42, loss = 0.0945, time: 17.2998\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3601\n",
      "Epoch 43, loss = 0.0907, time: 17.2655\n",
      "reconstruction loss = 0.0646, similarity loss: 0.3259\n",
      "Epoch 44, loss = 0.0867, time: 17.7053\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3030\n",
      "Epoch 45, loss = 0.0900, time: 17.5273\n",
      "reconstruction loss = 0.0631, similarity loss: 0.3316\n",
      "Epoch 46, loss = 0.0906, time: 17.5114\n",
      "reconstruction loss = 0.0646, similarity loss: 0.3250\n",
      "Epoch 47, loss = 0.0862, time: 17.2761\n",
      "reconstruction loss = 0.0634, similarity loss: 0.2917\n",
      "Epoch 48, loss = 0.0870, time: 17.3438\n",
      "reconstruction loss = 0.0644, similarity loss: 0.2910\n",
      "Epoch 49, loss = 0.0915, time: 17.3394\n",
      "reconstruction loss = 0.0661, similarity loss: 0.3194\n",
      "Epoch 50, loss = 0.0916, time: 17.2205\n",
      "reconstruction loss = 0.0632, similarity loss: 0.3471\n",
      "Epoch 51, loss = 0.0938, time: 17.2233\n",
      "reconstruction loss = 0.0652, similarity loss: 0.3504\n",
      "Epoch 52, loss = 0.0867, time: 17.3852\n",
      "reconstruction loss = 0.0640, similarity loss: 0.2912\n",
      "Epoch 53, loss = 0.0906, time: 17.4452\n",
      "reconstruction loss = 0.0634, similarity loss: 0.3349\n",
      "Epoch 54, loss = 0.0864, time: 17.2559\n",
      "reconstruction loss = 0.0652, similarity loss: 0.2779\n",
      "Epoch 55, loss = 0.0877, time: 17.3393\n",
      "reconstruction loss = 0.0636, similarity loss: 0.3050\n",
      "Epoch 56, loss = 0.0898, time: 17.4335\n",
      "reconstruction loss = 0.0657, similarity loss: 0.3066\n",
      "Epoch 57, loss = 0.0878, time: 17.3598\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3139\n",
      "Epoch 58, loss = 0.0902, time: 17.3680\n",
      "reconstruction loss = 0.0638, similarity loss: 0.3284\n",
      "Epoch 59, loss = 0.0830, time: 17.2782\n",
      "reconstruction loss = 0.0632, similarity loss: 0.2611\n",
      "Epoch 60, loss = 0.0946, time: 17.2823\n",
      "reconstruction loss = 0.0672, similarity loss: 0.3407\n",
      "Epoch 61, loss = 0.0861, time: 17.4135\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2746\n",
      "Epoch 62, loss = 0.0846, time: 17.2769\n",
      "reconstruction loss = 0.0663, similarity loss: 0.2493\n",
      "Epoch 63, loss = 0.0868, time: 17.3815\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3038\n",
      "Epoch 64, loss = 0.0812, time: 17.2099\n",
      "reconstruction loss = 0.0633, similarity loss: 0.2422\n",
      "Epoch 65, loss = 0.0871, time: 17.3848\n",
      "reconstruction loss = 0.0640, similarity loss: 0.2943\n",
      "Epoch 66, loss = 0.0911, time: 17.1051\n",
      "reconstruction loss = 0.0638, similarity loss: 0.3373\n",
      "Epoch 67, loss = 0.0765, time: 17.1823\n",
      "reconstruction loss = 0.0417, similarity loss: 0.3902\n",
      "Epoch 68, loss = 0.0678, time: 17.2673\n",
      "reconstruction loss = 0.0381, similarity loss: 0.3345\n",
      "Epoch 69, loss = 0.0650, time: 17.2493\n",
      "reconstruction loss = 0.0374, similarity loss: 0.3130\n",
      "Epoch 70, loss = 0.0623, time: 17.2742\n",
      "reconstruction loss = 0.0368, similarity loss: 0.2927\n",
      "Epoch 71, loss = 0.0574, time: 17.5128\n",
      "reconstruction loss = 0.0306, similarity loss: 0.2989\n",
      "Epoch 72, loss = 0.0626, time: 17.7839\n",
      "reconstruction loss = 0.0336, similarity loss: 0.3234\n",
      "Epoch 73, loss = 0.0536, time: 17.8566\n",
      "reconstruction loss = 0.0282, similarity loss: 0.2827\n",
      "Epoch 74, loss = 0.0568, time: 17.7566\n",
      "reconstruction loss = 0.0272, similarity loss: 0.3235\n",
      "Epoch 75, loss = 0.0588, time: 17.7354\n",
      "reconstruction loss = 0.0283, similarity loss: 0.3332\n",
      "Epoch 76, loss = 0.0595, time: 17.3011\n",
      "reconstruction loss = 0.0280, similarity loss: 0.3432\n",
      "Epoch 77, loss = 0.0575, time: 17.2640\n",
      "reconstruction loss = 0.0275, similarity loss: 0.3278\n",
      "Epoch 78, loss = 0.0534, time: 17.2528\n",
      "reconstruction loss = 0.0271, similarity loss: 0.2898\n",
      "Epoch 79, loss = 0.0508, time: 17.2211\n",
      "reconstruction loss = 0.0260, similarity loss: 0.2736\n",
      "Epoch 80, loss = 0.0529, time: 17.4135\n",
      "reconstruction loss = 0.0246, similarity loss: 0.3072\n",
      "Epoch 81, loss = 0.0501, time: 17.4401\n",
      "reconstruction loss = 0.0238, similarity loss: 0.2869\n",
      "Epoch 82, loss = 0.0474, time: 17.2678\n",
      "reconstruction loss = 0.0223, similarity loss: 0.2728\n",
      "Epoch 83, loss = 0.0461, time: 17.2363\n",
      "reconstruction loss = 0.0206, similarity loss: 0.2756\n",
      "Epoch 84, loss = 0.0590, time: 17.2989\n",
      "reconstruction loss = 0.0286, similarity loss: 0.3323\n",
      "Epoch 85, loss = 0.1063, time: 17.3594\n",
      "reconstruction loss = 0.0570, similarity loss: 0.5494\n",
      "Epoch 86, loss = 0.0708, time: 17.2517\n",
      "reconstruction loss = 0.0326, similarity loss: 0.4150\n",
      "Epoch 87, loss = 0.0587, time: 17.2997\n",
      "reconstruction loss = 0.0270, similarity loss: 0.3440\n",
      "Epoch 88, loss = 0.0520, time: 17.2833\n",
      "reconstruction loss = 0.0246, similarity loss: 0.2992\n",
      "Epoch 89, loss = 0.0550, time: 17.3615\n",
      "reconstruction loss = 0.0270, similarity loss: 0.3069\n",
      "Epoch 90, loss = 0.0580, time: 17.2424\n",
      "reconstruction loss = 0.0285, similarity loss: 0.3235\n",
      "Epoch 91, loss = 0.0532, time: 17.2679\n",
      "reconstruction loss = 0.0277, similarity loss: 0.2826\n",
      "Epoch 92, loss = 0.1303, time: 17.2991\n",
      "reconstruction loss = 0.0973, similarity loss: 0.4276\n",
      "Epoch 93, loss = 0.1805, time: 17.3146\n",
      "reconstruction loss = 0.1337, similarity loss: 0.6026\n",
      "Epoch 94, loss = 0.1689, time: 17.2991\n",
      "reconstruction loss = 0.1209, similarity loss: 0.6011\n",
      "Epoch 95, loss = 0.1676, time: 17.2709\n",
      "reconstruction loss = 0.1157, similarity loss: 0.6343\n",
      "Epoch 96, loss = 0.1579, time: 17.2521\n",
      "reconstruction loss = 0.1157, similarity loss: 0.5378\n",
      "Epoch 97, loss = 0.1555, time: 17.0964\n",
      "reconstruction loss = 0.1130, similarity loss: 0.5379\n",
      "Epoch 98, loss = 0.1568, time: 17.0647\n",
      "reconstruction loss = 0.1116, similarity loss: 0.5640\n",
      "Epoch 99, loss = 0.1462, time: 17.2989\n",
      "reconstruction loss = 0.1067, similarity loss: 0.5015\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_0_9')\n",
    "find_lr_ae(pars, lr0=1e-6, lr1=1e-2, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_62960/3857999388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8097\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'find_lr_0_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfind_lr_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mfind_lr_ae\u001b[1;34m(pars, lr0, lr1, n_epochs, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(datapath, dataset, num_train)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Cifar100'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIFAR100\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtrain_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mtrain_tar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Cifar10'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_0_1')\n",
    "find_lr_ae(pars, lr0=1e-8, lr1=1e-3, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:16:55.629884Z",
     "start_time": "2022-02-24T22:16:55.619882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: -1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = -1 # multitask\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:31:53.090126Z",
     "start_time": "2022-02-24T22:16:56.109034Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 130123.6332, time: 14.5800\n",
      "reconstruction loss = 219655.5052, similarity loss: 40852.5920\n",
      "Parameter containing:\n",
      "tensor([0.0010, 0.0010], requires_grad=True)\n",
      "Epoch 1, loss = 123709.4378, time: 8.5705\n",
      "reconstruction loss = 210357.2171, similarity loss: 37844.9031\n",
      "Parameter containing:\n",
      "tensor([0.0022, 0.0020], requires_grad=True)\n",
      "Epoch 2, loss = 123162.9208, time: 8.6629\n",
      "reconstruction loss = 208967.9123, similarity loss: 38726.6788\n",
      "Parameter containing:\n",
      "tensor([0.0035, 0.0032], requires_grad=True)\n",
      "Epoch 3, loss = 121950.9303, time: 8.9824\n",
      "reconstruction loss = 206577.9110, similarity loss: 39335.9872\n",
      "Parameter containing:\n",
      "tensor([0.0049, 0.0046], requires_grad=True)\n",
      "Epoch 4, loss = 123737.2620, time: 8.8113\n",
      "reconstruction loss = 209934.7198, similarity loss: 40336.5102\n",
      "Parameter containing:\n",
      "tensor([0.0065, 0.0062], requires_grad=True)\n",
      "Epoch 5, loss = 117163.3604, time: 9.0556\n",
      "reconstruction loss = 199139.7501, similarity loss: 38632.8495\n",
      "Parameter containing:\n",
      "tensor([0.0083, 0.0079], requires_grad=True)\n",
      "Epoch 6, loss = 121965.5328, time: 9.2271\n",
      "reconstruction loss = 207917.0066, similarity loss: 40533.1603\n",
      "Parameter containing:\n",
      "tensor([0.0103, 0.0098], requires_grad=True)\n",
      "Epoch 7, loss = 119925.4998, time: 8.8941\n",
      "reconstruction loss = 206142.9748, similarity loss: 39199.5009\n",
      "Parameter containing:\n",
      "tensor([0.0126, 0.0120], requires_grad=True)\n",
      "Epoch 8, loss = 120172.6431, time: 8.9564\n",
      "reconstruction loss = 207394.9120, similarity loss: 39631.5073\n",
      "Parameter containing:\n",
      "tensor([0.0151, 0.0144], requires_grad=True)\n",
      "Epoch 9, loss = 117579.6975, time: 8.9737\n",
      "reconstruction loss = 203822.6443, similarity loss: 39165.3485\n",
      "Parameter containing:\n",
      "tensor([0.0180, 0.0172], requires_grad=True)\n",
      "Epoch 10, loss = 118391.0290, time: 9.1651\n",
      "reconstruction loss = 205283.2484, similarity loss: 40842.4037\n",
      "Parameter containing:\n",
      "tensor([0.0211, 0.0204], requires_grad=True)\n",
      "Epoch 11, loss = 116759.4548, time: 8.7717\n",
      "reconstruction loss = 206210.9866, similarity loss: 38158.6788\n",
      "Parameter containing:\n",
      "tensor([0.0247, 0.0237], requires_grad=True)\n",
      "Epoch 12, loss = 116879.0503, time: 8.8328\n",
      "reconstruction loss = 206709.5259, similarity loss: 39749.7356\n",
      "Parameter containing:\n",
      "tensor([0.0287, 0.0275], requires_grad=True)\n",
      "Epoch 13, loss = 116447.6959, time: 9.1094\n",
      "reconstruction loss = 209714.5560, similarity loss: 37919.5304\n",
      "Parameter containing:\n",
      "tensor([0.0332, 0.0317], requires_grad=True)\n",
      "Epoch 14, loss = 110222.2149, time: 8.8496\n",
      "reconstruction loss = 198756.9441, similarity loss: 37815.3877\n",
      "Parameter containing:\n",
      "tensor([0.0381, 0.0363], requires_grad=True)\n",
      "Epoch 15, loss = 112289.3864, time: 8.7430\n",
      "reconstruction loss = 206115.6169, similarity loss: 37384.5936\n",
      "Parameter containing:\n",
      "tensor([0.0436, 0.0414], requires_grad=True)\n",
      "Epoch 16, loss = 107961.9608, time: 8.7060\n",
      "reconstruction loss = 200291.7816, similarity loss: 36529.4039\n",
      "Parameter containing:\n",
      "tensor([0.0497, 0.0469], requires_grad=True)\n",
      "Epoch 17, loss = 108323.8081, time: 9.1111\n",
      "reconstruction loss = 202130.8234, similarity loss: 38524.4903\n",
      "Parameter containing:\n",
      "tensor([0.0565, 0.0536], requires_grad=True)\n",
      "Epoch 18, loss = 109661.6675, time: 8.6831\n",
      "reconstruction loss = 208200.2452, similarity loss: 38987.1101\n",
      "Parameter containing:\n",
      "tensor([0.0643, 0.0611], requires_grad=True)\n",
      "Epoch 19, loss = 104640.8599, time: 8.8141\n",
      "reconstruction loss = 203190.3449, similarity loss: 36540.5919\n",
      "Parameter containing:\n",
      "tensor([0.0728, 0.0689], requires_grad=True)\n",
      "Epoch 20, loss = 106154.2533, time: 9.1205\n",
      "reconstruction loss = 207223.9284, similarity loss: 40358.5777\n",
      "Parameter containing:\n",
      "tensor([0.0825, 0.0785], requires_grad=True)\n",
      "Epoch 21, loss = 101757.2560, time: 8.8144\n",
      "reconstruction loss = 202004.1307, similarity loss: 40129.8155\n",
      "Parameter containing:\n",
      "tensor([0.0930, 0.0889], requires_grad=True)\n",
      "Epoch 22, loss = 101496.5419, time: 8.6629\n",
      "reconstruction loss = 206961.3055, similarity loss: 39980.6679\n",
      "Parameter containing:\n",
      "tensor([0.1048, 0.1006], requires_grad=True)\n",
      "Epoch 23, loss = 98804.3643, time: 8.8601\n",
      "reconstruction loss = 205105.7302, similarity loss: 41323.5771\n",
      "Parameter containing:\n",
      "tensor([0.1177, 0.1138], requires_grad=True)\n",
      "Epoch 24, loss = 97529.5810, time: 8.9118\n",
      "reconstruction loss = 211527.7161, similarity loss: 38558.0742\n",
      "Parameter containing:\n",
      "tensor([0.1325, 0.1276], requires_grad=True)\n",
      "Epoch 25, loss = 92719.8554, time: 8.7746\n",
      "reconstruction loss = 207358.2244, similarity loss: 37740.7899\n",
      "Parameter containing:\n",
      "tensor([0.1487, 0.1425], requires_grad=True)\n",
      "Epoch 26, loss = 89740.6650, time: 8.7007\n",
      "reconstruction loss = 207488.1920, similarity loss: 37747.3899\n",
      "Parameter containing:\n",
      "tensor([0.1662, 0.1587], requires_grad=True)\n",
      "Epoch 27, loss = 85171.7513, time: 8.7985\n",
      "reconstruction loss = 202085.5011, similarity loss: 39234.3450\n",
      "Parameter containing:\n",
      "tensor([0.1851, 0.1773], requires_grad=True)\n",
      "Epoch 28, loss = 86630.2504, time: 8.8514\n",
      "reconstruction loss = 213923.1844, similarity loss: 41658.3840\n",
      "Parameter containing:\n",
      "tensor([0.2071, 0.1991], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, loss = 79228.5636, time: 8.8657\n",
      "reconstruction loss = 206778.2391, similarity loss: 37738.9831\n",
      "Parameter containing:\n",
      "tensor([0.2302, 0.2207], requires_grad=True)\n",
      "Epoch 30, loss = 75448.9683, time: 9.0401\n",
      "reconstruction loss = 204771.8559, similarity loss: 39430.1884\n",
      "Parameter containing:\n",
      "tensor([0.2554, 0.2449], requires_grad=True)\n",
      "Epoch 31, loss = 71035.4880, time: 9.0120\n",
      "reconstruction loss = 202890.0972, similarity loss: 39271.8126\n",
      "Parameter containing:\n",
      "tensor([0.2823, 0.2717], requires_grad=True)\n",
      "Epoch 32, loss = 66830.3025, time: 8.8228\n",
      "reconstruction loss = 200508.3633, similarity loss: 40444.7755\n",
      "Parameter containing:\n",
      "tensor([0.3115, 0.3014], requires_grad=True)\n",
      "Epoch 33, loss = 63518.2556, time: 8.7553\n",
      "reconstruction loss = 206134.4345, similarity loss: 37355.6649\n",
      "Parameter containing:\n",
      "tensor([0.3437, 0.3320], requires_grad=True)\n",
      "Epoch 34, loss = 58617.3557, time: 8.6969\n",
      "reconstruction loss = 202628.5302, similarity loss: 37430.7472\n",
      "Parameter containing:\n",
      "tensor([0.3780, 0.3643], requires_grad=True)\n",
      "Epoch 35, loss = 56385.3896, time: 8.9096\n",
      "reconstruction loss = 209203.3484, similarity loss: 38874.0172\n",
      "Parameter containing:\n",
      "tensor([0.4163, 0.4002], requires_grad=True)\n",
      "Epoch 36, loss = 51145.7792, time: 8.6856\n",
      "reconstruction loss = 205929.1631, similarity loss: 37337.2769\n",
      "Parameter containing:\n",
      "tensor([0.4565, 0.4380], requires_grad=True)\n",
      "Epoch 37, loss = 47477.6979, time: 8.8218\n",
      "reconstruction loss = 205016.6372, similarity loss: 40148.2995\n",
      "Parameter containing:\n",
      "tensor([0.4994, 0.4809], requires_grad=True)\n",
      "Epoch 38, loss = 44086.2005, time: 8.8960\n",
      "reconstruction loss = 207554.3036, similarity loss: 41253.6603\n",
      "Parameter containing:\n",
      "tensor([0.5457, 0.5282], requires_grad=True)\n",
      "Epoch 39, loss = 39274.1512, time: 8.9257\n",
      "reconstruction loss = 202159.0063, similarity loss: 41400.4786\n",
      "Parameter containing:\n",
      "tensor([0.5931, 0.5781], requires_grad=True)\n",
      "Epoch 40, loss = 34993.0668, time: 8.6133\n",
      "reconstruction loss = 201080.0498, similarity loss: 38302.6866\n",
      "Parameter containing:\n",
      "tensor([0.6434, 0.6266], requires_grad=True)\n",
      "Epoch 41, loss = 32764.1541, time: 8.8181\n",
      "reconstruction loss = 209168.2464, similarity loss: 39543.0555\n",
      "Parameter containing:\n",
      "tensor([0.6976, 0.6794], requires_grad=True)\n",
      "Epoch 42, loss = 28722.0206, time: 8.6881\n",
      "reconstruction loss = 206686.0691, similarity loss: 36858.7161\n",
      "Parameter containing:\n",
      "tensor([0.7547, 0.7322], requires_grad=True)\n",
      "Epoch 43, loss = 24990.9165, time: 8.6319\n",
      "reconstruction loss = 198578.1073, similarity loss: 38592.9582\n",
      "Parameter containing:\n",
      "tensor([0.8116, 0.7894], requires_grad=True)\n",
      "Epoch 44, loss = 23478.8087, time: 8.8369\n",
      "reconstruction loss = 209655.8791, similarity loss: 41309.0016\n",
      "Parameter containing:\n",
      "tensor([0.8736, 0.8515], requires_grad=True)\n",
      "Epoch 45, loss = 20081.8206, time: 8.7352\n",
      "reconstruction loss = 203172.6239, similarity loss: 40188.2646\n",
      "Parameter containing:\n",
      "tensor([0.9367, 0.9163], requires_grad=True)\n",
      "Epoch 46, loss = 17567.9698, time: 8.6572\n",
      "reconstruction loss = 203010.9531, similarity loss: 38860.8984\n",
      "Parameter containing:\n",
      "tensor([1.0012, 0.9805], requires_grad=True)\n",
      "Epoch 47, loss = 15465.8896, time: 8.8849\n",
      "reconstruction loss = 205876.6946, similarity loss: 37008.8287\n",
      "Parameter containing:\n",
      "tensor([1.0687, 1.0423], requires_grad=True)\n",
      "Epoch 48, loss = 13913.3629, time: 8.7087\n",
      "reconstruction loss = 207901.7409, similarity loss: 42451.6445\n",
      "Parameter containing:\n",
      "tensor([1.1383, 1.1162], requires_grad=True)\n",
      "Epoch 49, loss = 11805.9901, time: 8.8447\n",
      "reconstruction loss = 203846.9914, similarity loss: 40852.9975\n",
      "Parameter containing:\n",
      "tensor([1.2092, 1.1881], requires_grad=True)\n",
      "Epoch 50, loss = 10191.2446, time: 8.6301\n",
      "reconstruction loss = 203687.9830, similarity loss: 39871.8877\n",
      "Parameter containing:\n",
      "tensor([1.2802, 1.2601], requires_grad=True)\n",
      "Epoch 51, loss = 8987.7528, time: 8.7140\n",
      "reconstruction loss = 210161.4898, similarity loss: 38384.1285\n",
      "Parameter containing:\n",
      "tensor([1.3556, 1.3315], requires_grad=True)\n",
      "Epoch 52, loss = 7735.5193, time: 8.8611\n",
      "reconstruction loss = 209007.9160, similarity loss: 39483.9115\n",
      "Parameter containing:\n",
      "tensor([1.4311, 1.4050], requires_grad=True)\n",
      "Epoch 53, loss = 6320.0147, time: 8.7615\n",
      "reconstruction loss = 198519.5386, similarity loss: 37011.5163\n",
      "Parameter containing:\n",
      "tensor([1.5052, 1.4761], requires_grad=True)\n",
      "Epoch 54, loss = 5627.7092, time: 8.8728\n",
      "reconstruction loss = 205282.8480, similarity loss: 38005.2101\n",
      "Parameter containing:\n",
      "tensor([1.5812, 1.5504], requires_grad=True)\n",
      "Epoch 55, loss = 4771.5518, time: 8.7243\n",
      "reconstruction loss = 203906.3819, similarity loss: 36360.2945\n",
      "Parameter containing:\n",
      "tensor([1.6580, 1.6223], requires_grad=True)\n",
      "Epoch 56, loss = 4233.4129, time: 8.8243\n",
      "reconstruction loss = 206489.7914, similarity loss: 42159.0569\n",
      "Parameter containing:\n",
      "tensor([1.7362, 1.7055], requires_grad=True)\n",
      "Epoch 57, loss = 3638.9634, time: 8.7539\n",
      "reconstruction loss = 212461.3278, similarity loss: 38624.1160\n",
      "Parameter containing:\n",
      "tensor([1.8173, 1.7842], requires_grad=True)\n",
      "Epoch 58, loss = 3006.9300, time: 9.1957\n",
      "reconstruction loss = 205099.7152, similarity loss: 38211.0602\n",
      "Parameter containing:\n",
      "tensor([1.8968, 1.8614], requires_grad=True)\n",
      "Epoch 59, loss = 2647.0040, time: 8.9744\n",
      "reconstruction loss = 209512.9612, similarity loss: 41618.3750\n",
      "Parameter containing:\n",
      "tensor([1.9771, 1.9455], requires_grad=True)\n",
      "Epoch 60, loss = 2212.0228, time: 8.8273\n",
      "reconstruction loss = 207169.3370, similarity loss: 39633.4103\n",
      "Parameter containing:\n",
      "tensor([2.0582, 2.0264], requires_grad=True)\n",
      "Epoch 61, loss = 1882.2798, time: 8.7428\n",
      "reconstruction loss = 204397.9739, similarity loss: 42104.7637\n",
      "Parameter containing:\n",
      "tensor([2.1377, 2.1117], requires_grad=True)\n",
      "Epoch 62, loss = 1583.4113, time: 8.7203\n",
      "reconstruction loss = 204083.9151, similarity loss: 39457.8836\n",
      "Parameter containing:\n",
      "tensor([2.2179, 2.1918], requires_grad=True)\n",
      "Epoch 63, loss = 1353.8764, time: 8.8130\n",
      "reconstruction loss = 205125.9977, similarity loss: 39271.2416\n",
      "Parameter containing:\n",
      "tensor([2.2984, 2.2725], requires_grad=True)\n",
      "Epoch 64, loss = 1159.3706, time: 8.7763\n",
      "reconstruction loss = 207569.9555, similarity loss: 38346.9542\n",
      "Parameter containing:\n",
      "tensor([2.3800, 2.3512], requires_grad=True)\n",
      "Epoch 65, loss = 985.0667, time: 8.8232\n",
      "reconstruction loss = 206428.7823, similarity loss: 39181.8893\n",
      "Parameter containing:\n",
      "tensor([2.4620, 2.4324], requires_grad=True)\n",
      "Epoch 66, loss = 840.9800, time: 8.7009\n",
      "reconstruction loss = 206963.9071, similarity loss: 39668.6241\n",
      "Parameter containing:\n",
      "tensor([2.5436, 2.5136], requires_grad=True)\n",
      "Epoch 67, loss = 715.3987, time: 8.8132\n",
      "reconstruction loss = 211067.4202, similarity loss: 35999.8057\n",
      "Parameter containing:\n",
      "tensor([2.6274, 2.5891], requires_grad=True)\n",
      "Epoch 68, loss = 585.5291, time: 8.9628\n",
      "reconstruction loss = 196494.5320, similarity loss: 40375.1531\n",
      "Parameter containing:\n",
      "tensor([2.7057, 2.6727], requires_grad=True)\n",
      "Epoch 69, loss = 516.4037, time: 8.9453\n",
      "reconstruction loss = 204336.6780, similarity loss: 40695.2673\n",
      "Parameter containing:\n",
      "tensor([2.7876, 2.7576], requires_grad=True)\n",
      "Epoch 70, loss = 437.5559, time: 8.9269\n",
      "reconstruction loss = 206178.8321, similarity loss: 38038.8592\n",
      "Parameter containing:\n",
      "tensor([2.8695, 2.8363], requires_grad=True)\n",
      "Epoch 71, loss = 379.4068, time: 8.6867\n",
      "reconstruction loss = 209917.6555, similarity loss: 39182.3626\n",
      "Parameter containing:\n",
      "tensor([2.9528, 2.9188], requires_grad=True)\n",
      "Epoch 72, loss = 317.4711, time: 9.0324\n",
      "reconstruction loss = 207989.3196, similarity loss: 37247.4364\n",
      "Parameter containing:\n",
      "tensor([3.0361, 2.9969], requires_grad=True)\n",
      "Epoch 73, loss = 270.3821, time: 8.9406\n",
      "reconstruction loss = 209595.1741, similarity loss: 35711.7782\n",
      "Parameter containing:\n",
      "tensor([3.1195, 3.0727], requires_grad=True)\n",
      "Epoch 74, loss = 226.8791, time: 8.9278\n",
      "reconstruction loss = 203925.5002, similarity loss: 36977.3007\n",
      "Parameter containing:\n",
      "tensor([3.2005, 3.1513], requires_grad=True)\n",
      "Epoch 75, loss = 193.6572, time: 8.9489\n",
      "reconstruction loss = 202364.4834, similarity loss: 37814.2608\n",
      "Parameter containing:\n",
      "tensor([3.2810, 3.2331], requires_grad=True)\n",
      "Epoch 76, loss = 167.4880, time: 8.6919\n",
      "reconstruction loss = 205008.6848, similarity loss: 37809.6236\n",
      "Parameter containing:\n",
      "tensor([3.3631, 3.3130], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, loss = 143.9475, time: 9.5503\n",
      "reconstruction loss = 205711.4250, similarity loss: 38212.7742\n",
      "Parameter containing:\n",
      "tensor([3.4447, 3.3954], requires_grad=True)\n",
      "Epoch 78, loss = 125.3470, time: 9.0145\n",
      "reconstruction loss = 210488.4595, similarity loss: 38058.1368\n",
      "Parameter containing:\n",
      "tensor([3.5296, 3.4755], requires_grad=True)\n",
      "Epoch 79, loss = 109.1450, time: 10.0432\n",
      "reconstruction loss = 211122.6866, similarity loss: 41993.9651\n",
      "Parameter containing:\n",
      "tensor([3.6134, 3.5645], requires_grad=True)\n",
      "Epoch 80, loss = 91.8170, time: 9.0247\n",
      "reconstruction loss = 209075.1575, similarity loss: 39159.8628\n",
      "Parameter containing:\n",
      "tensor([3.6968, 3.6471], requires_grad=True)\n",
      "Epoch 81, loss = 77.7652, time: 9.0050\n",
      "reconstruction loss = 202383.8677, similarity loss: 40741.8526\n",
      "Parameter containing:\n",
      "tensor([3.7766, 3.7320], requires_grad=True)\n",
      "Epoch 82, loss = 67.6964, time: 9.1695\n",
      "reconstruction loss = 204096.1639, similarity loss: 40335.7834\n",
      "Parameter containing:\n",
      "tensor([3.8581, 3.8146], requires_grad=True)\n",
      "Epoch 83, loss = 59.0602, time: 9.0081\n",
      "reconstruction loss = 209254.5805, similarity loss: 36788.1851\n",
      "Parameter containing:\n",
      "tensor([3.9408, 3.8899], requires_grad=True)\n",
      "Epoch 84, loss = 51.7028, time: 8.9115\n",
      "reconstruction loss = 207105.9598, similarity loss: 39977.6201\n",
      "Parameter containing:\n",
      "tensor([4.0230, 3.9711], requires_grad=True)\n",
      "Epoch 85, loss = 44.9170, time: 9.1677\n",
      "reconstruction loss = 204196.6412, similarity loss: 40487.0318\n",
      "Parameter containing:\n",
      "tensor([4.1042, 4.0534], requires_grad=True)\n",
      "Epoch 86, loss = 40.4078, time: 8.9570\n",
      "reconstruction loss = 212473.5952, similarity loss: 39488.5603\n",
      "Parameter containing:\n",
      "tensor([4.1875, 4.1340], requires_grad=True)\n",
      "Epoch 87, loss = 34.5734, time: 8.8212\n",
      "reconstruction loss = 202309.6415, similarity loss: 38870.4416\n",
      "Parameter containing:\n",
      "tensor([4.2680, 4.2103], requires_grad=True)\n",
      "Epoch 88, loss = 31.8998, time: 8.9795\n",
      "reconstruction loss = 211608.3900, similarity loss: 41092.2276\n",
      "Parameter containing:\n",
      "tensor([4.3514, 4.2919], requires_grad=True)\n",
      "Epoch 89, loss = 27.6758, time: 9.1119\n",
      "reconstruction loss = 203220.9864, similarity loss: 38296.5125\n",
      "Parameter containing:\n",
      "tensor([4.4312, 4.3653], requires_grad=True)\n",
      "Epoch 90, loss = 24.9232, time: 8.8551\n",
      "reconstruction loss = 201724.8587, similarity loss: 37539.0712\n",
      "Parameter containing:\n",
      "tensor([4.5097, 4.4368], requires_grad=True)\n",
      "Epoch 91, loss = 23.0204, time: 8.9279\n",
      "reconstruction loss = 206319.1470, similarity loss: 37742.0478\n",
      "Parameter containing:\n",
      "tensor([4.5914, 4.5080], requires_grad=True)\n",
      "Epoch 92, loss = 21.6199, time: 8.8604\n",
      "reconstruction loss = 217155.8416, similarity loss: 37982.7043\n",
      "Parameter containing:\n",
      "tensor([4.6749, 4.5785], requires_grad=True)\n",
      "Epoch 93, loss = 19.4340, time: 8.7874\n",
      "reconstruction loss = 202663.6101, similarity loss: 39711.3912\n",
      "Parameter containing:\n",
      "tensor([4.7537, 4.6522], requires_grad=True)\n",
      "Epoch 94, loss = 18.4434, time: 8.7832\n",
      "reconstruction loss = 212405.2473, similarity loss: 39417.0627\n",
      "Parameter containing:\n",
      "tensor([4.8350, 4.7200], requires_grad=True)\n",
      "Epoch 95, loss = 17.0853, time: 9.1015\n",
      "reconstruction loss = 206166.1874, similarity loss: 38647.0061\n",
      "Parameter containing:\n",
      "tensor([4.9143, 4.7862], requires_grad=True)\n",
      "Epoch 96, loss = 16.2117, time: 8.9000\n",
      "reconstruction loss = 207341.4192, similarity loss: 38730.7542\n",
      "Parameter containing:\n",
      "tensor([4.9930, 4.8507], requires_grad=True)\n",
      "Epoch 97, loss = 15.4504, time: 8.9154\n",
      "reconstruction loss = 208573.9102, similarity loss: 37663.5890\n",
      "Parameter containing:\n",
      "tensor([5.0707, 4.9074], requires_grad=True)\n",
      "Epoch 98, loss = 14.9501, time: 8.7308\n",
      "reconstruction loss = 212876.4080, similarity loss: 39663.1628\n",
      "Parameter containing:\n",
      "tensor([5.1496, 4.9680], requires_grad=True)\n",
      "Epoch 99, loss = 14.2360, time: 8.8234\n",
      "reconstruction loss = 203168.4597, similarity loss: 38193.0386\n",
      "Parameter containing:\n",
      "tensor([5.2236, 5.0197], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_mtl')\n",
    "find_lr_ae(pars, lr0=1e-5, lr1=1, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T22:59:06.523888Z",
     "start_time": "2022-02-24T22:59:06.504883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: -1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = -1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T00:04:23.146875Z",
     "start_time": "2022-02-24T22:59:08.583135Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_300_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_-1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.6419, time: 15.9900\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2675\n",
      "Parameter containing:\n",
      "tensor([-0.0499,  0.0204], requires_grad=True)\n",
      "Epoch 1, loss = 0.5781, time: 10.9410\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2387\n",
      "Parameter containing:\n",
      "tensor([-0.0996,  0.0364], requires_grad=True)\n",
      "Epoch 2, loss = 0.5065, time: 10.9122\n",
      "reconstruction loss = 0.0687, similarity loss: 1.1843\n",
      "Parameter containing:\n",
      "tensor([-0.1491,  0.0455], requires_grad=True)\n",
      "Epoch 3, loss = 0.5128, time: 10.8894\n",
      "reconstruction loss = 0.0675, similarity loss: 1.3022\n",
      "Parameter containing:\n",
      "tensor([-0.1984,  0.0618], requires_grad=True)\n",
      "Epoch 4, loss = 0.4312, time: 10.9420\n",
      "reconstruction loss = 0.0687, similarity loss: 1.2189\n",
      "Parameter containing:\n",
      "tensor([-0.2473,  0.0706], requires_grad=True)\n",
      "Epoch 5, loss = 0.4079, time: 10.9509\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2688\n",
      "Parameter containing:\n",
      "tensor([-0.2960,  0.0800], requires_grad=True)\n",
      "Epoch 6, loss = 0.3342, time: 10.7732\n",
      "reconstruction loss = 0.0680, similarity loss: 1.1962\n",
      "Parameter containing:\n",
      "tensor([-0.3444,  0.0813], requires_grad=True)\n",
      "Epoch 7, loss = 0.3008, time: 10.8753\n",
      "reconstruction loss = 0.0699, similarity loss: 1.2109\n",
      "Parameter containing:\n",
      "tensor([-0.3922,  0.0841], requires_grad=True)\n",
      "Epoch 8, loss = 0.2547, time: 11.0365\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2048\n",
      "Parameter containing:\n",
      "tensor([-0.4398,  0.0870], requires_grad=True)\n",
      "Epoch 9, loss = 0.2143, time: 11.1680\n",
      "reconstruction loss = 0.0675, similarity loss: 1.2023\n",
      "Parameter containing:\n",
      "tensor([-0.4870,  0.0864], requires_grad=True)\n",
      "Epoch 10, loss = 0.1968, time: 10.8070\n",
      "reconstruction loss = 0.0652, similarity loss: 1.2606\n",
      "Parameter containing:\n",
      "tensor([-0.5340,  0.0949], requires_grad=True)\n",
      "Epoch 11, loss = 0.1146, time: 10.8904\n",
      "reconstruction loss = 0.0660, similarity loss: 1.1517\n",
      "Parameter containing:\n",
      "tensor([-0.5804,  0.0894], requires_grad=True)\n",
      "Epoch 12, loss = 0.1280, time: 10.8385\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2662\n",
      "Parameter containing:\n",
      "tensor([-0.6260,  0.0929], requires_grad=True)\n",
      "Epoch 13, loss = 0.0562, time: 11.1426\n",
      "reconstruction loss = 0.0678, similarity loss: 1.1741\n",
      "Parameter containing:\n",
      "tensor([-0.6706,  0.0911], requires_grad=True)\n",
      "Epoch 14, loss = 0.0463, time: 11.3271\n",
      "reconstruction loss = 0.0661, similarity loss: 1.2376\n",
      "Parameter containing:\n",
      "tensor([-0.7150,  0.0953], requires_grad=True)\n",
      "Epoch 15, loss = 0.0123, time: 11.0412\n",
      "reconstruction loss = 0.0680, similarity loss: 1.2220\n",
      "Parameter containing:\n",
      "tensor([-0.7581,  0.0962], requires_grad=True)\n",
      "Epoch 16, loss = -0.0503, time: 11.1188\n",
      "reconstruction loss = 0.0678, similarity loss: 1.1429\n",
      "Parameter containing:\n",
      "tensor([-0.8003,  0.0915], requires_grad=True)\n",
      "Epoch 17, loss = -0.0586, time: 11.9966\n",
      "reconstruction loss = 0.0649, similarity loss: 1.2091\n",
      "Parameter containing:\n",
      "tensor([-0.8424,  0.0906], requires_grad=True)\n",
      "Epoch 18, loss = -0.0475, time: 11.4657\n",
      "reconstruction loss = 0.0712, similarity loss: 1.2568\n",
      "Parameter containing:\n",
      "tensor([-0.8814,  0.0961], requires_grad=True)\n",
      "Epoch 19, loss = -0.0868, time: 11.1545\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2470\n",
      "Parameter containing:\n",
      "tensor([-0.9208,  0.1004], requires_grad=True)\n",
      "Epoch 20, loss = -0.1312, time: 11.1935\n",
      "reconstruction loss = 0.0665, similarity loss: 1.1988\n",
      "Parameter containing:\n",
      "tensor([-0.9588,  0.0983], requires_grad=True)\n",
      "Epoch 21, loss = -0.1358, time: 10.7034\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2319\n",
      "Parameter containing:\n",
      "tensor([-0.9956,  0.0988], requires_grad=True)\n",
      "Epoch 22, loss = -0.1172, time: 10.9085\n",
      "reconstruction loss = 0.0678, similarity loss: 1.3158\n",
      "Parameter containing:\n",
      "tensor([-1.0303,  0.1084], requires_grad=True)\n",
      "Epoch 23, loss = -0.1652, time: 11.8777\n",
      "reconstruction loss = 0.0681, similarity loss: 1.2344\n",
      "Parameter containing:\n",
      "tensor([-1.0633,  0.1085], requires_grad=True)\n",
      "Epoch 24, loss = -0.2078, time: 11.5987\n",
      "reconstruction loss = 0.0659, similarity loss: 1.1866\n",
      "Parameter containing:\n",
      "tensor([-1.0955,  0.1038], requires_grad=True)\n",
      "Epoch 25, loss = -0.1863, time: 12.2406\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2516\n",
      "Parameter containing:\n",
      "tensor([-1.1252,  0.1046], requires_grad=True)\n",
      "Epoch 26, loss = -0.2233, time: 10.7550\n",
      "reconstruction loss = 0.0668, similarity loss: 1.1963\n",
      "Parameter containing:\n",
      "tensor([-1.1536,  0.1029], requires_grad=True)\n",
      "Epoch 27, loss = -0.1951, time: 10.3350\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2810\n",
      "Parameter containing:\n",
      "tensor([-1.1793,  0.1054], requires_grad=True)\n",
      "Epoch 28, loss = -0.2058, time: 10.7269\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2733\n",
      "Parameter containing:\n",
      "tensor([-1.2030,  0.1096], requires_grad=True)\n",
      "Epoch 29, loss = -0.2338, time: 10.6386\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2101\n",
      "Parameter containing:\n",
      "tensor([-1.2236,  0.1062], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, loss = -0.2051, time: 10.2764\n",
      "reconstruction loss = 0.0681, similarity loss: 1.2882\n",
      "Parameter containing:\n",
      "tensor([-1.2425,  0.1117], requires_grad=True)\n",
      "Epoch 31, loss = -0.2316, time: 10.9696\n",
      "reconstruction loss = 0.0680, similarity loss: 1.2314\n",
      "Parameter containing:\n",
      "tensor([-1.2597,  0.1109], requires_grad=True)\n",
      "Epoch 32, loss = -0.2527, time: 10.5501\n",
      "reconstruction loss = 0.0673, similarity loss: 1.1974\n",
      "Parameter containing:\n",
      "tensor([-1.2751,  0.1063], requires_grad=True)\n",
      "Epoch 33, loss = -0.2163, time: 10.3970\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2849\n",
      "Parameter containing:\n",
      "tensor([-1.2885,  0.1101], requires_grad=True)\n",
      "Epoch 34, loss = -0.2171, time: 10.9488\n",
      "reconstruction loss = 0.0690, similarity loss: 1.2664\n",
      "Parameter containing:\n",
      "tensor([-1.2971,  0.1123], requires_grad=True)\n",
      "Epoch 35, loss = -0.2770, time: 10.5481\n",
      "reconstruction loss = 0.0690, similarity loss: 1.1194\n",
      "Parameter containing:\n",
      "tensor([-1.3062,  0.1002], requires_grad=True)\n",
      "Epoch 36, loss = -0.2086, time: 10.4841\n",
      "reconstruction loss = 0.0677, similarity loss: 1.3098\n",
      "Parameter containing:\n",
      "tensor([-1.3142,  0.1073], requires_grad=True)\n",
      "Epoch 37, loss = -0.2080, time: 10.9134\n",
      "reconstruction loss = 0.0685, similarity loss: 1.2999\n",
      "Parameter containing:\n",
      "tensor([-1.3196,  0.1124], requires_grad=True)\n",
      "Epoch 38, loss = -0.2629, time: 10.3118\n",
      "reconstruction loss = 0.0660, similarity loss: 1.2085\n",
      "Parameter containing:\n",
      "tensor([-1.3281,  0.1089], requires_grad=True)\n",
      "Epoch 39, loss = -0.2250, time: 10.5620\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2916\n",
      "Parameter containing:\n",
      "tensor([-1.3341,  0.1124], requires_grad=True)\n",
      "Epoch 40, loss = -0.2473, time: 11.3576\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2256\n",
      "Parameter containing:\n",
      "tensor([-1.3380,  0.1114], requires_grad=True)\n",
      "Epoch 41, loss = -0.2441, time: 9.9634\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2438\n",
      "Parameter containing:\n",
      "tensor([-1.3418,  0.1104], requires_grad=True)\n",
      "Epoch 42, loss = -0.2459, time: 12.6705\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2179\n",
      "Parameter containing:\n",
      "tensor([-1.3428,  0.1067], requires_grad=True)\n",
      "Epoch 43, loss = -0.2322, time: 14.6955\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2538\n",
      "Parameter containing:\n",
      "tensor([-1.3441,  0.1101], requires_grad=True)\n",
      "Epoch 44, loss = -0.2476, time: 13.5635\n",
      "reconstruction loss = 0.0666, similarity loss: 1.2389\n",
      "Parameter containing:\n",
      "tensor([-1.3457,  0.1078], requires_grad=True)\n",
      "Epoch 45, loss = -0.2888, time: 12.7120\n",
      "reconstruction loss = 0.0671, similarity loss: 1.1273\n",
      "Parameter containing:\n",
      "tensor([-1.3472,  0.0984], requires_grad=True)\n",
      "Epoch 46, loss = -0.2603, time: 14.7573\n",
      "reconstruction loss = 0.0668, similarity loss: 1.2031\n",
      "Parameter containing:\n",
      "tensor([-1.3484,  0.0968], requires_grad=True)\n",
      "Epoch 47, loss = -0.2733, time: 14.4413\n",
      "reconstruction loss = 0.0675, similarity loss: 1.1587\n",
      "Parameter containing:\n",
      "tensor([-1.3477,  0.0909], requires_grad=True)\n",
      "Epoch 48, loss = -0.2313, time: 13.7533\n",
      "reconstruction loss = 0.0670, similarity loss: 1.2691\n",
      "Parameter containing:\n",
      "tensor([-1.3496,  0.0963], requires_grad=True)\n",
      "Epoch 49, loss = -0.2599, time: 12.0678\n",
      "reconstruction loss = 0.0673, similarity loss: 1.1958\n",
      "Parameter containing:\n",
      "tensor([-1.3496,  0.0969], requires_grad=True)\n",
      "Epoch 50, loss = -0.2523, time: 11.2824\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2164\n",
      "Parameter containing:\n",
      "tensor([-1.3498,  0.0981], requires_grad=True)\n",
      "Epoch 51, loss = -0.2469, time: 11.2038\n",
      "reconstruction loss = 0.0682, similarity loss: 1.2111\n",
      "Parameter containing:\n",
      "tensor([-1.3478,  0.0969], requires_grad=True)\n",
      "Epoch 52, loss = -0.2867, time: 11.2451\n",
      "reconstruction loss = 0.0655, similarity loss: 1.1628\n",
      "Parameter containing:\n",
      "tensor([-1.3515,  0.0911], requires_grad=True)\n",
      "Epoch 53, loss = -0.2502, time: 11.2913\n",
      "reconstruction loss = 0.0686, similarity loss: 1.1951\n",
      "Parameter containing:\n",
      "tensor([-1.3497,  0.0912], requires_grad=True)\n",
      "Epoch 54, loss = -0.2217, time: 11.3514\n",
      "reconstruction loss = 0.0663, similarity loss: 1.3065\n",
      "Parameter containing:\n",
      "tensor([-1.3500,  0.1002], requires_grad=True)\n",
      "Epoch 55, loss = -0.2297, time: 11.3335\n",
      "reconstruction loss = 0.0686, similarity loss: 1.2462\n",
      "Parameter containing:\n",
      "tensor([-1.3474,  0.1030], requires_grad=True)\n",
      "Epoch 56, loss = -0.2575, time: 10.9023\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2013\n",
      "Parameter containing:\n",
      "tensor([-1.3466,  0.1008], requires_grad=True)\n",
      "Epoch 57, loss = -0.1979, time: 11.5272\n",
      "reconstruction loss = 0.0695, similarity loss: 1.3078\n",
      "Parameter containing:\n",
      "tensor([-1.3428,  0.1091], requires_grad=True)\n",
      "Epoch 58, loss = -0.2256, time: 11.1679\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2617\n",
      "Parameter containing:\n",
      "tensor([-1.3420,  0.1111], requires_grad=True)\n",
      "Epoch 59, loss = -0.2484, time: 11.0975\n",
      "reconstruction loss = 0.0670, similarity loss: 1.2291\n",
      "Parameter containing:\n",
      "tensor([-1.3448,  0.1092], requires_grad=True)\n",
      "Epoch 60, loss = -0.2195, time: 11.5000\n",
      "reconstruction loss = 0.0704, similarity loss: 1.2374\n",
      "Parameter containing:\n",
      "tensor([-1.3394,  0.1099], requires_grad=True)\n",
      "Epoch 61, loss = -0.2703, time: 11.1088\n",
      "reconstruction loss = 0.0659, similarity loss: 1.1947\n",
      "Parameter containing:\n",
      "tensor([-1.3456,  0.1050], requires_grad=True)\n",
      "Epoch 62, loss = -0.2294, time: 11.3835\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2434\n",
      "Parameter containing:\n",
      "tensor([-1.3435,  0.1043], requires_grad=True)\n",
      "Epoch 63, loss = -0.2729, time: 11.2341\n",
      "reconstruction loss = 0.0662, similarity loss: 1.1836\n",
      "Parameter containing:\n",
      "tensor([-1.3480,  0.1006], requires_grad=True)\n",
      "Epoch 64, loss = -0.2310, time: 11.2389\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2665\n",
      "Parameter containing:\n",
      "tensor([-1.3498,  0.1032], requires_grad=True)\n",
      "Epoch 65, loss = -0.2677, time: 11.4430\n",
      "reconstruction loss = 0.0648, similarity loss: 1.2220\n",
      "Parameter containing:\n",
      "tensor([-1.3558,  0.1040], requires_grad=True)\n",
      "Epoch 66, loss = -0.2560, time: 11.0099\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2054\n",
      "Parameter containing:\n",
      "tensor([-1.3539,  0.1003], requires_grad=True)\n",
      "Epoch 67, loss = -0.2377, time: 11.2867\n",
      "reconstruction loss = 0.0658, similarity loss: 1.2774\n",
      "Parameter containing:\n",
      "tensor([-1.3563,  0.1065], requires_grad=True)\n",
      "Epoch 68, loss = -0.2270, time: 11.2052\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2700\n",
      "Parameter containing:\n",
      "tensor([-1.3532,  0.1087], requires_grad=True)\n",
      "Epoch 69, loss = -0.2629, time: 11.0012\n",
      "reconstruction loss = 0.0655, similarity loss: 1.2211\n",
      "Parameter containing:\n",
      "tensor([-1.3565,  0.1080], requires_grad=True)\n",
      "Epoch 70, loss = -0.2413, time: 11.1670\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2303\n",
      "Parameter containing:\n",
      "tensor([-1.3536,  0.1061], requires_grad=True)\n",
      "Epoch 71, loss = -0.2601, time: 11.1306\n",
      "reconstruction loss = 0.0653, similarity loss: 1.2305\n",
      "Parameter containing:\n",
      "tensor([-1.3575,  0.1064], requires_grad=True)\n",
      "Epoch 72, loss = -0.2633, time: 10.6738\n",
      "reconstruction loss = 0.0671, similarity loss: 1.1898\n",
      "Parameter containing:\n",
      "tensor([-1.3530,  0.1024], requires_grad=True)\n",
      "Epoch 73, loss = -0.2469, time: 11.1764\n",
      "reconstruction loss = 0.0696, similarity loss: 1.1844\n",
      "Parameter containing:\n",
      "tensor([-1.3464,  0.0975], requires_grad=True)\n",
      "Epoch 74, loss = -0.2715, time: 11.1155\n",
      "reconstruction loss = 0.0676, similarity loss: 1.1623\n",
      "Parameter containing:\n",
      "tensor([-1.3454,  0.0918], requires_grad=True)\n",
      "Epoch 75, loss = -0.2109, time: 11.7319\n",
      "reconstruction loss = 0.0701, similarity loss: 1.2650\n",
      "Parameter containing:\n",
      "tensor([-1.3395,  0.0979], requires_grad=True)\n",
      "Epoch 76, loss = -0.2169, time: 13.0013\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2943\n",
      "Parameter containing:\n",
      "tensor([-1.3430,  0.1051], requires_grad=True)\n",
      "Epoch 77, loss = -0.2258, time: 14.0058\n",
      "reconstruction loss = 0.0684, similarity loss: 1.2586\n",
      "Parameter containing:\n",
      "tensor([-1.3424,  0.1075], requires_grad=True)\n",
      "Epoch 78, loss = -0.2453, time: 12.2863\n",
      "reconstruction loss = 0.0686, similarity loss: 1.2074\n",
      "Parameter containing:\n",
      "tensor([-1.3414,  0.1063], requires_grad=True)\n",
      "Epoch 79, loss = -0.2616, time: 10.8916\n",
      "reconstruction loss = 0.0676, similarity loss: 1.1851\n",
      "Parameter containing:\n",
      "tensor([-1.3433,  0.0993], requires_grad=True)\n",
      "Epoch 80, loss = -0.2486, time: 12.2067\n",
      "reconstruction loss = 0.0691, similarity loss: 1.1900\n",
      "Parameter containing:\n",
      "tensor([-1.3409,  0.0964], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, loss = -0.2509, time: 11.7470\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2199\n",
      "Parameter containing:\n",
      "tensor([-1.3443,  0.0979], requires_grad=True)\n",
      "Epoch 82, loss = -0.2418, time: 11.5364\n",
      "reconstruction loss = 0.0668, similarity loss: 1.2488\n",
      "Parameter containing:\n",
      "tensor([-1.3466,  0.1010], requires_grad=True)\n",
      "Epoch 83, loss = -0.2402, time: 12.4983\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2335\n",
      "Parameter containing:\n",
      "tensor([-1.3473,  0.1011], requires_grad=True)\n",
      "Epoch 84, loss = -0.2575, time: 12.0416\n",
      "reconstruction loss = 0.0670, similarity loss: 1.2058\n",
      "Parameter containing:\n",
      "tensor([-1.3482,  0.0998], requires_grad=True)\n",
      "Epoch 85, loss = -0.2736, time: 12.2229\n",
      "reconstruction loss = 0.0664, similarity loss: 1.1780\n",
      "Parameter containing:\n",
      "tensor([-1.3501,  0.0975], requires_grad=True)\n",
      "Epoch 86, loss = -0.2367, time: 11.9903\n",
      "reconstruction loss = 0.0689, similarity loss: 1.2235\n",
      "Parameter containing:\n",
      "tensor([-1.3476,  0.0975], requires_grad=True)\n",
      "Epoch 87, loss = -0.2225, time: 10.9634\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2813\n",
      "Parameter containing:\n",
      "tensor([-1.3492,  0.1022], requires_grad=True)\n",
      "Epoch 88, loss = -0.2421, time: 10.6415\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2121\n",
      "Parameter containing:\n",
      "tensor([-1.3449,  0.1022], requires_grad=True)\n",
      "Epoch 89, loss = -0.2236, time: 10.3624\n",
      "reconstruction loss = 0.0663, similarity loss: 1.3024\n",
      "Parameter containing:\n",
      "tensor([-1.3487,  0.1096], requires_grad=True)\n",
      "Epoch 90, loss = -0.2434, time: 10.1832\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2083\n",
      "Parameter containing:\n",
      "tensor([-1.3448,  0.1062], requires_grad=True)\n",
      "Epoch 91, loss = -0.2756, time: 11.6645\n",
      "reconstruction loss = 0.0678, similarity loss: 1.1482\n",
      "Parameter containing:\n",
      "tensor([-1.3459,  0.0965], requires_grad=True)\n",
      "Epoch 92, loss = -0.2134, time: 11.9071\n",
      "reconstruction loss = 0.0690, similarity loss: 1.2778\n",
      "Parameter containing:\n",
      "tensor([-1.3413,  0.1027], requires_grad=True)\n",
      "Epoch 93, loss = -0.2720, time: 12.6922\n",
      "reconstruction loss = 0.0660, similarity loss: 1.1882\n",
      "Parameter containing:\n",
      "tensor([-1.3465,  0.0996], requires_grad=True)\n",
      "Epoch 94, loss = -0.2595, time: 12.4559\n",
      "reconstruction loss = 0.0675, similarity loss: 1.1925\n",
      "Parameter containing:\n",
      "tensor([-1.3480,  0.0970], requires_grad=True)\n",
      "Epoch 95, loss = -0.2219, time: 11.9423\n",
      "reconstruction loss = 0.0693, similarity loss: 1.2521\n",
      "Parameter containing:\n",
      "tensor([-1.3445,  0.1015], requires_grad=True)\n",
      "Epoch 96, loss = -0.2410, time: 12.0540\n",
      "reconstruction loss = 0.0698, similarity loss: 1.1964\n",
      "Parameter containing:\n",
      "tensor([-1.3398,  0.0984], requires_grad=True)\n",
      "Epoch 97, loss = -0.2499, time: 11.3073\n",
      "reconstruction loss = 0.0666, similarity loss: 1.2323\n",
      "Parameter containing:\n",
      "tensor([-1.3431,  0.1003], requires_grad=True)\n",
      "Epoch 98, loss = -0.2883, time: 11.8367\n",
      "reconstruction loss = 0.0690, similarity loss: 1.0963\n",
      "Parameter containing:\n",
      "tensor([-1.3408,  0.0891], requires_grad=True)\n",
      "Epoch 99, loss = -0.2475, time: 12.2197\n",
      "reconstruction loss = 0.0666, similarity loss: 1.2377\n",
      "Parameter containing:\n",
      "tensor([-1.3461,  0.0936], requires_grad=True)\n",
      "Epoch 100, loss = -0.2695, time: 11.8229\n",
      "reconstruction loss = 0.0657, similarity loss: 1.2007\n",
      "Parameter containing:\n",
      "tensor([-1.3527,  0.0919], requires_grad=True)\n",
      "Epoch 101, loss = -0.2151, time: 12.5930\n",
      "reconstruction loss = 0.0685, similarity loss: 1.2817\n",
      "Parameter containing:\n",
      "tensor([-1.3480,  0.0992], requires_grad=True)\n",
      "Epoch 102, loss = -0.2635, time: 11.4738\n",
      "reconstruction loss = 0.0654, similarity loss: 1.2214\n",
      "Parameter containing:\n",
      "tensor([-1.3532,  0.0991], requires_grad=True)\n",
      "Epoch 103, loss = -0.1898, time: 11.7621\n",
      "reconstruction loss = 0.0676, similarity loss: 1.3623\n",
      "Parameter containing:\n",
      "tensor([-1.3517,  0.1111], requires_grad=True)\n",
      "Epoch 104, loss = -0.2639, time: 11.7442\n",
      "reconstruction loss = 0.0677, similarity loss: 1.1776\n",
      "Parameter containing:\n",
      "tensor([-1.3492,  0.1076], requires_grad=True)\n",
      "Epoch 105, loss = -0.2548, time: 11.4293\n",
      "reconstruction loss = 0.0680, similarity loss: 1.1951\n",
      "Parameter containing:\n",
      "tensor([-1.3477,  0.1032], requires_grad=True)\n",
      "Epoch 106, loss = -0.2474, time: 11.7964\n",
      "reconstruction loss = 0.0685, similarity loss: 1.2043\n",
      "Parameter containing:\n",
      "tensor([-1.3442,  0.1003], requires_grad=True)\n",
      "Epoch 107, loss = -0.2505, time: 11.9372\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2427\n",
      "Parameter containing:\n",
      "tensor([-1.3494,  0.1024], requires_grad=True)\n",
      "Epoch 108, loss = -0.2651, time: 11.9613\n",
      "reconstruction loss = 0.0665, similarity loss: 1.1964\n",
      "Parameter containing:\n",
      "tensor([-1.3519,  0.0990], requires_grad=True)\n",
      "Epoch 109, loss = -0.2097, time: 12.1584\n",
      "reconstruction loss = 0.0686, similarity loss: 1.2937\n",
      "Parameter containing:\n",
      "tensor([-1.3474,  0.1064], requires_grad=True)\n",
      "Epoch 110, loss = -0.2239, time: 10.5996\n",
      "reconstruction loss = 0.0698, similarity loss: 1.2377\n",
      "Parameter containing:\n",
      "tensor([-1.3413,  0.1056], requires_grad=True)\n",
      "Epoch 111, loss = -0.2592, time: 12.9808\n",
      "reconstruction loss = 0.0674, similarity loss: 1.1945\n",
      "Parameter containing:\n",
      "tensor([-1.3429,  0.1034], requires_grad=True)\n",
      "Epoch 112, loss = -0.2389, time: 11.4190\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2502\n",
      "Parameter containing:\n",
      "tensor([-1.3461,  0.1041], requires_grad=True)\n",
      "Epoch 113, loss = -0.2632, time: 13.8614\n",
      "reconstruction loss = 0.0657, similarity loss: 1.2167\n",
      "Parameter containing:\n",
      "tensor([-1.3527,  0.1027], requires_grad=True)\n",
      "Epoch 114, loss = -0.2562, time: 14.6718\n",
      "reconstruction loss = 0.0681, similarity loss: 1.1887\n",
      "Parameter containing:\n",
      "tensor([-1.3487,  0.0978], requires_grad=True)\n",
      "Epoch 115, loss = -0.2939, time: 12.5404\n",
      "reconstruction loss = 0.0662, similarity loss: 1.1336\n",
      "Parameter containing:\n",
      "tensor([-1.3521,  0.0909], requires_grad=True)\n",
      "Epoch 116, loss = -0.2554, time: 13.9373\n",
      "reconstruction loss = 0.0668, similarity loss: 1.2145\n",
      "Parameter containing:\n",
      "tensor([-1.3523,  0.0927], requires_grad=True)\n",
      "Epoch 117, loss = -0.2733, time: 13.0988\n",
      "reconstruction loss = 0.0646, similarity loss: 1.2110\n",
      "Parameter containing:\n",
      "tensor([-1.3576,  0.0939], requires_grad=True)\n",
      "Epoch 118, loss = -0.2343, time: 13.6382\n",
      "reconstruction loss = 0.0691, similarity loss: 1.2239\n",
      "Parameter containing:\n",
      "tensor([-1.3488,  0.0960], requires_grad=True)\n",
      "Epoch 119, loss = -0.2290, time: 12.9044\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2648\n",
      "Parameter containing:\n",
      "tensor([-1.3479,  0.1005], requires_grad=True)\n",
      "Epoch 120, loss = -0.2568, time: 12.4837\n",
      "reconstruction loss = 0.0656, similarity loss: 1.2339\n",
      "Parameter containing:\n",
      "tensor([-1.3552,  0.1017], requires_grad=True)\n",
      "Epoch 121, loss = -0.2453, time: 13.4397\n",
      "reconstruction loss = 0.0697, similarity loss: 1.1863\n",
      "Parameter containing:\n",
      "tensor([-1.3457,  0.0982], requires_grad=True)\n",
      "Epoch 122, loss = -0.2601, time: 13.7102\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2194\n",
      "Parameter containing:\n",
      "tensor([-1.3506,  0.1004], requires_grad=True)\n",
      "Epoch 123, loss = -0.2371, time: 14.3210\n",
      "reconstruction loss = 0.0662, similarity loss: 1.2716\n",
      "Parameter containing:\n",
      "tensor([-1.3533,  0.1044], requires_grad=True)\n",
      "Epoch 124, loss = -0.2582, time: 13.9075\n",
      "reconstruction loss = 0.0678, similarity loss: 1.1895\n",
      "Parameter containing:\n",
      "tensor([-1.3512,  0.1008], requires_grad=True)\n",
      "Epoch 125, loss = -0.2243, time: 13.3821\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2865\n",
      "Parameter containing:\n",
      "tensor([-1.3506,  0.1057], requires_grad=True)\n",
      "Epoch 126, loss = -0.2397, time: 16.0186\n",
      "reconstruction loss = 0.0654, similarity loss: 1.2798\n",
      "Parameter containing:\n",
      "tensor([-1.3559,  0.1106], requires_grad=True)\n",
      "Epoch 127, loss = -0.2130, time: 17.9688\n",
      "reconstruction loss = 0.0695, similarity loss: 1.2708\n",
      "Parameter containing:\n",
      "tensor([-1.3467,  0.1133], requires_grad=True)\n",
      "Epoch 128, loss = -0.2350, time: 19.0169\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2561\n",
      "Parameter containing:\n",
      "tensor([-1.3465,  0.1121], requires_grad=True)\n",
      "Epoch 129, loss = -0.2352, time: 17.3501\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2580\n",
      "Parameter containing:\n",
      "tensor([-1.3488,  0.1129], requires_grad=True)\n",
      "Epoch 130, loss = -0.2307, time: 16.3166\n",
      "reconstruction loss = 0.0675, similarity loss: 1.2646\n",
      "Parameter containing:\n",
      "tensor([-1.3492,  0.1153], requires_grad=True)\n",
      "Epoch 131, loss = -0.2160, time: 17.6704\n",
      "reconstruction loss = 0.0691, similarity loss: 1.2709\n",
      "Parameter containing:\n",
      "tensor([-1.3446,  0.1140], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, loss = -0.2078, time: 17.2203\n",
      "reconstruction loss = 0.0689, similarity loss: 1.2954\n",
      "Parameter containing:\n",
      "tensor([-1.3415,  0.1199], requires_grad=True)\n",
      "Epoch 133, loss = -0.2518, time: 17.7553\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2058\n",
      "Parameter containing:\n",
      "tensor([-1.3431,  0.1148], requires_grad=True)\n",
      "Epoch 134, loss = -0.2428, time: 18.0587\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2263\n",
      "Parameter containing:\n",
      "tensor([-1.3427,  0.1099], requires_grad=True)\n",
      "Epoch 135, loss = -0.2297, time: 18.1474\n",
      "reconstruction loss = 0.0687, similarity loss: 1.2440\n",
      "Parameter containing:\n",
      "tensor([-1.3401,  0.1121], requires_grad=True)\n",
      "Epoch 136, loss = -0.2283, time: 18.3518\n",
      "reconstruction loss = 0.0697, similarity loss: 1.2299\n",
      "Parameter containing:\n",
      "tensor([-1.3379,  0.1090], requires_grad=True)\n",
      "Epoch 137, loss = -0.2617, time: 17.4683\n",
      "reconstruction loss = 0.0665, similarity loss: 1.2041\n",
      "Parameter containing:\n",
      "tensor([-1.3446,  0.1037], requires_grad=True)\n",
      "Epoch 138, loss = -0.2802, time: 15.4875\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1467\n",
      "Parameter containing:\n",
      "tensor([-1.3462,  0.0959], requires_grad=True)\n",
      "Epoch 139, loss = -0.2461, time: 14.3798\n",
      "reconstruction loss = 0.0675, similarity loss: 1.2261\n",
      "Parameter containing:\n",
      "tensor([-1.3479,  0.0985], requires_grad=True)\n",
      "Epoch 140, loss = -0.2171, time: 13.2752\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2936\n",
      "Parameter containing:\n",
      "tensor([-1.3468,  0.1055], requires_grad=True)\n",
      "Epoch 141, loss = -0.2419, time: 12.1427\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2644\n",
      "Parameter containing:\n",
      "tensor([-1.3512,  0.1082], requires_grad=True)\n",
      "Epoch 142, loss = -0.2447, time: 13.4825\n",
      "reconstruction loss = 0.0661, similarity loss: 1.2556\n",
      "Parameter containing:\n",
      "tensor([-1.3551,  0.1102], requires_grad=True)\n",
      "Epoch 143, loss = -0.2436, time: 11.8886\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2300\n",
      "Parameter containing:\n",
      "tensor([-1.3522,  0.1090], requires_grad=True)\n",
      "Epoch 144, loss = -0.2612, time: 15.2503\n",
      "reconstruction loss = 0.0658, similarity loss: 1.2199\n",
      "Parameter containing:\n",
      "tensor([-1.3554,  0.1084], requires_grad=True)\n",
      "Epoch 145, loss = -0.2650, time: 12.3813\n",
      "reconstruction loss = 0.0685, similarity loss: 1.1605\n",
      "Parameter containing:\n",
      "tensor([-1.3492,  0.0974], requires_grad=True)\n",
      "Epoch 146, loss = -0.2529, time: 13.1355\n",
      "reconstruction loss = 0.0665, similarity loss: 1.2267\n",
      "Parameter containing:\n",
      "tensor([-1.3512,  0.0994], requires_grad=True)\n",
      "Epoch 147, loss = -0.2715, time: 13.1000\n",
      "reconstruction loss = 0.0657, similarity loss: 1.1961\n",
      "Parameter containing:\n",
      "tensor([-1.3555,  0.0958], requires_grad=True)\n",
      "Epoch 148, loss = -0.2290, time: 12.7599\n",
      "reconstruction loss = 0.0687, similarity loss: 1.2451\n",
      "Parameter containing:\n",
      "tensor([-1.3497,  0.0984], requires_grad=True)\n",
      "Epoch 149, loss = -0.2742, time: 12.5484\n",
      "reconstruction loss = 0.0641, similarity loss: 1.2181\n",
      "Parameter containing:\n",
      "tensor([-1.3582,  0.0984], requires_grad=True)\n",
      "Epoch 150, loss = -0.2616, time: 14.0187\n",
      "reconstruction loss = 0.0662, similarity loss: 1.2106\n",
      "Parameter containing:\n",
      "tensor([-1.3600,  0.0988], requires_grad=True)\n",
      "Epoch 151, loss = -0.2674, time: 13.6617\n",
      "reconstruction loss = 0.0664, similarity loss: 1.1942\n",
      "Parameter containing:\n",
      "tensor([-1.3578,  0.0968], requires_grad=True)\n",
      "Epoch 152, loss = -0.2674, time: 13.6689\n",
      "reconstruction loss = 0.0676, similarity loss: 1.1716\n",
      "Parameter containing:\n",
      "tensor([-1.3544,  0.0916], requires_grad=True)\n",
      "Epoch 153, loss = -0.2631, time: 12.6307\n",
      "reconstruction loss = 0.0667, similarity loss: 1.1993\n",
      "Parameter containing:\n",
      "tensor([-1.3555,  0.0937], requires_grad=True)\n",
      "Epoch 154, loss = -0.2554, time: 14.0177\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2059\n",
      "Parameter containing:\n",
      "tensor([-1.3508,  0.0938], requires_grad=True)\n",
      "Epoch 155, loss = -0.2727, time: 12.7778\n",
      "reconstruction loss = 0.0680, similarity loss: 1.1520\n",
      "Parameter containing:\n",
      "tensor([-1.3489,  0.0859], requires_grad=True)\n",
      "Epoch 156, loss = -0.2459, time: 13.4415\n",
      "reconstruction loss = 0.0662, similarity loss: 1.2478\n",
      "Parameter containing:\n",
      "tensor([-1.3526,  0.0922], requires_grad=True)\n",
      "Epoch 157, loss = -0.2584, time: 13.4567\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2007\n",
      "Parameter containing:\n",
      "tensor([-1.3515,  0.0929], requires_grad=True)\n",
      "Epoch 158, loss = -0.1826, time: 13.1942\n",
      "reconstruction loss = 0.0701, similarity loss: 1.3325\n",
      "Parameter containing:\n",
      "tensor([-1.3430,  0.1025], requires_grad=True)\n",
      "Epoch 159, loss = -0.1836, time: 14.3098\n",
      "reconstruction loss = 0.0673, similarity loss: 1.3842\n",
      "Parameter containing:\n",
      "tensor([-1.3436,  0.1180], requires_grad=True)\n",
      "Epoch 160, loss = -0.2654, time: 12.4300\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1812\n",
      "Parameter containing:\n",
      "tensor([-1.3475,  0.1108], requires_grad=True)\n",
      "Epoch 161, loss = -0.2496, time: 12.6154\n",
      "reconstruction loss = 0.0684, similarity loss: 1.1991\n",
      "Parameter containing:\n",
      "tensor([-1.3445,  0.1061], requires_grad=True)\n",
      "Epoch 162, loss = -0.2857, time: 13.5684\n",
      "reconstruction loss = 0.0667, similarity loss: 1.1419\n",
      "Parameter containing:\n",
      "tensor([-1.3466,  0.0978], requires_grad=True)\n",
      "Epoch 163, loss = -0.2047, time: 13.4507\n",
      "reconstruction loss = 0.0686, similarity loss: 1.3067\n",
      "Parameter containing:\n",
      "tensor([-1.3453,  0.1047], requires_grad=True)\n",
      "Epoch 164, loss = -0.2185, time: 13.5510\n",
      "reconstruction loss = 0.0690, similarity loss: 1.2663\n",
      "Parameter containing:\n",
      "tensor([-1.3426,  0.1104], requires_grad=True)\n",
      "Epoch 165, loss = -0.2193, time: 13.9814\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2865\n",
      "Parameter containing:\n",
      "tensor([-1.3429,  0.1132], requires_grad=True)\n",
      "Epoch 166, loss = -0.2324, time: 13.3199\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2524\n",
      "Parameter containing:\n",
      "tensor([-1.3432,  0.1136], requires_grad=True)\n",
      "Epoch 167, loss = -0.2311, time: 13.6372\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2378\n",
      "Parameter containing:\n",
      "tensor([-1.3412,  0.1124], requires_grad=True)\n",
      "Epoch 168, loss = -0.2544, time: 13.1334\n",
      "reconstruction loss = 0.0668, similarity loss: 1.2167\n",
      "Parameter containing:\n",
      "tensor([-1.3461,  0.1086], requires_grad=True)\n",
      "Epoch 169, loss = -0.2756, time: 12.5186\n",
      "reconstruction loss = 0.0675, similarity loss: 1.1516\n",
      "Parameter containing:\n",
      "tensor([-1.3467,  0.1010], requires_grad=True)\n",
      "Epoch 170, loss = -0.2379, time: 12.7931\n",
      "reconstruction loss = 0.0705, similarity loss: 1.1911\n",
      "Parameter containing:\n",
      "tensor([-1.3380,  0.0989], requires_grad=True)\n",
      "Epoch 171, loss = -0.2410, time: 13.2833\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2151\n",
      "Parameter containing:\n",
      "tensor([-1.3389,  0.0988], requires_grad=True)\n",
      "Epoch 172, loss = -0.2676, time: 11.1885\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2022\n",
      "Parameter containing:\n",
      "tensor([-1.3466,  0.0957], requires_grad=True)\n",
      "Epoch 173, loss = -0.2166, time: 10.2340\n",
      "reconstruction loss = 0.0697, similarity loss: 1.2575\n",
      "Parameter containing:\n",
      "tensor([-1.3411,  0.1000], requires_grad=True)\n",
      "Epoch 174, loss = -0.2587, time: 11.7533\n",
      "reconstruction loss = 0.0680, similarity loss: 1.1851\n",
      "Parameter containing:\n",
      "tensor([-1.3418,  0.0975], requires_grad=True)\n",
      "Epoch 175, loss = -0.2563, time: 12.8632\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2150\n",
      "Parameter containing:\n",
      "tensor([-1.3462,  0.0964], requires_grad=True)\n",
      "Epoch 176, loss = -0.2065, time: 12.6316\n",
      "reconstruction loss = 0.0698, similarity loss: 1.2799\n",
      "Parameter containing:\n",
      "tensor([-1.3414,  0.1015], requires_grad=True)\n",
      "Epoch 177, loss = -0.2557, time: 12.3501\n",
      "reconstruction loss = 0.0670, similarity loss: 1.2104\n",
      "Parameter containing:\n",
      "tensor([-1.3430,  0.1022], requires_grad=True)\n",
      "Epoch 178, loss = -0.2420, time: 12.6561\n",
      "reconstruction loss = 0.0665, similarity loss: 1.2537\n",
      "Parameter containing:\n",
      "tensor([-1.3486,  0.1044], requires_grad=True)\n",
      "Epoch 179, loss = -0.2599, time: 12.4624\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1974\n",
      "Parameter containing:\n",
      "tensor([-1.3515,  0.1002], requires_grad=True)\n",
      "Epoch 180, loss = -0.2303, time: 12.9051\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2501\n",
      "Parameter containing:\n",
      "tensor([-1.3471,  0.1028], requires_grad=True)\n",
      "Epoch 181, loss = -0.2493, time: 12.4074\n",
      "reconstruction loss = 0.0696, similarity loss: 1.1793\n",
      "Parameter containing:\n",
      "tensor([-1.3401,  0.0979], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, loss = -0.2334, time: 11.3481\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2486\n",
      "Parameter containing:\n",
      "tensor([-1.3429,  0.0995], requires_grad=True)\n",
      "Epoch 183, loss = -0.2527, time: 10.8022\n",
      "reconstruction loss = 0.0654, similarity loss: 1.2469\n",
      "Parameter containing:\n",
      "tensor([-1.3498,  0.1025], requires_grad=True)\n",
      "Epoch 184, loss = -0.2414, time: 12.5463\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2334\n",
      "Parameter containing:\n",
      "tensor([-1.3496,  0.1047], requires_grad=True)\n",
      "Epoch 185, loss = -0.2299, time: 12.4245\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2502\n",
      "Parameter containing:\n",
      "tensor([-1.3468,  0.1049], requires_grad=True)\n",
      "Epoch 186, loss = -0.2252, time: 12.4299\n",
      "reconstruction loss = 0.0664, similarity loss: 1.2978\n",
      "Parameter containing:\n",
      "tensor([-1.3492,  0.1112], requires_grad=True)\n",
      "Epoch 187, loss = -0.2469, time: 12.0044\n",
      "reconstruction loss = 0.0647, similarity loss: 1.2751\n",
      "Parameter containing:\n",
      "tensor([-1.3582,  0.1117], requires_grad=True)\n",
      "Epoch 188, loss = -0.2497, time: 10.2867\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2010\n",
      "Parameter containing:\n",
      "tensor([-1.3503,  0.1087], requires_grad=True)\n",
      "Epoch 189, loss = -0.2099, time: 11.5936\n",
      "reconstruction loss = 0.0670, similarity loss: 1.3245\n",
      "Parameter containing:\n",
      "tensor([-1.3522,  0.1163], requires_grad=True)\n",
      "Epoch 190, loss = -0.2051, time: 12.6163\n",
      "reconstruction loss = 0.0673, similarity loss: 1.3315\n",
      "Parameter containing:\n",
      "tensor([-1.3503,  0.1224], requires_grad=True)\n",
      "Epoch 191, loss = -0.2595, time: 12.0840\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1954\n",
      "Parameter containing:\n",
      "tensor([-1.3514,  0.1162], requires_grad=True)\n",
      "Epoch 192, loss = -0.2203, time: 12.3530\n",
      "reconstruction loss = 0.0690, similarity loss: 1.2619\n",
      "Parameter containing:\n",
      "tensor([-1.3475,  0.1142], requires_grad=True)\n",
      "Epoch 193, loss = -0.2681, time: 11.8994\n",
      "reconstruction loss = 0.0664, similarity loss: 1.1913\n",
      "Parameter containing:\n",
      "tensor([-1.3491,  0.1099], requires_grad=True)\n",
      "Epoch 194, loss = -0.2604, time: 11.8283\n",
      "reconstruction loss = 0.0684, similarity loss: 1.1738\n",
      "Parameter containing:\n",
      "tensor([-1.3467,  0.1032], requires_grad=True)\n",
      "Epoch 195, loss = -0.2285, time: 11.1026\n",
      "reconstruction loss = 0.0691, similarity loss: 1.2406\n",
      "Parameter containing:\n",
      "tensor([-1.3431,  0.1029], requires_grad=True)\n",
      "Epoch 196, loss = -0.2321, time: 11.8426\n",
      "reconstruction loss = 0.0675, similarity loss: 1.2605\n",
      "Parameter containing:\n",
      "tensor([-1.3443,  0.1057], requires_grad=True)\n",
      "Epoch 197, loss = -0.2300, time: 10.4284\n",
      "reconstruction loss = 0.0663, similarity loss: 1.2866\n",
      "Parameter containing:\n",
      "tensor([-1.3484,  0.1113], requires_grad=True)\n",
      "Epoch 198, loss = -0.2073, time: 11.1672\n",
      "reconstruction loss = 0.0704, similarity loss: 1.2685\n",
      "Parameter containing:\n",
      "tensor([-1.3407,  0.1139], requires_grad=True)\n",
      "Epoch 199, loss = -0.2657, time: 11.7965\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2063\n",
      "Parameter containing:\n",
      "tensor([-1.3468,  0.1093], requires_grad=True)\n",
      "Epoch 200, loss = -0.2380, time: 12.2374\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2498\n",
      "Parameter containing:\n",
      "tensor([-1.3483,  0.1089], requires_grad=True)\n",
      "Epoch 201, loss = -0.2410, time: 11.6797\n",
      "reconstruction loss = 0.0675, similarity loss: 1.2384\n",
      "Parameter containing:\n",
      "tensor([-1.3484,  0.1080], requires_grad=True)\n",
      "Epoch 202, loss = -0.2200, time: 11.7325\n",
      "reconstruction loss = 0.0696, similarity loss: 1.2522\n",
      "Parameter containing:\n",
      "tensor([-1.3414,  0.1093], requires_grad=True)\n",
      "Epoch 203, loss = -0.2260, time: 11.9939\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2801\n",
      "Parameter containing:\n",
      "tensor([-1.3445,  0.1120], requires_grad=True)\n",
      "Epoch 204, loss = -0.2565, time: 11.8542\n",
      "reconstruction loss = 0.0660, similarity loss: 1.2273\n",
      "Parameter containing:\n",
      "tensor([-1.3507,  0.1124], requires_grad=True)\n",
      "Epoch 205, loss = -0.2687, time: 12.2541\n",
      "reconstruction loss = 0.0661, similarity loss: 1.1942\n",
      "Parameter containing:\n",
      "tensor([-1.3525,  0.1061], requires_grad=True)\n",
      "Epoch 206, loss = -0.2370, time: 11.7725\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2445\n",
      "Parameter containing:\n",
      "tensor([-1.3504,  0.1070], requires_grad=True)\n",
      "Epoch 207, loss = -0.2383, time: 11.8507\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2394\n",
      "Parameter containing:\n",
      "tensor([-1.3471,  0.1078], requires_grad=True)\n",
      "Epoch 208, loss = -0.2385, time: 12.4264\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2390\n",
      "Parameter containing:\n",
      "tensor([-1.3477,  0.1070], requires_grad=True)\n",
      "Epoch 209, loss = -0.2583, time: 11.4543\n",
      "reconstruction loss = 0.0677, similarity loss: 1.1915\n",
      "Parameter containing:\n",
      "tensor([-1.3456,  0.1032], requires_grad=True)\n",
      "Epoch 210, loss = -0.2258, time: 12.2906\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2789\n",
      "Parameter containing:\n",
      "tensor([-1.3471,  0.1057], requires_grad=True)\n",
      "Epoch 211, loss = -0.2591, time: 11.7253\n",
      "reconstruction loss = 0.0661, similarity loss: 1.2198\n",
      "Parameter containing:\n",
      "tensor([-1.3525,  0.1063], requires_grad=True)\n",
      "Epoch 212, loss = -0.2644, time: 11.9416\n",
      "reconstruction loss = 0.0675, similarity loss: 1.1796\n",
      "Parameter containing:\n",
      "tensor([-1.3502,  0.0998], requires_grad=True)\n",
      "Epoch 213, loss = -0.2292, time: 11.7335\n",
      "reconstruction loss = 0.0679, similarity loss: 1.2591\n",
      "Parameter containing:\n",
      "tensor([-1.3479,  0.1029], requires_grad=True)\n",
      "Epoch 214, loss = -0.2682, time: 11.1370\n",
      "reconstruction loss = 0.0657, similarity loss: 1.2034\n",
      "Parameter containing:\n",
      "tensor([-1.3533,  0.1016], requires_grad=True)\n",
      "Epoch 215, loss = -0.1955, time: 11.4403\n",
      "reconstruction loss = 0.0681, similarity loss: 1.3395\n",
      "Parameter containing:\n",
      "tensor([-1.3494,  0.1122], requires_grad=True)\n",
      "Epoch 216, loss = -0.2452, time: 10.3453\n",
      "reconstruction loss = 0.0652, similarity loss: 1.2710\n",
      "Parameter containing:\n",
      "tensor([-1.3535,  0.1129], requires_grad=True)\n",
      "Epoch 217, loss = -0.2304, time: 11.4846\n",
      "reconstruction loss = 0.0701, similarity loss: 1.2158\n",
      "Parameter containing:\n",
      "tensor([-1.3463,  0.1102], requires_grad=True)\n",
      "Epoch 218, loss = -0.2577, time: 12.0098\n",
      "reconstruction loss = 0.0665, similarity loss: 1.2153\n",
      "Parameter containing:\n",
      "tensor([-1.3491,  0.1078], requires_grad=True)\n",
      "Epoch 219, loss = -0.2284, time: 11.5798\n",
      "reconstruction loss = 0.0678, similarity loss: 1.2629\n",
      "Parameter containing:\n",
      "tensor([-1.3481,  0.1100], requires_grad=True)\n",
      "Epoch 220, loss = -0.2554, time: 12.1948\n",
      "reconstruction loss = 0.0687, similarity loss: 1.1803\n",
      "Parameter containing:\n",
      "tensor([-1.3434,  0.1039], requires_grad=True)\n",
      "Epoch 221, loss = -0.2307, time: 12.6966\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2593\n",
      "Parameter containing:\n",
      "tensor([-1.3464,  0.1041], requires_grad=True)\n",
      "Epoch 222, loss = -0.2865, time: 12.0122\n",
      "reconstruction loss = 0.0674, similarity loss: 1.1289\n",
      "Parameter containing:\n",
      "tensor([-1.3460,  0.0950], requires_grad=True)\n",
      "Epoch 223, loss = -0.2688, time: 12.2030\n",
      "reconstruction loss = 0.0660, similarity loss: 1.1981\n",
      "Parameter containing:\n",
      "tensor([-1.3513,  0.0935], requires_grad=True)\n",
      "Epoch 224, loss = -0.2479, time: 11.6148\n",
      "reconstruction loss = 0.0677, similarity loss: 1.2162\n",
      "Parameter containing:\n",
      "tensor([-1.3497,  0.0956], requires_grad=True)\n",
      "Epoch 225, loss = -0.2523, time: 12.0738\n",
      "reconstruction loss = 0.0671, similarity loss: 1.2177\n",
      "Parameter containing:\n",
      "tensor([-1.3494,  0.0971], requires_grad=True)\n",
      "Epoch 226, loss = -0.2356, time: 11.8489\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2493\n",
      "Parameter containing:\n",
      "tensor([-1.3491,  0.1000], requires_grad=True)\n",
      "Epoch 227, loss = -0.2627, time: 11.1226\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1898\n",
      "Parameter containing:\n",
      "tensor([-1.3479,  0.0969], requires_grad=True)\n",
      "Epoch 228, loss = -0.2185, time: 11.5002\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2900\n",
      "Parameter containing:\n",
      "tensor([-1.3494,  0.1029], requires_grad=True)\n",
      "Epoch 229, loss = -0.2435, time: 10.3222\n",
      "reconstruction loss = 0.0700, similarity loss: 1.1867\n",
      "Parameter containing:\n",
      "tensor([-1.3423,  0.0995], requires_grad=True)\n",
      "Epoch 230, loss = -0.2635, time: 10.2159\n",
      "reconstruction loss = 0.0660, similarity loss: 1.2096\n",
      "Parameter containing:\n",
      "tensor([-1.3466,  0.0978], requires_grad=True)\n",
      "Epoch 231, loss = -0.2332, time: 11.0187\n",
      "reconstruction loss = 0.0682, similarity loss: 1.2438\n",
      "Parameter containing:\n",
      "tensor([-1.3454,  0.1002], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, loss = -0.2429, time: 10.6880\n",
      "reconstruction loss = 0.0691, similarity loss: 1.2040\n",
      "Parameter containing:\n",
      "tensor([-1.3405,  0.1000], requires_grad=True)\n",
      "Epoch 233, loss = -0.2454, time: 11.1300\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2261\n",
      "Parameter containing:\n",
      "tensor([-1.3443,  0.1003], requires_grad=True)\n",
      "Epoch 234, loss = -0.2299, time: 11.2264\n",
      "reconstruction loss = 0.0669, similarity loss: 1.2759\n",
      "Parameter containing:\n",
      "tensor([-1.3491,  0.1041], requires_grad=True)\n",
      "Epoch 235, loss = -0.2612, time: 10.2155\n",
      "reconstruction loss = 0.0653, similarity loss: 1.2282\n",
      "Parameter containing:\n",
      "tensor([-1.3540,  0.1039], requires_grad=True)\n",
      "Epoch 236, loss = -0.2386, time: 11.6223\n",
      "reconstruction loss = 0.0689, similarity loss: 1.2187\n",
      "Parameter containing:\n",
      "tensor([-1.3474,  0.1033], requires_grad=True)\n",
      "Epoch 237, loss = -0.2320, time: 11.4161\n",
      "reconstruction loss = 0.0676, similarity loss: 1.2585\n",
      "Parameter containing:\n",
      "tensor([-1.3473,  0.1072], requires_grad=True)\n",
      "Epoch 238, loss = -0.2510, time: 13.2294\n",
      "reconstruction loss = 0.0658, similarity loss: 1.2453\n",
      "Parameter containing:\n",
      "tensor([-1.3529,  0.1080], requires_grad=True)\n",
      "Epoch 239, loss = -0.2725, time: 13.5946\n",
      "reconstruction loss = 0.0682, similarity loss: 1.1484\n",
      "Parameter containing:\n",
      "tensor([-1.3481,  0.0981], requires_grad=True)\n",
      "Epoch 240, loss = -0.2267, time: 14.3862\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2579\n",
      "Parameter containing:\n",
      "tensor([-1.3453,  0.1001], requires_grad=True)\n",
      "Epoch 241, loss = -0.2109, time: 13.5313\n",
      "reconstruction loss = 0.0678, similarity loss: 1.3073\n",
      "Parameter containing:\n",
      "tensor([-1.3456,  0.1094], requires_grad=True)\n",
      "Epoch 242, loss = -0.2443, time: 12.3936\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2333\n",
      "Parameter containing:\n",
      "tensor([-1.3463,  0.1087], requires_grad=True)\n",
      "Epoch 243, loss = -0.2503, time: 12.2976\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2305\n",
      "Parameter containing:\n",
      "tensor([-1.3519,  0.1072], requires_grad=True)\n",
      "Epoch 244, loss = -0.2399, time: 13.8701\n",
      "reconstruction loss = 0.0690, similarity loss: 1.2129\n",
      "Parameter containing:\n",
      "tensor([-1.3447,  0.1045], requires_grad=True)\n",
      "Epoch 245, loss = -0.1991, time: 13.8555\n",
      "reconstruction loss = 0.0714, similarity loss: 1.2708\n",
      "Parameter containing:\n",
      "tensor([-1.3356,  0.1081], requires_grad=True)\n",
      "Epoch 246, loss = -0.2557, time: 13.2749\n",
      "reconstruction loss = 0.0663, similarity loss: 1.2217\n",
      "Parameter containing:\n",
      "tensor([-1.3416,  0.1046], requires_grad=True)\n",
      "Epoch 247, loss = -0.2489, time: 11.6471\n",
      "reconstruction loss = 0.0691, similarity loss: 1.1893\n",
      "Parameter containing:\n",
      "tensor([-1.3401,  0.1033], requires_grad=True)\n",
      "Epoch 248, loss = -0.2919, time: 13.1025\n",
      "reconstruction loss = 0.0679, similarity loss: 1.1070\n",
      "Parameter containing:\n",
      "tensor([-1.3415,  0.0919], requires_grad=True)\n",
      "Epoch 249, loss = -0.2656, time: 12.5794\n",
      "reconstruction loss = 0.0684, similarity loss: 1.1625\n",
      "Parameter containing:\n",
      "tensor([-1.3425,  0.0878], requires_grad=True)\n",
      "Epoch 250, loss = -0.2713, time: 12.7645\n",
      "reconstruction loss = 0.0663, similarity loss: 1.1865\n",
      "Parameter containing:\n",
      "tensor([-1.3476,  0.0878], requires_grad=True)\n",
      "Epoch 251, loss = -0.2394, time: 13.9817\n",
      "reconstruction loss = 0.0681, similarity loss: 1.2297\n",
      "Parameter containing:\n",
      "tensor([-1.3454,  0.0876], requires_grad=True)\n",
      "Epoch 252, loss = -0.2658, time: 13.7591\n",
      "reconstruction loss = 0.0674, similarity loss: 1.1799\n",
      "Parameter containing:\n",
      "tensor([-1.3481,  0.0899], requires_grad=True)\n",
      "Epoch 253, loss = -0.2026, time: 11.7164\n",
      "reconstruction loss = 0.0675, similarity loss: 1.3299\n",
      "Parameter containing:\n",
      "tensor([-1.3471,  0.1011], requires_grad=True)\n",
      "Epoch 254, loss = -0.2779, time: 11.0812\n",
      "reconstruction loss = 0.0653, similarity loss: 1.1876\n",
      "Parameter containing:\n",
      "tensor([-1.3517,  0.1003], requires_grad=True)\n",
      "Epoch 255, loss = -0.2728, time: 11.9759\n",
      "reconstruction loss = 0.0672, similarity loss: 1.1651\n",
      "Parameter containing:\n",
      "tensor([-1.3533,  0.0922], requires_grad=True)\n",
      "Epoch 256, loss = -0.2509, time: 10.7712\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2168\n",
      "Parameter containing:\n",
      "tensor([-1.3528,  0.0935], requires_grad=True)\n",
      "Epoch 257, loss = -0.2454, time: 10.2660\n",
      "reconstruction loss = 0.0664, similarity loss: 1.2460\n",
      "Parameter containing:\n",
      "tensor([-1.3527,  0.0989], requires_grad=True)\n",
      "Epoch 258, loss = -0.2314, time: 11.3687\n",
      "reconstruction loss = 0.0686, similarity loss: 1.2409\n",
      "Parameter containing:\n",
      "tensor([-1.3480,  0.1003], requires_grad=True)\n",
      "Epoch 259, loss = -0.2160, time: 10.6196\n",
      "reconstruction loss = 0.0673, similarity loss: 1.3034\n",
      "Parameter containing:\n",
      "tensor([-1.3473,  0.1079], requires_grad=True)\n",
      "Epoch 260, loss = -0.2543, time: 10.8841\n",
      "reconstruction loss = 0.0687, similarity loss: 1.1822\n",
      "Parameter containing:\n",
      "tensor([-1.3455,  0.1040], requires_grad=True)\n",
      "Epoch 261, loss = -0.2576, time: 10.7170\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2031\n",
      "Parameter containing:\n",
      "tensor([-1.3475,  0.1005], requires_grad=True)\n",
      "Epoch 262, loss = -0.2358, time: 10.5553\n",
      "reconstruction loss = 0.0684, similarity loss: 1.2340\n",
      "Parameter containing:\n",
      "tensor([-1.3459,  0.1020], requires_grad=True)\n",
      "Epoch 263, loss = -0.2525, time: 10.9583\n",
      "reconstruction loss = 0.0672, similarity loss: 1.2151\n",
      "Parameter containing:\n",
      "tensor([-1.3458,  0.1008], requires_grad=True)\n",
      "Epoch 264, loss = -0.1775, time: 10.4885\n",
      "reconstruction loss = 0.0696, similarity loss: 1.3570\n",
      "Parameter containing:\n",
      "tensor([-1.3425,  0.1131], requires_grad=True)\n",
      "Epoch 265, loss = -0.2144, time: 10.4517\n",
      "reconstruction loss = 0.0688, similarity loss: 1.2804\n",
      "Parameter containing:\n",
      "tensor([-1.3409,  0.1156], requires_grad=True)\n",
      "Epoch 266, loss = -0.2310, time: 12.0290\n",
      "reconstruction loss = 0.0651, similarity loss: 1.3080\n",
      "Parameter containing:\n",
      "tensor([-1.3488,  0.1217], requires_grad=True)\n",
      "Epoch 267, loss = -0.2287, time: 10.5260\n",
      "reconstruction loss = 0.0680, similarity loss: 1.2590\n",
      "Parameter containing:\n",
      "tensor([-1.3480,  0.1195], requires_grad=True)\n",
      "Epoch 268, loss = -0.1945, time: 10.7251\n",
      "reconstruction loss = 0.0693, similarity loss: 1.3219\n",
      "Parameter containing:\n",
      "tensor([-1.3431,  0.1235], requires_grad=True)\n",
      "Epoch 269, loss = -0.2345, time: 10.8567\n",
      "reconstruction loss = 0.0681, similarity loss: 1.2416\n",
      "Parameter containing:\n",
      "tensor([-1.3437,  0.1199], requires_grad=True)\n",
      "Epoch 270, loss = -0.2087, time: 12.8741\n",
      "reconstruction loss = 0.0707, similarity loss: 1.2607\n",
      "Parameter containing:\n",
      "tensor([-1.3372,  0.1191], requires_grad=True)\n",
      "Epoch 271, loss = -0.2295, time: 13.4137\n",
      "reconstruction loss = 0.0683, similarity loss: 1.2518\n",
      "Parameter containing:\n",
      "tensor([-1.3369,  0.1175], requires_grad=True)\n",
      "Epoch 272, loss = -0.2125, time: 13.1172\n",
      "reconstruction loss = 0.0658, similarity loss: 1.3418\n",
      "Parameter containing:\n",
      "tensor([-1.3455,  0.1249], requires_grad=True)\n",
      "Epoch 273, loss = -0.1985, time: 13.1691\n",
      "reconstruction loss = 0.0684, similarity loss: 1.3298\n",
      "Parameter containing:\n",
      "tensor([-1.3451,  0.1293], requires_grad=True)\n",
      "Epoch 274, loss = -0.2648, time: 13.1985\n",
      "reconstruction loss = 0.0677, similarity loss: 1.1725\n",
      "Parameter containing:\n",
      "tensor([-1.3462,  0.1175], requires_grad=True)\n",
      "Epoch 275, loss = -0.2097, time: 12.8103\n",
      "reconstruction loss = 0.0701, similarity loss: 1.2690\n",
      "Parameter containing:\n",
      "tensor([-1.3388,  0.1189], requires_grad=True)\n",
      "Epoch 276, loss = -0.2642, time: 13.2628\n",
      "reconstruction loss = 0.0701, similarity loss: 1.1317\n",
      "Parameter containing:\n",
      "tensor([-1.3344,  0.1079], requires_grad=True)\n",
      "Epoch 277, loss = -0.2442, time: 12.8512\n",
      "reconstruction loss = 0.0668, similarity loss: 1.2428\n",
      "Parameter containing:\n",
      "tensor([-1.3408,  0.1050], requires_grad=True)\n",
      "Epoch 278, loss = -0.2369, time: 13.0259\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2623\n",
      "Parameter containing:\n",
      "tensor([-1.3474,  0.1089], requires_grad=True)\n",
      "Epoch 279, loss = -0.2553, time: 12.1548\n",
      "reconstruction loss = 0.0662, similarity loss: 1.2270\n",
      "Parameter containing:\n",
      "tensor([-1.3516,  0.1076], requires_grad=True)\n",
      "Epoch 280, loss = -0.2365, time: 12.8396\n",
      "reconstruction loss = 0.0687, similarity loss: 1.2267\n",
      "Parameter containing:\n",
      "tensor([-1.3476,  0.1066], requires_grad=True)\n",
      "Epoch 281, loss = -0.2390, time: 12.7118\n",
      "reconstruction loss = 0.0667, similarity loss: 1.2567\n",
      "Parameter containing:\n",
      "tensor([-1.3477,  0.1082], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282, loss = -0.2471, time: 11.1461\n",
      "reconstruction loss = 0.0682, similarity loss: 1.2107\n",
      "Parameter containing:\n",
      "tensor([-1.3464,  0.1056], requires_grad=True)\n",
      "Epoch 283, loss = -0.2205, time: 13.4938\n",
      "reconstruction loss = 0.0673, similarity loss: 1.2916\n",
      "Parameter containing:\n",
      "tensor([-1.3473,  0.1107], requires_grad=True)\n",
      "Epoch 284, loss = -0.2037, time: 14.7363\n",
      "reconstruction loss = 0.0683, similarity loss: 1.3157\n",
      "Parameter containing:\n",
      "tensor([-1.3453,  0.1175], requires_grad=True)\n",
      "Epoch 285, loss = -0.2186, time: 14.6114\n",
      "reconstruction loss = 0.0674, similarity loss: 1.2963\n",
      "Parameter containing:\n",
      "tensor([-1.3470,  0.1184], requires_grad=True)\n",
      "Epoch 286, loss = -0.2012, time: 12.4658\n",
      "reconstruction loss = 0.0693, similarity loss: 1.3049\n",
      "Parameter containing:\n",
      "tensor([-1.3430,  0.1221], requires_grad=True)\n",
      "Epoch 287, loss = -0.2467, time: 13.2539\n",
      "reconstruction loss = 0.0659, similarity loss: 1.2525\n",
      "Parameter containing:\n",
      "tensor([-1.3483,  0.1209], requires_grad=True)\n",
      "Epoch 288, loss = -0.2485, time: 14.3229\n",
      "reconstruction loss = 0.0688, similarity loss: 1.1945\n",
      "Parameter containing:\n",
      "tensor([-1.3441,  0.1147], requires_grad=True)\n",
      "Epoch 289, loss = -0.1953, time: 12.0371\n",
      "reconstruction loss = 0.0682, similarity loss: 1.3396\n",
      "Parameter containing:\n",
      "tensor([-1.3433,  0.1192], requires_grad=True)\n",
      "Epoch 290, loss = -0.2731, time: 11.0097\n",
      "reconstruction loss = 0.0660, similarity loss: 1.1836\n",
      "Parameter containing:\n",
      "tensor([-1.3493,  0.1125], requires_grad=True)\n",
      "Epoch 291, loss = -0.2843, time: 11.7655\n",
      "reconstruction loss = 0.0647, similarity loss: 1.1823\n",
      "Parameter containing:\n",
      "tensor([-1.3577,  0.1069], requires_grad=True)\n",
      "Epoch 292, loss = -0.2510, time: 14.9877\n",
      "reconstruction loss = 0.0690, similarity loss: 1.1863\n",
      "Parameter containing:\n",
      "tensor([-1.3501,  0.1011], requires_grad=True)\n",
      "Epoch 293, loss = -0.2429, time: 15.3954\n",
      "reconstruction loss = 0.0664, similarity loss: 1.2538\n",
      "Parameter containing:\n",
      "tensor([-1.3511,  0.1024], requires_grad=True)\n",
      "Epoch 294, loss = -0.2181, time: 14.5611\n",
      "reconstruction loss = 0.0686, similarity loss: 1.2743\n",
      "Parameter containing:\n",
      "tensor([-1.3467,  0.1081], requires_grad=True)\n",
      "Epoch 295, loss = -0.2401, time: 13.4581\n",
      "reconstruction loss = 0.0670, similarity loss: 1.2494\n",
      "Parameter containing:\n",
      "tensor([-1.3493,  0.1086], requires_grad=True)\n",
      "Epoch 296, loss = -0.2624, time: 11.9287\n",
      "reconstruction loss = 0.0681, similarity loss: 1.1732\n",
      "Parameter containing:\n",
      "tensor([-1.3461,  0.1040], requires_grad=True)\n",
      "Epoch 297, loss = -0.2546, time: 10.6298\n",
      "reconstruction loss = 0.0656, similarity loss: 1.2390\n",
      "Parameter containing:\n",
      "tensor([-1.3510,  0.1016], requires_grad=True)\n",
      "Epoch 298, loss = -0.2123, time: 11.1816\n",
      "reconstruction loss = 0.0687, similarity loss: 1.2865\n",
      "Parameter containing:\n",
      "tensor([-1.3472,  0.1093], requires_grad=True)\n",
      "Epoch 299, loss = -0.2757, time: 11.8268\n",
      "reconstruction loss = 0.0658, similarity loss: 1.1824\n",
      "Parameter containing:\n",
      "tensor([-1.3528,  0.1040], requires_grad=True)\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 2.0294, val.acc = 0.3476\n",
      "Epoch 1, loss = 1.8421, val.acc = 0.3774\n",
      "Epoch 2, loss = 1.7629, val.acc = 0.3956\n",
      "Epoch 3, loss = 1.7092, val.acc = 0.4150\n",
      "Epoch 4, loss = 1.6683, val.acc = 0.4302\n",
      "Epoch 5, loss = 1.6352, val.acc = 0.4420\n",
      "Epoch 6, loss = 1.6075, val.acc = 0.4508\n",
      "Epoch 7, loss = 1.5837, val.acc = 0.4596\n",
      "Epoch 8, loss = 1.5628, val.acc = 0.4670\n",
      "Epoch 9, loss = 1.5443, val.acc = 0.4764\n",
      "Epoch 10, loss = 1.5276, val.acc = 0.4824\n",
      "Epoch 11, loss = 1.5124, val.acc = 0.4876\n",
      "Epoch 12, loss = 1.4985, val.acc = 0.4932\n",
      "Epoch 13, loss = 1.4857, val.acc = 0.4980\n",
      "Epoch 14, loss = 1.4738, val.acc = 0.5026\n",
      "Epoch 15, loss = 1.4627, val.acc = 0.5070\n",
      "Epoch 16, loss = 1.4523, val.acc = 0.5098\n",
      "Epoch 17, loss = 1.4426, val.acc = 0.5120\n",
      "Epoch 18, loss = 1.4333, val.acc = 0.5152\n",
      "Epoch 19, loss = 1.4246, val.acc = 0.5196\n",
      "Epoch 20, loss = 1.4163, val.acc = 0.5214\n",
      "Epoch 21, loss = 1.4084, val.acc = 0.5242\n",
      "Epoch 22, loss = 1.4008, val.acc = 0.5272\n",
      "Epoch 23, loss = 1.3936, val.acc = 0.5286\n",
      "Epoch 24, loss = 1.3867, val.acc = 0.5308\n",
      "Epoch 25, loss = 1.3800, val.acc = 0.5332\n",
      "Epoch 26, loss = 1.3736, val.acc = 0.5350\n",
      "Epoch 27, loss = 1.3674, val.acc = 0.5368\n",
      "Epoch 28, loss = 1.3614, val.acc = 0.5398\n",
      "Epoch 29, loss = 1.3557, val.acc = 0.5426\n",
      "Epoch 30, loss = 1.3501, val.acc = 0.5436\n",
      "Epoch 31, loss = 1.3447, val.acc = 0.5444\n",
      "Epoch 32, loss = 1.3394, val.acc = 0.5454\n",
      "Epoch 33, loss = 1.3343, val.acc = 0.5472\n",
      "Epoch 34, loss = 1.3293, val.acc = 0.5490\n",
      "Epoch 35, loss = 1.3245, val.acc = 0.5512\n",
      "Epoch 36, loss = 1.3198, val.acc = 0.5530\n",
      "Epoch 37, loss = 1.3152, val.acc = 0.5540\n",
      "Epoch 38, loss = 1.3108, val.acc = 0.5558\n",
      "Epoch 39, loss = 1.3064, val.acc = 0.5572\n",
      "Epoch 40, loss = 1.3022, val.acc = 0.5584\n",
      "Epoch 41, loss = 1.2980, val.acc = 0.5594\n",
      "Epoch 42, loss = 1.2939, val.acc = 0.5612\n",
      "Epoch 43, loss = 1.2900, val.acc = 0.5626\n",
      "Epoch 44, loss = 1.2861, val.acc = 0.5632\n",
      "Epoch 45, loss = 1.2823, val.acc = 0.5642\n",
      "Epoch 46, loss = 1.2785, val.acc = 0.5644\n",
      "Epoch 47, loss = 1.2749, val.acc = 0.5646\n",
      "Epoch 48, loss = 1.2713, val.acc = 0.5664\n",
      "Epoch 49, loss = 1.2678, val.acc = 0.5668\n",
      "Epoch 50, loss = 1.2643, val.acc = 0.5678\n",
      "Epoch 51, loss = 1.2609, val.acc = 0.5696\n",
      "Epoch 52, loss = 1.2576, val.acc = 0.5706\n",
      "Epoch 53, loss = 1.2543, val.acc = 0.5720\n",
      "Epoch 54, loss = 1.2511, val.acc = 0.5724\n",
      "Epoch 55, loss = 1.2480, val.acc = 0.5730\n",
      "Epoch 56, loss = 1.2449, val.acc = 0.5742\n",
      "Epoch 57, loss = 1.2418, val.acc = 0.5742\n",
      "Epoch 58, loss = 1.2388, val.acc = 0.5752\n",
      "Epoch 59, loss = 1.2359, val.acc = 0.5770\n",
      "Epoch 60, loss = 1.2330, val.acc = 0.5786\n",
      "Epoch 61, loss = 1.2301, val.acc = 0.5788\n",
      "Epoch 62, loss = 1.2273, val.acc = 0.5796\n",
      "Epoch 63, loss = 1.2245, val.acc = 0.5804\n",
      "Epoch 64, loss = 1.2218, val.acc = 0.5820\n",
      "Epoch 65, loss = 1.2191, val.acc = 0.5834\n",
      "Epoch 66, loss = 1.2164, val.acc = 0.5842\n",
      "Epoch 67, loss = 1.2138, val.acc = 0.5852\n",
      "Epoch 68, loss = 1.2112, val.acc = 0.5864\n",
      "Epoch 69, loss = 1.2087, val.acc = 0.5874\n",
      "Epoch 70, loss = 1.2062, val.acc = 0.5886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, loss = 1.2037, val.acc = 0.5894\n",
      "Epoch 72, loss = 1.2012, val.acc = 0.5902\n",
      "Epoch 73, loss = 1.1988, val.acc = 0.5910\n",
      "Epoch 74, loss = 1.1964, val.acc = 0.5916\n",
      "Epoch 75, loss = 1.1941, val.acc = 0.5926\n",
      "Epoch 76, loss = 1.1917, val.acc = 0.5940\n",
      "Epoch 77, loss = 1.1894, val.acc = 0.5942\n",
      "Epoch 78, loss = 1.1872, val.acc = 0.5954\n",
      "Epoch 79, loss = 1.1849, val.acc = 0.5960\n",
      "Epoch 80, loss = 1.1827, val.acc = 0.5958\n",
      "Epoch 81, loss = 1.1805, val.acc = 0.5958\n",
      "Epoch 82, loss = 1.1784, val.acc = 0.5972\n",
      "Epoch 83, loss = 1.1762, val.acc = 0.5974\n",
      "Epoch 84, loss = 1.1741, val.acc = 0.5980\n",
      "Epoch 85, loss = 1.1720, val.acc = 0.6000\n",
      "Epoch 86, loss = 1.1700, val.acc = 0.6002\n",
      "Epoch 87, loss = 1.1679, val.acc = 0.6012\n",
      "Epoch 88, loss = 1.1659, val.acc = 0.6014\n",
      "Epoch 89, loss = 1.1639, val.acc = 0.6022\n",
      "Epoch 90, loss = 1.1619, val.acc = 0.6032\n",
      "Epoch 91, loss = 1.1600, val.acc = 0.6034\n",
      "Epoch 92, loss = 1.1580, val.acc = 0.6034\n",
      "Epoch 93, loss = 1.1561, val.acc = 0.6042\n",
      "Epoch 94, loss = 1.1542, val.acc = 0.6054\n",
      "Epoch 95, loss = 1.1523, val.acc = 0.6062\n",
      "Epoch 96, loss = 1.1505, val.acc = 0.6062\n",
      "Epoch 97, loss = 1.1486, val.acc = 0.6064\n",
      "Epoch 98, loss = 1.1468, val.acc = 0.6070\n",
      "Epoch 99, loss = 1.1450, val.acc = 0.6076\n",
      "Rep: 1, te.acc = 0.5919\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5919]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='mtl')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.8597, time: 28.3978\n",
      "reconstruction loss = 0.0695, similarity loss: 0.7902\n",
      "Epoch 1, loss = 0.5884, time: 17.9342\n",
      "reconstruction loss = 0.0672, similarity loss: 0.5212\n",
      "Epoch 2, loss = 0.6232, time: 17.9262\n",
      "reconstruction loss = 0.0782, similarity loss: 0.5449\n",
      "Epoch 3, loss = 0.6862, time: 17.7025\n",
      "reconstruction loss = 0.0662, similarity loss: 0.6200\n",
      "Epoch 4, loss = 0.6844, time: 17.5729\n",
      "reconstruction loss = 0.0651, similarity loss: 0.6193\n",
      "Epoch 5, loss = 0.6775, time: 17.8640\n",
      "reconstruction loss = 0.0760, similarity loss: 0.6015\n",
      "Epoch 6, loss = 0.5701, time: 17.7810\n",
      "reconstruction loss = 0.0679, similarity loss: 0.5022\n",
      "Epoch 7, loss = 0.5017, time: 17.9439\n",
      "reconstruction loss = 0.0652, similarity loss: 0.4365\n",
      "Epoch 8, loss = 0.7033, time: 17.6855\n",
      "reconstruction loss = 0.0661, similarity loss: 0.6372\n",
      "Epoch 9, loss = 0.6668, time: 17.7840\n",
      "reconstruction loss = 0.0630, similarity loss: 0.6038\n",
      "Epoch 10, loss = 0.5959, time: 17.6548\n",
      "reconstruction loss = 0.0675, similarity loss: 0.5284\n",
      "Epoch 11, loss = 0.6627, time: 17.8146\n",
      "reconstruction loss = 0.0655, similarity loss: 0.5972\n",
      "Epoch 12, loss = 0.6165, time: 17.7104\n",
      "reconstruction loss = 0.0653, similarity loss: 0.5512\n",
      "Epoch 13, loss = 0.7116, time: 17.6292\n",
      "reconstruction loss = 0.0648, similarity loss: 0.6468\n",
      "Epoch 14, loss = 0.6178, time: 17.4992\n",
      "reconstruction loss = 0.0669, similarity loss: 0.5509\n",
      "Epoch 15, loss = 0.5789, time: 17.3069\n",
      "reconstruction loss = 0.0638, similarity loss: 0.5151\n",
      "Epoch 16, loss = 0.6218, time: 17.4226\n",
      "reconstruction loss = 0.0666, similarity loss: 0.5552\n",
      "Epoch 17, loss = 0.6744, time: 17.4276\n",
      "reconstruction loss = 0.0635, similarity loss: 0.6109\n",
      "Epoch 18, loss = 0.5442, time: 17.6899\n",
      "reconstruction loss = 0.0634, similarity loss: 0.4808\n",
      "Epoch 19, loss = 0.5213, time: 17.4123\n",
      "reconstruction loss = 0.0646, similarity loss: 0.4567\n",
      "Epoch 20, loss = 0.4141, time: 17.5126\n",
      "reconstruction loss = 0.0634, similarity loss: 0.3507\n",
      "Epoch 21, loss = 0.4457, time: 17.3120\n",
      "reconstruction loss = 0.0669, similarity loss: 0.3787\n",
      "Epoch 22, loss = 0.3889, time: 17.5344\n",
      "reconstruction loss = 0.0617, similarity loss: 0.3272\n",
      "Epoch 23, loss = 0.3799, time: 17.2101\n",
      "reconstruction loss = 0.0635, similarity loss: 0.3164\n",
      "Epoch 24, loss = 0.4250, time: 17.3600\n",
      "reconstruction loss = 0.0641, similarity loss: 0.3610\n",
      "Epoch 25, loss = 0.3787, time: 17.6785\n",
      "reconstruction loss = 0.0625, similarity loss: 0.3162\n",
      "Epoch 26, loss = 0.4034, time: 17.3439\n",
      "reconstruction loss = 0.0659, similarity loss: 0.3375\n",
      "Epoch 27, loss = 0.4130, time: 17.0971\n",
      "reconstruction loss = 0.0608, similarity loss: 0.3522\n",
      "Epoch 28, loss = 0.3633, time: 17.2985\n",
      "reconstruction loss = 0.0626, similarity loss: 0.3007\n",
      "Epoch 29, loss = 0.3854, time: 17.3615\n",
      "reconstruction loss = 0.0682, similarity loss: 0.3172\n",
      "Epoch 30, loss = 0.3708, time: 17.9424\n",
      "reconstruction loss = 0.0690, similarity loss: 0.3018\n",
      "Epoch 31, loss = 0.3618, time: 17.4181\n",
      "reconstruction loss = 0.0613, similarity loss: 0.3005\n",
      "Epoch 32, loss = 0.3647, time: 17.7754\n",
      "reconstruction loss = 0.0647, similarity loss: 0.2999\n",
      "Epoch 33, loss = 0.3837, time: 17.7719\n",
      "reconstruction loss = 0.0639, similarity loss: 0.3198\n",
      "Epoch 34, loss = 0.3749, time: 17.3992\n",
      "reconstruction loss = 0.0667, similarity loss: 0.3082\n",
      "Epoch 35, loss = 0.3350, time: 17.3866\n",
      "reconstruction loss = 0.0627, similarity loss: 0.2723\n",
      "Epoch 36, loss = 0.3268, time: 17.3438\n",
      "reconstruction loss = 0.0661, similarity loss: 0.2606\n",
      "Epoch 37, loss = 0.3356, time: 17.8275\n",
      "reconstruction loss = 0.0628, similarity loss: 0.2728\n",
      "Epoch 38, loss = 0.4102, time: 17.4920\n",
      "reconstruction loss = 0.0660, similarity loss: 0.3442\n",
      "Epoch 39, loss = 0.3715, time: 17.6937\n",
      "reconstruction loss = 0.0596, similarity loss: 0.3119\n",
      "Epoch 40, loss = 0.3586, time: 16.9987\n",
      "reconstruction loss = 0.0622, similarity loss: 0.2964\n",
      "Epoch 41, loss = 0.3762, time: 17.6320\n",
      "reconstruction loss = 0.0651, similarity loss: 0.3111\n",
      "Epoch 42, loss = 0.3481, time: 17.8129\n",
      "reconstruction loss = 0.0629, similarity loss: 0.2852\n",
      "Epoch 43, loss = 0.3611, time: 17.3949\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2960\n",
      "Epoch 44, loss = 0.3761, time: 17.8881\n",
      "reconstruction loss = 0.0646, similarity loss: 0.3115\n",
      "Epoch 45, loss = 0.3738, time: 17.5375\n",
      "reconstruction loss = 0.0649, similarity loss: 0.3089\n",
      "Epoch 46, loss = 0.3615, time: 17.4259\n",
      "reconstruction loss = 0.0653, similarity loss: 0.2962\n",
      "Epoch 47, loss = 0.3033, time: 17.4362\n",
      "reconstruction loss = 0.0638, similarity loss: 0.2395\n",
      "Epoch 48, loss = 0.3411, time: 17.5299\n",
      "reconstruction loss = 0.0622, similarity loss: 0.2789\n",
      "Epoch 49, loss = 0.3343, time: 17.6003\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2707\n",
      "Epoch 50, loss = 0.3273, time: 18.0617\n",
      "reconstruction loss = 0.0656, similarity loss: 0.2617\n",
      "Epoch 51, loss = 0.3090, time: 17.5271\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2453\n",
      "Epoch 52, loss = 0.3228, time: 17.5915\n",
      "reconstruction loss = 0.0654, similarity loss: 0.2574\n",
      "Epoch 53, loss = 0.3640, time: 17.5628\n",
      "reconstruction loss = 0.0643, similarity loss: 0.2997\n",
      "Epoch 54, loss = 0.3670, time: 17.8213\n",
      "reconstruction loss = 0.0641, similarity loss: 0.3029\n",
      "Epoch 55, loss = 0.3371, time: 17.3497\n",
      "reconstruction loss = 0.0642, similarity loss: 0.2729\n",
      "Epoch 56, loss = 0.3319, time: 17.2171\n",
      "reconstruction loss = 0.0642, similarity loss: 0.2677\n",
      "Epoch 57, loss = 0.3045, time: 17.2188\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2407\n",
      "Epoch 58, loss = 0.2959, time: 17.3020\n",
      "reconstruction loss = 0.0640, similarity loss: 0.2319\n",
      "Epoch 59, loss = 0.3393, time: 17.5275\n",
      "reconstruction loss = 0.0639, similarity loss: 0.2753\n",
      "Epoch 60, loss = 0.3271, time: 17.4200\n",
      "reconstruction loss = 0.0612, similarity loss: 0.2659\n",
      "Epoch 61, loss = 0.3137, time: 18.1506\n",
      "reconstruction loss = 0.0660, similarity loss: 0.2477\n",
      "Epoch 62, loss = 0.3137, time: 17.8099\n",
      "reconstruction loss = 0.0626, similarity loss: 0.2511\n",
      "Epoch 63, loss = 0.3450, time: 17.4394\n",
      "reconstruction loss = 0.0673, similarity loss: 0.2777\n",
      "Epoch 64, loss = 0.3161, time: 17.4393\n",
      "reconstruction loss = 0.0625, similarity loss: 0.2536\n",
      "Epoch 65, loss = 0.3496, time: 18.2641\n",
      "reconstruction loss = 0.0647, similarity loss: 0.2849\n",
      "Epoch 66, loss = 0.3231, time: 17.5848\n",
      "reconstruction loss = 0.0653, similarity loss: 0.2578\n",
      "Epoch 67, loss = 0.3564, time: 17.6912\n",
      "reconstruction loss = 0.0646, similarity loss: 0.2918\n",
      "Epoch 68, loss = 0.2975, time: 17.7357\n",
      "reconstruction loss = 0.0660, similarity loss: 0.2315\n",
      "Epoch 69, loss = 0.3330, time: 17.5177\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2679\n",
      "Epoch 70, loss = 0.3280, time: 18.0142\n",
      "reconstruction loss = 0.0659, similarity loss: 0.2622\n",
      "Epoch 71, loss = 0.3192, time: 17.7993\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2555\n",
      "Epoch 72, loss = 0.3118, time: 17.5690\n",
      "reconstruction loss = 0.0645, similarity loss: 0.2474\n",
      "Epoch 73, loss = 0.2953, time: 17.2923\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2315\n",
      "Epoch 74, loss = 0.2727, time: 17.6821\n",
      "reconstruction loss = 0.0644, similarity loss: 0.2083\n",
      "Epoch 75, loss = 0.3383, time: 18.1346\n",
      "reconstruction loss = 0.0642, similarity loss: 0.2741\n",
      "Epoch 76, loss = 0.3407, time: 18.1461\n",
      "reconstruction loss = 0.0637, similarity loss: 0.2770\n",
      "Epoch 77, loss = 0.3036, time: 17.9040\n",
      "reconstruction loss = 0.0648, similarity loss: 0.2388\n",
      "Epoch 78, loss = 0.3139, time: 18.1842\n",
      "reconstruction loss = 0.0638, similarity loss: 0.2501\n",
      "Epoch 79, loss = 0.2951, time: 17.9036\n",
      "reconstruction loss = 0.0660, similarity loss: 0.2290\n",
      "Epoch 80, loss = 0.3300, time: 17.7689\n",
      "reconstruction loss = 0.0641, similarity loss: 0.2659\n",
      "Epoch 81, loss = 0.3052, time: 18.2117\n",
      "reconstruction loss = 0.0668, similarity loss: 0.2384\n",
      "Epoch 82, loss = 0.3031, time: 18.1754\n",
      "reconstruction loss = 0.0665, similarity loss: 0.2366\n",
      "Epoch 83, loss = 0.3103, time: 17.6003\n",
      "reconstruction loss = 0.0620, similarity loss: 0.2483\n",
      "Epoch 84, loss = 0.2947, time: 17.5431\n",
      "reconstruction loss = 0.0653, similarity loss: 0.2294\n",
      "Epoch 85, loss = 0.3183, time: 17.3721\n",
      "reconstruction loss = 0.0635, similarity loss: 0.2547\n",
      "Epoch 86, loss = 0.2951, time: 17.3262\n",
      "reconstruction loss = 0.0648, similarity loss: 0.2304\n",
      "Epoch 87, loss = 0.3349, time: 17.0396\n",
      "reconstruction loss = 0.0665, similarity loss: 0.2684\n",
      "Epoch 88, loss = 0.2960, time: 17.3830\n",
      "reconstruction loss = 0.0659, similarity loss: 0.2301\n",
      "Epoch 89, loss = 0.3281, time: 17.8778\n",
      "reconstruction loss = 0.0677, similarity loss: 0.2604\n",
      "Epoch 90, loss = 0.3318, time: 17.8628\n",
      "reconstruction loss = 0.0650, similarity loss: 0.2667\n",
      "Epoch 91, loss = 0.2973, time: 17.4856\n",
      "reconstruction loss = 0.0652, similarity loss: 0.2322\n",
      "Epoch 92, loss = 0.3008, time: 17.3207\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2357\n",
      "Epoch 93, loss = 0.2831, time: 17.2711\n",
      "reconstruction loss = 0.0639, similarity loss: 0.2192\n",
      "Epoch 94, loss = 0.2848, time: 17.5007\n",
      "reconstruction loss = 0.0614, similarity loss: 0.2233\n",
      "Epoch 95, loss = 0.3146, time: 17.6709\n",
      "reconstruction loss = 0.0664, similarity loss: 0.2482\n",
      "Epoch 96, loss = 0.2842, time: 17.6192\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2192\n",
      "Epoch 97, loss = 0.3081, time: 17.7416\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2431\n",
      "Epoch 98, loss = 0.3064, time: 17.2399\n",
      "reconstruction loss = 0.0667, similarity loss: 0.2398\n",
      "Epoch 99, loss = 0.2948, time: 17.6736\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2297\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1, verbose=1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1, verbose=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "-- Epoch 1\n",
      "\n",
      "Norm: 3604.79, NNZs: 8192, Bias: 41.276313, T: 45000, Avg. loss: 56289.392309\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4330.50, NNZs: 8192, Bias: -41.383601, T: 45000, Avg. loss: 73297.336301\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5782.57, NNZs: 8192, Bias: -54.220675, T: 45000, Avg. loss: 77312.667331\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4395.29, NNZs: 8192, Bias: -4.898155, T: 45000, Avg. loss: 57224.094669\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5650.10, NNZs: 8192, Bias: -36.785647, T: 45000, Avg. loss: 81004.746366\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6186.17, NNZs: 8192, Bias: -25.325334, T: 45000, Avg. loss: 74037.475335\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5375.28, NNZs: 8192, Bias: -42.653661, T: 45000, Avg. loss: 79940.291711Norm: 5023.42, NNZs: 8192, Bias: -54.682589, T: 45000, Avg. loss: 77167.896863\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 2\n",
      "\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3816.52, NNZs: 8192, Bias: -12.658163, T: 45000, Avg. loss: 70126.383943\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3463.55, NNZs: 8192, Bias: -16.160051, T: 45000, Avg. loss: 79676.329455\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2859.40, NNZs: 8192, Bias: 43.527964, T: 90000, Avg. loss: 8919.562191\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4077.01, NNZs: 8192, Bias: -53.929908, T: 90000, Avg. loss: 12063.725419\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3395.57, NNZs: 8192, Bias: -46.078479, T: 90000, Avg. loss: 12015.268371\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3313.21, NNZs: 8192, Bias: -5.322726, T: 90000, Avg. loss: 9053.146636\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4123.71, NNZs: 8192, Bias: -40.199529, T: 90000, Avg. loss: 12657.494275\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3686.46, NNZs: 8192, Bias: -45.521971, T: 90000, Avg. loss: 11980.888168\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4578.25, NNZs: 8192, Bias: -25.943599, T: 90000, Avg. loss: 10968.324469\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3584.38, NNZs: 8192, Bias: -56.915714, T: 90000, Avg. loss: 12511.312697\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2685.76, NNZs: 8192, Bias: -12.103944, T: 90000, Avg. loss: 11888.600239\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2655.78, NNZs: 8192, Bias: -17.219891, T: 90000, Avg. loss: 13501.814154\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2491.41, NNZs: 8192, Bias: 45.063892, T: 135000, Avg. loss: 5078.009475\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3258.54, NNZs: 8192, Bias: -53.649841, T: 135000, Avg. loss: 6778.874159\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2802.84, NNZs: 8192, Bias: -48.148098, T: 135000, Avg. loss: 6904.407563\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2797.79, NNZs: 8192, Bias: -5.310109, T: 135000, Avg. loss: 5087.337666\n",
      "Total training time: 3.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3377.89, NNZs: 8192, Bias: -42.162040, T: 135000, Avg. loss: 7174.363523\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2892.29, NNZs: 8192, Bias: -47.146988, T: 135000, Avg. loss: 6888.746292\n",
      "Total training time: 3.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2903.17, NNZs: 8192, Bias: -57.904412, T: 135000, Avg. loss: 7295.158398\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3656.88, NNZs: 8192, Bias: -26.538835, T: 135000, Avg. loss: 6242.710022\n",
      "Total training time: 3.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2189.91, NNZs: 8192, Bias: -11.868733, T: 135000, Avg. loss: 6906.728272\n",
      "Total training time: 3.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2296.80, NNZs: 8192, Bias: -17.763442, T: 135000, Avg. loss: 7788.613580\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2222.36, NNZs: 8192, Bias: 45.619289, T: 180000, Avg. loss: 3596.923299\n",
      "Total training time: 4.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2809.81, NNZs: 8192, Bias: -53.663276, T: 180000, Avg. loss: 4750.015169\n",
      "Total training time: 4.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.94, NNZs: 8192, Bias: -5.319122, T: 180000, Avg. loss: 3509.957901\n",
      "Total training time: 4.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2456.33, NNZs: 8192, Bias: -49.417453, T: 180000, Avg. loss: 4802.309107\n",
      "Total training time: 4.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2971.65, NNZs: 8192, Bias: -43.247118, T: 180000, Avg. loss: 4910.428181\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2522.95, NNZs: 8192, Bias: -48.085907, T: 180000, Avg. loss: 4706.895210\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3154.45, NNZs: 8192, Bias: -27.176295, T: 180000, Avg. loss: 4242.303983\n",
      "Total training time: 4.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2461.12, NNZs: 8192, Bias: -58.402453, T: 180000, Avg. loss: 5205.191787\n",
      "Total training time: 4.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1892.24, NNZs: 8192, Bias: -11.296886, T: 180000, Avg. loss: 4854.972720\n",
      "Total training time: 4.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2083.48, NNZs: 8192, Bias: -18.069223, T: 180000, Avg. loss: 5511.059688\n",
      "Total training time: 4.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2509.81, NNZs: 8192, Bias: -53.461136, T: 225000, Avg. loss: 3617.627947\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2016.81, NNZs: 8192, Bias: 46.113231, T: 225000, Avg. loss: 2691.071237\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2283.67, NNZs: 8192, Bias: -5.272106, T: 225000, Avg. loss: 2667.330881\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2218.17, NNZs: 8192, Bias: -50.363190, T: 225000, Avg. loss: 3721.117708\n",
      "Total training time: 5.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2262.88, NNZs: 8192, Bias: -48.667756, T: 225000, Avg. loss: 3594.980727\n",
      "Total training time: 5.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2678.97, NNZs: 8192, Bias: -44.076952, T: 225000, Avg. loss: 3774.266880\n",
      "Total training time: 5.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2800.47, NNZs: 8192, Bias: -27.485599, T: 225000, Avg. loss: 3277.082886\n",
      "Total training time: 5.20 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2184.23, NNZs: 8192, Bias: -58.691396, T: 225000, Avg. loss: 3948.153623\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1698.51, NNZs: 8192, Bias: -10.862383, T: 225000, Avg. loss: 3676.943418\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1890.89, NNZs: 8192, Bias: -18.122188, T: 225000, Avg. loss: 4285.732907\n",
      "Total training time: 5.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2281.57, NNZs: 8192, Bias: -53.345391, T: 270000, Avg. loss: 2935.673264\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1874.28, NNZs: 8192, Bias: 46.391893, T: 270000, Avg. loss: 2186.925232\n",
      "Total training time: 6.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2108.16, NNZs: 8192, Bias: -5.388988, T: 270000, Avg. loss: 2128.109637\n",
      "Total training time: 6.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2055.78, NNZs: 8192, Bias: -51.174690, T: 270000, Avg. loss: 2972.910331\n",
      "Total training time: 6.21 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2084.25, NNZs: 8192, Bias: -49.102507, T: 270000, Avg. loss: 2888.734887\n",
      "Total training time: 6.18 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2469.22, NNZs: 8192, Bias: -44.759308, T: 270000, Avg. loss: 3003.342625\n",
      "Total training time: 6.20 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2529.55, NNZs: 8192, Bias: -27.849031, T: 270000, Avg. loss: 2657.610586\n",
      "Total training time: 6.26 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1964.73, NNZs: 8192, Bias: -58.903902, T: 270000, Avg. loss: 3183.993858\n",
      "Total training time: 6.30 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1569.19, NNZs: 8192, Bias: -10.545378, T: 270000, Avg. loss: 3037.173300\n",
      "Total training time: 6.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1778.86, NNZs: 8192, Bias: -18.444422, T: 270000, Avg. loss: 3416.670941\n",
      "Total training time: 6.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2129.41, NNZs: 8192, Bias: -53.343773, T: 315000, Avg. loss: 2433.530234\n",
      "Total training time: 7.19 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1757.27, NNZs: 8192, Bias: 46.666981, T: 315000, Avg. loss: 1817.871783\n",
      "Total training time: 7.22 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1969.80, NNZs: 8192, Bias: -5.425649, T: 315000, Avg. loss: 1763.679475\n",
      "Total training time: 7.19 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1922.72, NNZs: 8192, Bias: -51.888644, T: 315000, Avg. loss: 2502.765002\n",
      "Total training time: 7.29 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1935.53, NNZs: 8192, Bias: -49.473948, T: 315000, Avg. loss: 2426.472070\n",
      "Total training time: 7.24 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2350.49, NNZs: 8192, Bias: -28.258807, T: 315000, Avg. loss: 2217.632902\n",
      "Total training time: 7.31 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2308.30, NNZs: 8192, Bias: -45.198936, T: 315000, Avg. loss: 2489.622604\n",
      "Total training time: 7.29 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1798.16, NNZs: 8192, Bias: -59.073013, T: 315000, Avg. loss: 2722.224094\n",
      "Total training time: 7.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1483.11, NNZs: 8192, Bias: -10.447961, T: 315000, Avg. loss: 2542.601734\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1664.18, NNZs: 8192, Bias: -18.542523, T: 315000, Avg. loss: 2874.825904\n",
      "Total training time: 7.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1989.61, NNZs: 8192, Bias: -53.257518, T: 360000, Avg. loss: 2111.620996\n",
      "Total training time: 8.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1659.52, NNZs: 8192, Bias: 46.869903, T: 360000, Avg. loss: 1550.611978\n",
      "Total training time: 8.23 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1862.70, NNZs: 8192, Bias: -5.399323, T: 360000, Avg. loss: 1502.247284\n",
      "Total training time: 8.21 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1807.75, NNZs: 8192, Bias: -49.770505, T: 360000, Avg. loss: 2089.541025\n",
      "Total training time: 8.25 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1802.09, NNZs: 8192, Bias: -52.273327, T: 360000, Avg. loss: 2144.704139\n",
      "Total training time: 8.33 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2190.06, NNZs: 8192, Bias: -28.498195, T: 360000, Avg. loss: 1880.782415\n",
      "Total training time: 8.33 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2172.29, NNZs: 8192, Bias: -45.674588, T: 360000, Avg. loss: 2124.043249\n",
      "Total training time: 8.32 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1668.69, NNZs: 8192, Bias: -59.218132, T: 360000, Avg. loss: 2316.206093\n",
      "Total training time: 8.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1405.67, NNZs: 8192, Bias: -10.326728, T: 360000, Avg. loss: 2181.707071\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1562.45, NNZs: 8192, Bias: -18.688232, T: 360000, Avg. loss: 2491.704609\n",
      "Total training time: 8.50 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1890.56, NNZs: 8192, Bias: -53.229293, T: 405000, Avg. loss: 1818.774594\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1570.27, NNZs: 8192, Bias: 46.998962, T: 405000, Avg. loss: 1378.403110\n",
      "Total training time: 9.22 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1770.82, NNZs: 8192, Bias: -5.452508, T: 405000, Avg. loss: 1295.438701\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1706.06, NNZs: 8192, Bias: -50.005885, T: 405000, Avg. loss: 1786.286572\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2059.75, NNZs: 8192, Bias: -28.762591, T: 405000, Avg. loss: 1622.998971\n",
      "Total training time: 9.29 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1718.60, NNZs: 8192, Bias: -52.664708, T: 405000, Avg. loss: 1860.617581\n",
      "Total training time: 9.33 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2062.71, NNZs: 8192, Bias: -46.089049, T: 405000, Avg. loss: 1877.664919\n",
      "Total training time: 9.32 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1570.62, NNZs: 8192, Bias: -59.295878, T: 405000, Avg. loss: 2049.348813\n",
      "Total training time: 9.44 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1337.23, NNZs: 8192, Bias: -10.172146, T: 405000, Avg. loss: 1894.200215\n",
      "Total training time: 9.49 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1475.58, NNZs: 8192, Bias: -18.770636, T: 405000, Avg. loss: 2179.898131\n",
      "Total training time: 9.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1786.29, NNZs: 8192, Bias: -53.087973, T: 450000, Avg. loss: 1620.745235\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1494.69, NNZs: 8192, Bias: 47.135671, T: 450000, Avg. loss: 1228.506287\n",
      "Total training time: 10.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1702.86, NNZs: 8192, Bias: -5.473231, T: 450000, Avg. loss: 1147.934593\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1621.04, NNZs: 8192, Bias: -50.311024, T: 450000, Avg. loss: 1629.512886\n",
      "Total training time: 10.26 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1946.83, NNZs: 8192, Bias: -29.020900, T: 450000, Avg. loss: 1473.287931\n",
      "Total training time: 10.31 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1634.02, NNZs: 8192, Bias: -53.065558, T: 450000, Avg. loss: 1672.879553\n",
      "Total training time: 10.35 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1973.44, NNZs: 8192, Bias: -46.369065, T: 450000, Avg. loss: 1639.296804\n",
      "Total training time: 10.34 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1490.46, NNZs: 8192, Bias: -59.366340, T: 450000, Avg. loss: 1831.344197\n",
      "Total training time: 10.47 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1283.14, NNZs: 8192, Bias: -9.967536, T: 450000, Avg. loss: 1698.885066\n",
      "Total training time: 10.51 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1425.20, NNZs: 8192, Bias: -18.819569, T: 450000, Avg. loss: 1941.430365\n",
      "Total training time: 10.53 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1706.98, NNZs: 8192, Bias: -53.067155, T: 495000, Avg. loss: 1444.499344\n",
      "Total training time: 11.15 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1629.82, NNZs: 8192, Bias: -5.368304, T: 495000, Avg. loss: 1024.500315\n",
      "Total training time: 11.14 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1433.05, NNZs: 8192, Bias: 47.259894, T: 495000, Avg. loss: 1069.784228\n",
      "Total training time: 11.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1858.15, NNZs: 8192, Bias: -29.211402, T: 495000, Avg. loss: 1286.597053\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1566.03, NNZs: 8192, Bias: -50.478100, T: 495000, Avg. loss: 1465.655589\n",
      "Total training time: 11.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1559.10, NNZs: 8192, Bias: -53.424909, T: 495000, Avg. loss: 1526.885986\n",
      "Total training time: 11.33 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1893.02, NNZs: 8192, Bias: -46.705923, T: 495000, Avg. loss: 1453.425926\n",
      "Total training time: 11.30 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1407.87, NNZs: 8192, Bias: -59.494365, T: 495000, Avg. loss: 1670.320394\n",
      "Total training time: 11.43 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1243.08, NNZs: 8192, Bias: -9.861691, T: 495000, Avg. loss: 1536.789177\n",
      "Total training time: 11.48 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1365.72, NNZs: 8192, Bias: -18.841412, T: 495000, Avg. loss: 1754.268868\n",
      "Total training time: 11.50 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1633.24, NNZs: 8192, Bias: -53.026742, T: 540000, Avg. loss: 1302.526983\n",
      "Total training time: 12.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1570.30, NNZs: 8192, Bias: -5.389991, T: 540000, Avg. loss: 923.240922\n",
      "Total training time: 12.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1379.18, NNZs: 8192, Bias: 47.355586, T: 540000, Avg. loss: 990.769569\n",
      "Total training time: 12.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1772.13, NNZs: 8192, Bias: -29.423706, T: 540000, Avg. loss: 1204.338271\n",
      "Total training time: 12.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1500.34, NNZs: 8192, Bias: -50.692129, T: 540000, Avg. loss: 1338.524449\n",
      "Total training time: 12.19 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1492.43, NNZs: 8192, Bias: -53.734423, T: 540000, Avg. loss: 1372.250440\n",
      "Total training time: 12.30 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1821.71, NNZs: 8192, Bias: -46.974616, T: 540000, Avg. loss: 1327.175156\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1347.06, NNZs: 8192, Bias: -59.590283, T: 540000, Avg. loss: 1503.773332\n",
      "Total training time: 12.40 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1206.32, NNZs: 8192, Bias: -9.801764, T: 540000, Avg. loss: 1389.246431\n",
      "Total training time: 12.47 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1317.20, NNZs: 8192, Bias: -18.957129, T: 540000, Avg. loss: 1582.359238\n",
      "Total training time: 12.48 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1509.95, NNZs: 8192, Bias: -5.390665, T: 585000, Avg. loss: 850.600559\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1568.57, NNZs: 8192, Bias: -52.973805, T: 585000, Avg. loss: 1216.690408\n",
      "Total training time: 13.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1330.77, NNZs: 8192, Bias: 47.446097, T: 585000, Avg. loss: 904.078858\n",
      "Total training time: 13.14 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1700.09, NNZs: 8192, Bias: -29.619038, T: 585000, Avg. loss: 1081.580917\n",
      "Total training time: 13.16 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1442.43, NNZs: 8192, Bias: -50.851800, T: 585000, Avg. loss: 1193.598019\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1761.77, NNZs: 8192, Bias: -47.168758, T: 585000, Avg. loss: 1179.871701\n",
      "Total training time: 13.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1435.81, NNZs: 8192, Bias: -53.999892, T: 585000, Avg. loss: 1273.826032\n",
      "Total training time: 13.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1301.43, NNZs: 8192, Bias: -59.680100, T: 585000, Avg. loss: 1368.721059\n",
      "Total training time: 13.36 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1266.00, NNZs: 8192, Bias: -18.938821, T: 585000, Avg. loss: 1458.209461\n",
      "Total training time: 13.45 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1172.89, NNZs: 8192, Bias: -9.678099, T: 585000, Avg. loss: 1277.691501\n",
      "Total training time: 13.46 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1463.26, NNZs: 8192, Bias: -5.357617, T: 630000, Avg. loss: 776.921487\n",
      "Total training time: 13.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1508.70, NNZs: 8192, Bias: -52.891789, T: 630000, Avg. loss: 1127.764801\n",
      "Total training time: 13.99 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1291.48, NNZs: 8192, Bias: 47.578464, T: 630000, Avg. loss: 824.213999\n",
      "Total training time: 14.10 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1633.87, NNZs: 8192, Bias: -29.782233, T: 630000, Avg. loss: 995.196211\n",
      "Total training time: 14.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1391.53, NNZs: 8192, Bias: -50.966465, T: 630000, Avg. loss: 1113.808703\n",
      "Total training time: 14.10 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1702.96, NNZs: 8192, Bias: -47.365980, T: 630000, Avg. loss: 1103.526730\n",
      "Total training time: 14.17 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1383.85, NNZs: 8192, Bias: -54.180006, T: 630000, Avg. loss: 1160.397494\n",
      "Total training time: 14.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1256.11, NNZs: 8192, Bias: -59.746254, T: 630000, Avg. loss: 1263.896699\n",
      "Total training time: 14.34 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1217.39, NNZs: 8192, Bias: -18.955384, T: 630000, Avg. loss: 1361.415275\n",
      "Total training time: 14.42 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1143.37, NNZs: 8192, Bias: -9.629173, T: 630000, Avg. loss: 1192.450599\n",
      "Total training time: 14.46 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1419.27, NNZs: 8192, Bias: -5.388299, T: 675000, Avg. loss: 725.675966\n",
      "Total training time: 14.91 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1462.39, NNZs: 8192, Bias: -52.860815, T: 675000, Avg. loss: 1018.328269\n",
      "Total training time: 14.94 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1249.60, NNZs: 8192, Bias: 47.609826, T: 675000, Avg. loss: 766.820208\n",
      "Total training time: 15.05 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1580.94, NNZs: 8192, Bias: -29.904980, T: 675000, Avg. loss: 916.227006\n",
      "Total training time: 15.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1352.41, NNZs: 8192, Bias: -51.135678, T: 675000, Avg. loss: 1005.912145\n",
      "Total training time: 15.04 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1655.19, NNZs: 8192, Bias: -47.520607, T: 675000, Avg. loss: 1019.953130\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1339.73, NNZs: 8192, Bias: -54.365161, T: 675000, Avg. loss: 1075.469070\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1219.32, NNZs: 8192, Bias: -59.791283, T: 675000, Avg. loss: 1163.872158\n",
      "Total training time: 15.32 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1178.84, NNZs: 8192, Bias: -18.969960, T: 675000, Avg. loss: 1252.256326\n",
      "Total training time: 15.39 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1107.76, NNZs: 8192, Bias: -9.552295, T: 675000, Avg. loss: 1104.117045\n",
      "Total training time: 15.46 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1382.17, NNZs: 8192, Bias: -5.345418, T: 720000, Avg. loss: 664.895179\n",
      "Total training time: 15.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1413.29, NNZs: 8192, Bias: -52.804337, T: 720000, Avg. loss: 945.462778\n",
      "Total training time: 15.90 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1215.48, NNZs: 8192, Bias: 47.752158, T: 720000, Avg. loss: 712.112881\n",
      "Total training time: 16.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1528.24, NNZs: 8192, Bias: -30.049233, T: 720000, Avg. loss: 849.634634\n",
      "Total training time: 16.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1318.13, NNZs: 8192, Bias: -51.264230, T: 720000, Avg. loss: 964.849725\n",
      "Total training time: 16.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1610.04, NNZs: 8192, Bias: -47.707259, T: 720000, Avg. loss: 945.239849\n",
      "Total training time: 16.07 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1300.24, NNZs: 8192, Bias: -54.594753, T: 720000, Avg. loss: 990.875676\n",
      "Total training time: 16.17 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1178.38, NNZs: 8192, Bias: -59.862223, T: 720000, Avg. loss: 1084.810183\n",
      "Total training time: 16.28 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1144.13, NNZs: 8192, Bias: -18.984205, T: 720000, Avg. loss: 1171.192921\n",
      "Total training time: 16.36 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1083.62, NNZs: 8192, Bias: -9.464979, T: 720000, Avg. loss: 1009.780361\n",
      "Total training time: 16.44 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1342.65, NNZs: 8192, Bias: -5.387688, T: 765000, Avg. loss: 621.219611\n",
      "Total training time: 16.81 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1372.84, NNZs: 8192, Bias: -52.777569, T: 765000, Avg. loss: 895.388970\n",
      "Total training time: 16.84 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1183.80, NNZs: 8192, Bias: 47.832204, T: 765000, Avg. loss: 662.115966\n",
      "Total training time: 16.99 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1482.77, NNZs: 8192, Bias: -30.211507, T: 765000, Avg. loss: 792.774278\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1283.68, NNZs: 8192, Bias: -51.358200, T: 765000, Avg. loss: 895.371411\n",
      "Total training time: 16.96 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1566.35, NNZs: 8192, Bias: -47.828317, T: 765000, Avg. loss: 882.562992\n",
      "Total training time: 17.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1263.70, NNZs: 8192, Bias: -54.782233, T: 765000, Avg. loss: 921.452972\n",
      "Total training time: 17.13 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1142.56, NNZs: 8192, Bias: -59.929940, T: 765000, Avg. loss: 1027.837921\n",
      "Total training time: 17.26 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1113.70, NNZs: 8192, Bias: -19.011191, T: 765000, Avg. loss: 1088.204279\n",
      "Total training time: 17.33 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1057.42, NNZs: 8192, Bias: -9.383699, T: 765000, Avg. loss: 957.031561\n",
      "Total training time: 17.40 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1310.20, NNZs: 8192, Bias: -5.362268, T: 810000, Avg. loss: 580.928998\n",
      "Total training time: 17.77 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1335.99, NNZs: 8192, Bias: -52.727277, T: 810000, Avg. loss: 855.274674\n",
      "Total training time: 17.80 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1441.54, NNZs: 8192, Bias: -30.326270, T: 810000, Avg. loss: 750.949337\n",
      "Total training time: 17.92 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1152.42, NNZs: 8192, Bias: 47.881915, T: 810000, Avg. loss: 628.066658\n",
      "Total training time: 17.94 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1249.43, NNZs: 8192, Bias: -51.485083, T: 810000, Avg. loss: 840.343826\n",
      "Total training time: 17.93 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1520.93, NNZs: 8192, Bias: -47.966916, T: 810000, Avg. loss: 830.214396\n",
      "Total training time: 17.98 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1230.48, NNZs: 8192, Bias: -54.920969, T: 810000, Avg. loss: 887.953365\n",
      "Total training time: 18.09 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1110.14, NNZs: 8192, Bias: -59.992805, T: 810000, Avg. loss: 958.946638\n",
      "Total training time: 18.23 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1084.98, NNZs: 8192, Bias: -19.023934, T: 810000, Avg. loss: 1037.739951\n",
      "Total training time: 18.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1033.48, NNZs: 8192, Bias: -9.295677, T: 810000, Avg. loss: 897.203314\n",
      "Total training time: 18.39 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1280.51, NNZs: 8192, Bias: -5.374464, T: 855000, Avg. loss: 548.252346\n",
      "Total training time: 18.70 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1301.08, NNZs: 8192, Bias: -52.691965, T: 855000, Avg. loss: 783.887380\n",
      "Total training time: 18.74 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1399.25, NNZs: 8192, Bias: -30.457708, T: 855000, Avg. loss: 696.660897\n",
      "Total training time: 18.87 seconds.\n",
      "-- Epoch 20Norm: 1126.32, NNZs: 8192, Bias: 47.929520, T: 855000, Avg. loss: 587.783523\n",
      "\n",
      "Total training time: 18.88 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1220.18, NNZs: 8192, Bias: -51.568957, T: 855000, Avg. loss: 785.138603\n",
      "Total training time: 18.88 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1483.68, NNZs: 8192, Bias: -48.051121, T: 855000, Avg. loss: 756.814619\n",
      "Total training time: 18.92 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1200.35, NNZs: 8192, Bias: -55.064437, T: 855000, Avg. loss: 828.564462\n",
      "Total training time: 19.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1081.88, NNZs: 8192, Bias: -60.017510, T: 855000, Avg. loss: 905.444985\n",
      "Total training time: 19.20 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1059.23, NNZs: 8192, Bias: -19.059588, T: 855000, Avg. loss: 982.693739\n",
      "Total training time: 19.28 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1012.23, NNZs: 8192, Bias: -9.272155, T: 855000, Avg. loss: 849.406130\n",
      "Total training time: 19.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1250.80, NNZs: 8192, Bias: -5.374297, T: 900000, Avg. loss: 518.525551\n",
      "Total training time: 19.66 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1268.42, NNZs: 8192, Bias: -52.669079, T: 900000, Avg. loss: 741.799030\n",
      "Total training time: 19.69 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1363.07, NNZs: 8192, Bias: -30.560140, T: 900000, Avg. loss: 652.471685\n",
      "Total training time: 19.82 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1102.09, NNZs: 8192, Bias: 47.975589, T: 900000, Avg. loss: 555.146548\n",
      "Total training time: 19.84 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1193.62, NNZs: 8192, Bias: -51.648577, T: 900000, Avg. loss: 742.072653\n",
      "Total training time: 19.84 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1449.28, NNZs: 8192, Bias: -48.165430, T: 900000, Avg. loss: 713.826572\n",
      "Total training time: 19.89 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1167.63, NNZs: 8192, Bias: -55.201402, T: 900000, Avg. loss: 785.140132\n",
      "Total training time: 20.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1054.22, NNZs: 8192, Bias: -60.075448, T: 900000, Avg. loss: 860.511559\n",
      "Total training time: 20.18 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1037.49, NNZs: 8192, Bias: -19.071416, T: 900000, Avg. loss: 910.943228\n",
      "Total training time: 20.27 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 992.80, NNZs: 8192, Bias: -9.203938, T: 900000, Avg. loss: 800.213177\n",
      "Total training time: 20.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1222.24, NNZs: 8192, Bias: -5.384674, T: 945000, Avg. loss: 488.517330\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1237.50, NNZs: 8192, Bias: -52.625399, T: 945000, Avg. loss: 704.868955\n",
      "Total training time: 20.66 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1076.48, NNZs: 8192, Bias: 48.040954, T: 945000, Avg. loss: 530.872613\n",
      "Total training time: 20.83 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1329.92, NNZs: 8192, Bias: -30.700744, T: 945000, Avg. loss: 622.300329\n",
      "Total training time: 20.82 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1170.51, NNZs: 8192, Bias: -51.745875, T: 945000, Avg. loss: 700.430644\n",
      "Total training time: 20.82 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1418.36, NNZs: 8192, Bias: -48.295137, T: 945000, Avg. loss: 683.729839\n",
      "Total training time: 20.90 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1139.70, NNZs: 8192, Bias: -55.353373, T: 945000, Avg. loss: 735.655216\n",
      "Total training time: 21.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1026.77, NNZs: 8192, Bias: -60.151169, T: 945000, Avg. loss: 821.537014\n",
      "Total training time: 21.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1014.70, NNZs: 8192, Bias: -19.103769, T: 945000, Avg. loss: 873.664048\n",
      "Total training time: 21.27 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 972.21, NNZs: 8192, Bias: -9.214697, T: 945000, Avg. loss: 762.550102\n",
      "Total training time: 21.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1200.23, NNZs: 8192, Bias: -5.384103, T: 990000, Avg. loss: 453.302322\n",
      "Total training time: 21.59 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1210.95, NNZs: 8192, Bias: -52.604986, T: 990000, Avg. loss: 656.253909\n",
      "Total training time: 21.68 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1054.97, NNZs: 8192, Bias: 48.082186, T: 990000, Avg. loss: 495.985748\n",
      "Total training time: 21.80 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1303.27, NNZs: 8192, Bias: -30.772551, T: 990000, Avg. loss: 593.094198\n",
      "Total training time: 21.80 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1145.37, NNZs: 8192, Bias: -51.797220, T: 990000, Avg. loss: 658.267667\n",
      "Total training time: 21.80 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1391.69, NNZs: 8192, Bias: -48.377831, T: 990000, Avg. loss: 650.454114\n",
      "Total training time: 21.87 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1115.59, NNZs: 8192, Bias: -55.486938, T: 990000, Avg. loss: 704.971216\n",
      "Total training time: 21.99 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1010.37, NNZs: 8192, Bias: -60.202288, T: 990000, Avg. loss: 766.533897\n",
      "Total training time: 22.20 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 992.83, NNZs: 8192, Bias: -19.113861, T: 990000, Avg. loss: 836.209079\n",
      "Total training time: 22.27 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 954.45, NNZs: 8192, Bias: -9.173104, T: 990000, Avg. loss: 724.389439\n",
      "Total training time: 22.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1174.51, NNZs: 8192, Bias: -5.374035, T: 1035000, Avg. loss: 439.093203\n",
      "Total training time: 22.58 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1183.99, NNZs: 8192, Bias: -52.595501, T: 1035000, Avg. loss: 641.106354\n",
      "Total training time: 22.68 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1032.83, NNZs: 8192, Bias: 48.121325, T: 1035000, Avg. loss: 472.306689\n",
      "Total training time: 22.83 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1273.47, NNZs: 8192, Bias: -30.861203, T: 1035000, Avg. loss: 559.917552\n",
      "Total training time: 22.82 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1124.62, NNZs: 8192, Bias: -51.856433, T: 1035000, Avg. loss: 632.188565\n",
      "Total training time: 22.81 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1364.72, NNZs: 8192, Bias: -48.486436, T: 1035000, Avg. loss: 615.308103\n",
      "Total training time: 22.91 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1089.69, NNZs: 8192, Bias: -55.594736, T: 1035000, Avg. loss: 667.589697\n",
      "Total training time: 23.04 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 990.51, NNZs: 8192, Bias: -60.232189, T: 1035000, Avg. loss: 730.220069\n",
      "Total training time: 23.26 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 972.21, NNZs: 8192, Bias: -19.104064, T: 1035000, Avg. loss: 796.529326\n",
      "Total training time: 23.34 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 938.73, NNZs: 8192, Bias: -9.114156, T: 1035000, Avg. loss: 676.292772\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1152.00, NNZs: 8192, Bias: -5.392658, T: 1080000, Avg. loss: 422.081974\n",
      "Total training time: 23.61 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1160.60, NNZs: 8192, Bias: -52.576446, T: 1080000, Avg. loss: 614.936595\n",
      "Total training time: 23.71 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1246.79, NNZs: 8192, Bias: -30.936676, T: 1080000, Avg. loss: 529.517918\n",
      "Total training time: 23.84 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1013.03, NNZs: 8192, Bias: 48.149976, T: 1080000, Avg. loss: 456.453436\n",
      "Total training time: 23.87 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1104.29, NNZs: 8192, Bias: -51.951189, T: 1080000, Avg. loss: 600.526085\n",
      "Total training time: 23.84 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1340.25, NNZs: 8192, Bias: -48.571751, T: 1080000, Avg. loss: 586.999189\n",
      "Total training time: 23.93 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1068.78, NNZs: 8192, Bias: -55.707946, T: 1080000, Avg. loss: 635.110756\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 969.17, NNZs: 8192, Bias: -60.251253, T: 1080000, Avg. loss: 698.076483\n",
      "Total training time: 24.28 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 955.41, NNZs: 8192, Bias: -19.114151, T: 1080000, Avg. loss: 762.371488\n",
      "Total training time: 24.36 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 925.32, NNZs: 8192, Bias: -9.123569, T: 1080000, Avg. loss: 647.049199\n",
      "Total training time: 24.52 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1129.31, NNZs: 8192, Bias: -5.374258, T: 1125000, Avg. loss: 402.641019\n",
      "Total training time: 24.59 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1138.18, NNZs: 8192, Bias: -52.567260, T: 1125000, Avg. loss: 579.077060\n",
      "Total training time: 24.70 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1223.78, NNZs: 8192, Bias: -31.009266, T: 1125000, Avg. loss: 506.812387\n",
      "Total training time: 24.86 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 995.44, NNZs: 8192, Bias: 48.204074, T: 1125000, Avg. loss: 433.395294\n",
      "Total training time: 24.88 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1085.37, NNZs: 8192, Bias: -52.005564, T: 1125000, Avg. loss: 583.456496\n",
      "Total training time: 24.85 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1320.28, NNZs: 8192, Bias: -48.635235, T: 1125000, Avg. loss: 560.542502\n",
      "Total training time: 24.95 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1047.66, NNZs: 8192, Bias: -55.807742, T: 1125000, Avg. loss: 613.869775\n",
      "Total training time: 25.08 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 948.85, NNZs: 8192, Bias: -60.287730, T: 1125000, Avg. loss: 670.604461\n",
      "Total training time: 25.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 936.96, NNZs: 8192, Bias: -19.132401, T: 1125000, Avg. loss: 721.453752\n",
      "Total training time: 25.39 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 910.28, NNZs: 8192, Bias: -9.042572, T: 1125000, Avg. loss: 624.918032\n",
      "Total training time: 25.57 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1109.89, NNZs: 8192, Bias: -5.409183, T: 1170000, Avg. loss: 383.978524\n",
      "Total training time: 25.59 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1117.82, NNZs: 8192, Bias: -52.566846, T: 1170000, Avg. loss: 557.679157\n",
      "Total training time: 25.72 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1199.14, NNZs: 8192, Bias: -31.079037, T: 1170000, Avg. loss: 494.309880\n",
      "Total training time: 25.87 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 977.99, NNZs: 8192, Bias: 48.247645, T: 1170000, Avg. loss: 413.841744\n",
      "Total training time: 25.91 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1066.78, NNZs: 8192, Bias: -52.067053, T: 1170000, Avg. loss: 557.672401\n",
      "Total training time: 25.86 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1297.59, NNZs: 8192, Bias: -48.722542, T: 1170000, Avg. loss: 540.069390\n",
      "Total training time: 25.95 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1027.84, NNZs: 8192, Bias: -55.912363, T: 1170000, Avg. loss: 584.848906\n",
      "Total training time: 26.11 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 935.20, NNZs: 8192, Bias: -60.348564, T: 1170000, Avg. loss: 646.682438\n",
      "Total training time: 26.34 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 921.43, NNZs: 8192, Bias: -19.132611, T: 1170000, Avg. loss: 690.106307\n",
      "Total training time: 26.43 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1088.94, NNZs: 8192, Bias: -5.375451, T: 1215000, Avg. loss: 363.267379\n",
      "Total training time: 26.59 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 896.25, NNZs: 8192, Bias: -9.034032, T: 1170000, Avg. loss: 602.005886\n",
      "Total training time: 26.64 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1099.45, NNZs: 8192, Bias: -52.558410, T: 1215000, Avg. loss: 533.898653\n",
      "Total training time: 26.73 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1176.82, NNZs: 8192, Bias: -31.162687, T: 1215000, Avg. loss: 469.559698\n",
      "Total training time: 26.91 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1046.94, NNZs: 8192, Bias: -52.126173, T: 1215000, Avg. loss: 530.498845\n",
      "Total training time: 26.89 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 961.26, NNZs: 8192, Bias: 48.305919, T: 1215000, Avg. loss: 393.904673\n",
      "Total training time: 26.95 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1275.76, NNZs: 8192, Bias: -48.789533, T: 1215000, Avg. loss: 516.214178\n",
      "Total training time: 26.99 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1009.13, NNZs: 8192, Bias: -56.004115, T: 1215000, Avg. loss: 569.222806\n",
      "Total training time: 27.17 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 919.67, NNZs: 8192, Bias: -60.382045, T: 1215000, Avg. loss: 620.197103\n",
      "Total training time: 27.39 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 904.76, NNZs: 8192, Bias: -19.132859, T: 1215000, Avg. loss: 674.010562\n",
      "Total training time: 27.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1070.49, NNZs: 8192, Bias: -5.359194, T: 1260000, Avg. loss: 347.872202\n",
      "Total training time: 27.61 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 883.00, NNZs: 8192, Bias: -9.000943, T: 1215000, Avg. loss: 573.661171\n",
      "Total training time: 27.73 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1078.48, NNZs: 8192, Bias: -52.542064, T: 1260000, Avg. loss: 518.705540\n",
      "Total training time: 27.77 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1155.71, NNZs: 8192, Bias: -31.235358, T: 1260000, Avg. loss: 449.189252\n",
      "Total training time: 27.94 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1031.55, NNZs: 8192, Bias: -52.191006, T: 1260000, Avg. loss: 504.568365\n",
      "Total training time: 27.91 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 946.48, NNZs: 8192, Bias: 48.330150, T: 1260000, Avg. loss: 374.951408\n",
      "Total training time: 27.98 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1255.01, NNZs: 8192, Bias: -48.870175, T: 1260000, Avg. loss: 487.345200\n",
      "Total training time: 28.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 992.32, NNZs: 8192, Bias: -56.060651, T: 1260000, Avg. loss: 536.510497\n",
      "Total training time: 28.20 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 906.47, NNZs: 8192, Bias: -60.406362, T: 1260000, Avg. loss: 588.666938\n",
      "Total training time: 28.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 890.85, NNZs: 8192, Bias: -19.149195, T: 1260000, Avg. loss: 642.856267\n",
      "Total training time: 28.53 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1053.55, NNZs: 8192, Bias: -5.374717, T: 1305000, Avg. loss: 332.337248\n",
      "Total training time: 28.60 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 870.95, NNZs: 8192, Bias: -8.984861, T: 1260000, Avg. loss: 552.601554\n",
      "Total training time: 28.74 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1061.90, NNZs: 8192, Bias: -52.510512, T: 1305000, Avg. loss: 497.604002\n",
      "Total training time: 28.75 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1136.46, NNZs: 8192, Bias: -31.321109, T: 1305000, Avg. loss: 425.485019\n",
      "Total training time: 28.92 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1016.05, NNZs: 8192, Bias: -52.253428, T: 1305000, Avg. loss: 486.220120\n",
      "Total training time: 28.92 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 930.12, NNZs: 8192, Bias: 48.345790, T: 1305000, Avg. loss: 365.396056\n",
      "Total training time: 28.98 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1235.07, NNZs: 8192, Bias: -48.924739, T: 1305000, Avg. loss: 473.861930\n",
      "Total training time: 29.03 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 977.54, NNZs: 8192, Bias: -56.170007, T: 1305000, Avg. loss: 512.198784\n",
      "Total training time: 29.19 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 894.20, NNZs: 8192, Bias: -60.461159, T: 1305000, Avg. loss: 569.217793\n",
      "Total training time: 29.43 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 876.76, NNZs: 8192, Bias: -19.180385, T: 1305000, Avg. loss: 617.314545\n",
      "Total training time: 29.56 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1036.64, NNZs: 8192, Bias: -5.367236, T: 1350000, Avg. loss: 325.845316\n",
      "Total training time: 29.60 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1046.89, NNZs: 8192, Bias: -52.495105, T: 1350000, Avg. loss: 473.436693\n",
      "Total training time: 29.74 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 859.04, NNZs: 8192, Bias: -8.976497, T: 1305000, Avg. loss: 536.800857\n",
      "Total training time: 29.78 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1117.69, NNZs: 8192, Bias: -31.388821, T: 1350000, Avg. loss: 408.773298\n",
      "Total training time: 29.90 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1001.39, NNZs: 8192, Bias: -52.291318, T: 1350000, Avg. loss: 472.978646\n",
      "Total training time: 29.91 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 916.60, NNZs: 8192, Bias: 48.390996, T: 1350000, Avg. loss: 353.400718\n",
      "Total training time: 29.98 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1215.50, NNZs: 8192, Bias: -48.962176, T: 1350000, Avg. loss: 459.791711\n",
      "Total training time: 30.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 962.11, NNZs: 8192, Bias: -56.253007, T: 1350000, Avg. loss: 502.218764\n",
      "Total training time: 30.20 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 880.25, NNZs: 8192, Bias: -60.483428, T: 1350000, Avg. loss: 561.363550\n",
      "Total training time: 30.49 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 862.92, NNZs: 8192, Bias: -19.188351, T: 1350000, Avg. loss: 596.974508\n",
      "Total training time: 30.58 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1022.02, NNZs: 8192, Bias: -5.367090, T: 1395000, Avg. loss: 307.385253\n",
      "Total training time: 30.59 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1031.50, NNZs: 8192, Bias: -52.458581, T: 1395000, Avg. loss: 455.816185\n",
      "Total training time: 30.75 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 847.20, NNZs: 8192, Bias: -8.946252, T: 1350000, Avg. loss: 507.371194\n",
      "Total training time: 30.83 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1098.99, NNZs: 8192, Bias: -31.461886, T: 1395000, Avg. loss: 397.128444\n",
      "Total training time: 30.91 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 987.84, NNZs: 8192, Bias: -52.364339, T: 1395000, Avg. loss: 449.752499\n",
      "Total training time: 30.93 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 902.52, NNZs: 8192, Bias: 48.398395, T: 1395000, Avg. loss: 340.908721\n",
      "Total training time: 31.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1196.46, NNZs: 8192, Bias: -49.027754, T: 1395000, Avg. loss: 435.724797\n",
      "Total training time: 31.06 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 946.18, NNZs: 8192, Bias: -56.311214, T: 1395000, Avg. loss: 482.266358\n",
      "Total training time: 31.24 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 868.24, NNZs: 8192, Bias: -60.490605, T: 1395000, Avg. loss: 537.076040\n",
      "Total training time: 31.51 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1007.59, NNZs: 8192, Bias: -5.360132, T: 1440000, Avg. loss: 305.771633\n",
      "Total training time: 31.57 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 851.18, NNZs: 8192, Bias: -19.180779, T: 1395000, Avg. loss: 573.805036\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1015.71, NNZs: 8192, Bias: -52.451520, T: 1440000, Avg. loss: 440.461399\n",
      "Total training time: 31.72 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 834.15, NNZs: 8192, Bias: -8.916750, T: 1395000, Avg. loss: 496.676338\n",
      "Total training time: 31.85 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1081.65, NNZs: 8192, Bias: -31.511170, T: 1440000, Avg. loss: 377.510275\n",
      "Total training time: 31.89 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 974.27, NNZs: 8192, Bias: -52.413833, T: 1440000, Avg. loss: 429.067446\n",
      "Total training time: 31.92 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 889.89, NNZs: 8192, Bias: 48.433623, T: 1440000, Avg. loss: 332.894106\n",
      "Total training time: 31.98 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1180.24, NNZs: 8192, Bias: -49.077128, T: 1440000, Avg. loss: 411.301923\n",
      "Total training time: 32.04 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 933.15, NNZs: 8192, Bias: -56.388815, T: 1440000, Avg. loss: 462.513045\n",
      "Total training time: 32.24 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 856.25, NNZs: 8192, Bias: -60.525808, T: 1440000, Avg. loss: 511.483396\n",
      "Total training time: 32.52 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 993.06, NNZs: 8192, Bias: -5.373927, T: 1485000, Avg. loss: 289.375884\n",
      "Total training time: 32.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 838.93, NNZs: 8192, Bias: -19.201916, T: 1440000, Avg. loss: 549.451961\n",
      "Total training time: 32.64 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1000.49, NNZs: 8192, Bias: -52.430788, T: 1485000, Avg. loss: 422.683633\n",
      "Total training time: 32.72 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1065.17, NNZs: 8192, Bias: -31.565955, T: 1485000, Avg. loss: 370.104459\n",
      "Total training time: 32.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 824.73, NNZs: 8192, Bias: -8.902249, T: 1440000, Avg. loss: 476.150717\n",
      "Total training time: 32.87 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 876.37, NNZs: 8192, Bias: 48.453805, T: 1485000, Avg. loss: 317.689993\n",
      "Total training time: 32.95 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 960.12, NNZs: 8192, Bias: -52.461613, T: 1485000, Avg. loss: 423.438356\n",
      "Total training time: 32.90 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1163.13, NNZs: 8192, Bias: -49.132038, T: 1485000, Avg. loss: 409.939049\n",
      "Total training time: 33.03 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 920.53, NNZs: 8192, Bias: -56.457237, T: 1485000, Avg. loss: 454.061487\n",
      "Total training time: 33.22 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 846.08, NNZs: 8192, Bias: -60.539619, T: 1485000, Avg. loss: 504.477598\n",
      "Total training time: 33.51 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 979.00, NNZs: 8192, Bias: -5.360794, T: 1530000, Avg. loss: 276.896376\n",
      "Total training time: 33.53 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 828.48, NNZs: 8192, Bias: -19.201803, T: 1485000, Avg. loss: 535.926017\n",
      "Total training time: 33.64 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 987.68, NNZs: 8192, Bias: -52.430640, T: 1530000, Avg. loss: 415.865722\n",
      "Total training time: 33.70 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1048.42, NNZs: 8192, Bias: -31.632114, T: 1530000, Avg. loss: 354.610609\n",
      "Total training time: 33.86 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 813.86, NNZs: 8192, Bias: -8.874909, T: 1485000, Avg. loss: 461.504843\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 865.05, NNZs: 8192, Bias: 48.486901, T: 1530000, Avg. loss: 308.864408\n",
      "Total training time: 33.94 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 946.26, NNZs: 8192, Bias: -52.508013, T: 1530000, Avg. loss: 407.313144\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1146.06, NNZs: 8192, Bias: -49.171934, T: 1530000, Avg. loss: 389.059999\n",
      "Total training time: 34.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 908.65, NNZs: 8192, Bias: -56.536946, T: 1530000, Avg. loss: 434.058372\n",
      "Total training time: 34.21 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 836.28, NNZs: 8192, Bias: -60.592909, T: 1530000, Avg. loss: 486.476851\n",
      "Total training time: 34.51 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 965.71, NNZs: 8192, Bias: -5.367226, T: 1575000, Avg. loss: 268.641307\n",
      "Total training time: 34.52 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 817.62, NNZs: 8192, Bias: -19.195027, T: 1530000, Avg. loss: 521.552399\n",
      "Total training time: 34.65 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 972.85, NNZs: 8192, Bias: -52.411251, T: 1575000, Avg. loss: 397.210574\n",
      "Total training time: 34.68 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1033.19, NNZs: 8192, Bias: -31.670631, T: 1575000, Avg. loss: 347.113003\n",
      "Total training time: 34.84 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 934.05, NNZs: 8192, Bias: -52.559364, T: 1575000, Avg. loss: 400.194584\n",
      "Total training time: 34.86 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 804.72, NNZs: 8192, Bias: -8.855280, T: 1530000, Avg. loss: 451.246309\n",
      "Total training time: 34.90 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 854.13, NNZs: 8192, Bias: 48.506229, T: 1575000, Avg. loss: 294.717563\n",
      "Total training time: 34.94 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1130.32, NNZs: 8192, Bias: -49.217036, T: 1575000, Avg. loss: 379.655638\n",
      "Total training time: 35.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 896.36, NNZs: 8192, Bias: -56.607938, T: 1575000, Avg. loss: 422.851009\n",
      "Total training time: 35.24 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 825.68, NNZs: 8192, Bias: -60.618332, T: 1575000, Avg. loss: 461.450295\n",
      "Total training time: 35.56 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 951.97, NNZs: 8192, Bias: -5.367193, T: 1620000, Avg. loss: 260.089431\n",
      "Total training time: 35.55 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 959.62, NNZs: 8192, Bias: -52.411063, T: 1620000, Avg. loss: 377.322064\n",
      "Total training time: 35.72 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 807.25, NNZs: 8192, Bias: -19.208278, T: 1575000, Avg. loss: 507.839425\n",
      "Total training time: 35.72 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1019.60, NNZs: 8192, Bias: -31.708056, T: 1620000, Avg. loss: 337.932926\n",
      "Total training time: 35.88 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 922.32, NNZs: 8192, Bias: -52.590450, T: 1620000, Avg. loss: 381.519529\n",
      "Total training time: 35.91 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 843.39, NNZs: 8192, Bias: 48.518890, T: 1620000, Avg. loss: 284.370031\n",
      "Total training time: 35.98 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 795.30, NNZs: 8192, Bias: -8.849132, T: 1575000, Avg. loss: 437.505625\n",
      "Total training time: 35.98 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1115.65, NNZs: 8192, Bias: -49.267008, T: 1620000, Avg. loss: 362.418863\n",
      "Total training time: 36.06 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 883.73, NNZs: 8192, Bias: -56.676928, T: 1620000, Avg. loss: 415.068706\n",
      "Total training time: 36.27 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 939.45, NNZs: 8192, Bias: -5.373321, T: 1665000, Avg. loss: 255.259352\n",
      "Total training time: 36.52 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 816.27, NNZs: 8192, Bias: -60.643139, T: 1620000, Avg. loss: 455.824608\n",
      "Total training time: 36.58 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 947.42, NNZs: 8192, Bias: -52.386634, T: 1665000, Avg. loss: 368.316164\n",
      "Total training time: 36.72 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 796.69, NNZs: 8192, Bias: -19.227104, T: 1620000, Avg. loss: 490.756883\n",
      "Total training time: 36.75 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1005.39, NNZs: 8192, Bias: -31.756769, T: 1665000, Avg. loss: 324.703042\n",
      "Total training time: 36.88 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 912.70, NNZs: 8192, Bias: -52.632977, T: 1665000, Avg. loss: 374.115067\n",
      "Total training time: 36.92 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 832.47, NNZs: 8192, Bias: 48.555263, T: 1665000, Avg. loss: 273.314514\n",
      "Total training time: 37.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 786.66, NNZs: 8192, Bias: -8.830372, T: 1620000, Avg. loss: 422.379999\n",
      "Total training time: 37.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1102.09, NNZs: 8192, Bias: -49.303457, T: 1665000, Avg. loss: 356.012477\n",
      "Total training time: 37.05 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 872.67, NNZs: 8192, Bias: -56.731695, T: 1665000, Avg. loss: 397.162633\n",
      "Total training time: 37.28 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 927.50, NNZs: 8192, Bias: -5.361677, T: 1710000, Avg. loss: 241.370305\n",
      "Total training time: 37.50 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 805.79, NNZs: 8192, Bias: -60.655338, T: 1665000, Avg. loss: 441.103327\n",
      "Total training time: 37.58 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 935.21, NNZs: 8192, Bias: -52.386773, T: 1710000, Avg. loss: 364.442125\n",
      "Total training time: 37.68 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 786.55, NNZs: 8192, Bias: -19.221224, T: 1665000, Avg. loss: 475.157036\n",
      "Total training time: 37.74 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 992.43, NNZs: 8192, Bias: -31.816075, T: 1710000, Avg. loss: 312.716209\n",
      "Total training time: 37.86 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 903.38, NNZs: 8192, Bias: -52.680132, T: 1710000, Avg. loss: 357.930389\n",
      "Total training time: 37.90 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 823.50, NNZs: 8192, Bias: 48.590531, T: 1710000, Avg. loss: 267.903467\n",
      "Total training time: 37.99 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 779.03, NNZs: 8192, Bias: -8.806130, T: 1665000, Avg. loss: 405.333853\n",
      "Total training time: 38.04 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1088.58, NNZs: 8192, Bias: -49.327287, T: 1710000, Avg. loss: 347.665195\n",
      "Total training time: 38.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 861.36, NNZs: 8192, Bias: -56.785084, T: 1710000, Avg. loss: 389.487438\n",
      "Total training time: 38.27 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 916.23, NNZs: 8192, Bias: -5.367521, T: 1755000, Avg. loss: 234.768155\n",
      "Total training time: 38.48 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 796.44, NNZs: 8192, Bias: -60.673111, T: 1710000, Avg. loss: 424.264724\n",
      "Total training time: 38.60 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 923.17, NNZs: 8192, Bias: -52.375405, T: 1755000, Avg. loss: 356.220464\n",
      "Total training time: 38.68 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 778.00, NNZs: 8192, Bias: -19.233056, T: 1710000, Avg. loss: 467.356879\n",
      "Total training time: 38.76 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 980.10, NNZs: 8192, Bias: -31.873863, T: 1755000, Avg. loss: 307.360104\n",
      "Total training time: 38.85 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 892.76, NNZs: 8192, Bias: -52.714497, T: 1755000, Avg. loss: 348.721398\n",
      "Total training time: 38.88 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 814.69, NNZs: 8192, Bias: 48.602062, T: 1755000, Avg. loss: 259.038374\n",
      "Total training time: 38.99 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1075.37, NNZs: 8192, Bias: -49.368008, T: 1755000, Avg. loss: 330.432051\n",
      "Total training time: 39.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 771.37, NNZs: 8192, Bias: -8.788662, T: 1710000, Avg. loss: 389.495850\n",
      "Total training time: 39.08 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 851.13, NNZs: 8192, Bias: -56.837075, T: 1755000, Avg. loss: 380.360523\n",
      "Total training time: 39.29 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 905.06, NNZs: 8192, Bias: -5.367560, T: 1800000, Avg. loss: 235.954756\n",
      "Total training time: 39.46 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 788.31, NNZs: 8192, Bias: -60.702078, T: 1755000, Avg. loss: 417.681558\n",
      "Total training time: 39.61 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 911.47, NNZs: 8192, Bias: -52.375533, T: 1800000, Avg. loss: 343.521736\n",
      "Total training time: 39.67 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 769.38, NNZs: 8192, Bias: -19.244434, T: 1755000, Avg. loss: 446.659694\n",
      "Total training time: 39.77 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 967.71, NNZs: 8192, Bias: -31.907710, T: 1800000, Avg. loss: 298.155855\n",
      "Total training time: 39.84 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 883.81, NNZs: 8192, Bias: -52.759366, T: 1800000, Avg. loss: 339.690253\n",
      "Total training time: 39.87 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 804.99, NNZs: 8192, Bias: 48.602065, T: 1800000, Avg. loss: 254.444179\n",
      "Total training time: 39.98 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1063.38, NNZs: 8192, Bias: -49.418614, T: 1800000, Avg. loss: 327.254029\n",
      "Total training time: 40.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 763.85, NNZs: 8192, Bias: -8.777040, T: 1755000, Avg. loss: 389.657956\n",
      "Total training time: 40.09 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 839.52, NNZs: 8192, Bias: -56.893241, T: 1800000, Avg. loss: 370.211089\n",
      "Total training time: 40.30 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 894.38, NNZs: 8192, Bias: -5.367753, T: 1845000, Avg. loss: 223.329674\n",
      "Total training time: 40.46 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 780.90, NNZs: 8192, Bias: -60.724688, T: 1800000, Avg. loss: 406.047681\n",
      "Total training time: 40.66 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 901.93, NNZs: 8192, Bias: -52.364619, T: 1845000, Avg. loss: 335.247952\n",
      "Total training time: 40.70 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 761.77, NNZs: 8192, Bias: -19.261261, T: 1800000, Avg. loss: 435.454815\n",
      "Total training time: 40.84 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 957.35, NNZs: 8192, Bias: -31.957076, T: 1845000, Avg. loss: 285.105201\n",
      "Total training time: 40.87 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 873.87, NNZs: 8192, Bias: -52.797612, T: 1845000, Avg. loss: 329.042586\n",
      "Total training time: 40.91 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 795.05, NNZs: 8192, Bias: 48.602209, T: 1845000, Avg. loss: 249.059156\n",
      "Total training time: 41.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1051.06, NNZs: 8192, Bias: -49.445994, T: 1845000, Avg. loss: 312.407825\n",
      "Total training time: 41.07 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 755.13, NNZs: 8192, Bias: -8.788428, T: 1800000, Avg. loss: 377.852746\n",
      "Total training time: 41.19 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 830.29, NNZs: 8192, Bias: -56.942541, T: 1845000, Avg. loss: 358.380154\n",
      "Total training time: 41.40 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 885.05, NNZs: 8192, Bias: -5.373112, T: 1890000, Avg. loss: 217.078717\n",
      "Total training time: 41.56 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 773.92, NNZs: 8192, Bias: -60.752341, T: 1845000, Avg. loss: 388.698487\n",
      "Total training time: 41.77 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 891.58, NNZs: 8192, Bias: -52.353945, T: 1890000, Avg. loss: 321.329326\n",
      "Total training time: 41.80 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 947.08, NNZs: 8192, Bias: -32.016057, T: 1890000, Avg. loss: 278.881044\n",
      "Total training time: 41.96 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 753.81, NNZs: 8192, Bias: -19.250311, T: 1845000, Avg. loss: 428.835341\n",
      "Total training time: 41.97 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 864.37, NNZs: 8192, Bias: -52.813763, T: 1890000, Avg. loss: 320.530861\n",
      "Total training time: 42.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 786.07, NNZs: 8192, Bias: 48.623856, T: 1890000, Avg. loss: 240.057808\n",
      "Total training time: 42.11 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1039.13, NNZs: 8192, Bias: -49.483345, T: 1890000, Avg. loss: 304.126329\n",
      "Total training time: 42.14 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 748.09, NNZs: 8192, Bias: -8.760882, T: 1845000, Avg. loss: 366.779729\n",
      "Total training time: 42.30 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 820.70, NNZs: 8192, Bias: -56.990773, T: 1890000, Avg. loss: 349.087720\n",
      "Total training time: 42.49 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 874.85, NNZs: 8192, Bias: -5.373147, T: 1935000, Avg. loss: 214.164371\n",
      "Total training time: 42.61 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 881.96, NNZs: 8192, Bias: -52.348692, T: 1935000, Avg. loss: 315.303761\n",
      "Total training time: 42.88 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 765.29, NNZs: 8192, Bias: -60.768460, T: 1890000, Avg. loss: 381.320433\n",
      "Total training time: 42.88 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 936.31, NNZs: 8192, Bias: -32.042283, T: 1935000, Avg. loss: 273.820979\n",
      "Total training time: 43.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 745.43, NNZs: 8192, Bias: -19.261084, T: 1890000, Avg. loss: 412.686340\n",
      "Total training time: 43.08 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 856.36, NNZs: 8192, Bias: -52.850475, T: 1935000, Avg. loss: 313.720964\n",
      "Total training time: 43.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 778.10, NNZs: 8192, Bias: 48.629135, T: 1935000, Avg. loss: 238.422628\n",
      "Total training time: 43.19 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1028.01, NNZs: 8192, Bias: -49.504242, T: 1935000, Avg. loss: 301.079686\n",
      "Total training time: 43.24 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 739.98, NNZs: 8192, Bias: -8.750438, T: 1890000, Avg. loss: 355.436616\n",
      "Total training time: 43.46 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 811.57, NNZs: 8192, Bias: -57.037766, T: 1935000, Avg. loss: 338.306870\n",
      "Total training time: 43.63 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 865.32, NNZs: 8192, Bias: -5.373154, T: 1980000, Avg. loss: 207.428227\n",
      "Total training time: 43.74 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 872.78, NNZs: 8192, Bias: -52.333361, T: 1980000, Avg. loss: 313.656190\n",
      "Total training time: 44.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 758.52, NNZs: 8192, Bias: -60.789372, T: 1935000, Avg. loss: 374.115124\n",
      "Total training time: 44.04 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 926.34, NNZs: 8192, Bias: -32.088286, T: 1980000, Avg. loss: 266.974332\n",
      "Total training time: 44.17 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 848.22, NNZs: 8192, Bias: -52.886217, T: 1980000, Avg. loss: 300.089972\n",
      "Total training time: 44.23 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 737.49, NNZs: 8192, Bias: -19.255818, T: 1935000, Avg. loss: 403.570740\n",
      "Total training time: 44.26 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 769.87, NNZs: 8192, Bias: 48.649755, T: 1980000, Avg. loss: 227.655162\n",
      "Total training time: 44.34 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1017.11, NNZs: 8192, Bias: -49.540104, T: 1980000, Avg. loss: 291.992164\n",
      "Total training time: 44.35 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 732.98, NNZs: 8192, Bias: -8.745419, T: 1935000, Avg. loss: 349.046963\n",
      "Total training time: 44.59 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 803.86, NNZs: 8192, Bias: -57.078470, T: 1980000, Avg. loss: 336.502836\n",
      "Total training time: 44.72 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 856.30, NNZs: 8192, Bias: -5.373219, T: 2025000, Avg. loss: 199.997447\n",
      "Total training time: 44.79 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 862.97, NNZs: 8192, Bias: -52.338226, T: 2025000, Avg. loss: 306.791512\n",
      "Total training time: 45.08 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 750.87, NNZs: 8192, Bias: -60.820085, T: 1980000, Avg. loss: 363.073472\n",
      "Total training time: 45.13 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 917.24, NNZs: 8192, Bias: -32.118085, T: 2025000, Avg. loss: 260.744743\n",
      "Total training time: 45.22 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 839.51, NNZs: 8192, Bias: -52.906194, T: 2025000, Avg. loss: 295.059451\n",
      "Total training time: 45.30 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 729.72, NNZs: 8192, Bias: -19.260910, T: 1980000, Avg. loss: 396.541009\n",
      "Total training time: 45.35 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 762.00, NNZs: 8192, Bias: 48.674713, T: 2025000, Avg. loss: 221.533206\n",
      "Total training time: 45.40 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1005.82, NNZs: 8192, Bias: -49.565021, T: 2025000, Avg. loss: 283.917480\n",
      "Total training time: 45.41 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 725.71, NNZs: 8192, Bias: -8.750612, T: 1980000, Avg. loss: 338.290497\n",
      "Total training time: 45.69 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 795.34, NNZs: 8192, Bias: -57.108479, T: 2025000, Avg. loss: 323.724059\n",
      "Total training time: 45.79 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 846.46, NNZs: 8192, Bias: -5.368364, T: 2070000, Avg. loss: 197.016790\n",
      "Total training time: 45.83 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 854.75, NNZs: 8192, Bias: -52.323535, T: 2070000, Avg. loss: 291.340239\n",
      "Total training time: 46.12 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 743.91, NNZs: 8192, Bias: -60.835034, T: 2025000, Avg. loss: 354.491571\n",
      "Total training time: 46.20 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 906.08, NNZs: 8192, Bias: -32.157190, T: 2070000, Avg. loss: 254.976305\n",
      "Total training time: 46.27 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 832.24, NNZs: 8192, Bias: -52.935431, T: 2070000, Avg. loss: 288.954719\n",
      "Total training time: 46.34 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 755.37, NNZs: 8192, Bias: 48.689295, T: 2070000, Avg. loss: 218.636540\n",
      "Total training time: 46.46 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 723.34, NNZs: 8192, Bias: -19.265960, T: 2025000, Avg. loss: 380.892381\n",
      "Total training time: 46.44 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 995.13, NNZs: 8192, Bias: -49.584589, T: 2070000, Avg. loss: 276.101080\n",
      "Total training time: 46.44 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 718.93, NNZs: 8192, Bias: -8.735727, T: 2025000, Avg. loss: 331.150241\n",
      "Total training time: 46.77 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 787.10, NNZs: 8192, Bias: -57.162235, T: 2070000, Avg. loss: 314.569432\n",
      "Total training time: 46.83 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 838.24, NNZs: 8192, Bias: -5.368361, T: 2115000, Avg. loss: 188.728560\n",
      "Total training time: 46.85 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 847.32, NNZs: 8192, Bias: -52.328402, T: 2115000, Avg. loss: 282.414022\n",
      "Total training time: 47.13 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 736.86, NNZs: 8192, Bias: -60.840085, T: 2070000, Avg. loss: 348.635921\n",
      "Total training time: 47.23 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 897.37, NNZs: 8192, Bias: -32.190655, T: 2115000, Avg. loss: 246.776840\n",
      "Total training time: 47.28 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 824.14, NNZs: 8192, Bias: -52.959122, T: 2115000, Avg. loss: 284.020523\n",
      "Total training time: 47.34 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 748.33, NNZs: 8192, Bias: 48.708432, T: 2115000, Avg. loss: 213.587940\n",
      "Total training time: 47.47 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 985.64, NNZs: 8192, Bias: -49.603685, T: 2115000, Avg. loss: 270.697617\n",
      "Total training time: 47.46 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 717.03, NNZs: 8192, Bias: -19.285511, T: 2070000, Avg. loss: 373.398924\n",
      "Total training time: 47.47 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 712.75, NNZs: 8192, Bias: -8.721220, T: 2070000, Avg. loss: 324.157115\n",
      "Total training time: 47.82 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 779.01, NNZs: 8192, Bias: -57.190916, T: 2115000, Avg. loss: 309.852178\n",
      "Total training time: 47.85 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 830.62, NNZs: 8192, Bias: -5.363634, T: 2160000, Avg. loss: 185.737041\n",
      "Total training time: 47.85 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 839.24, NNZs: 8192, Bias: -52.319125, T: 2160000, Avg. loss: 278.043246\n",
      "Total training time: 48.14 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 730.65, NNZs: 8192, Bias: -60.864076, T: 2115000, Avg. loss: 336.876267\n",
      "Total training time: 48.25 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 889.66, NNZs: 8192, Bias: -32.237385, T: 2160000, Avg. loss: 238.118495\n",
      "Total training time: 48.30 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 816.16, NNZs: 8192, Bias: -52.982369, T: 2160000, Avg. loss: 276.289271\n",
      "Total training time: 48.34 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 740.65, NNZs: 8192, Bias: 48.717761, T: 2160000, Avg. loss: 207.565327\n",
      "Total training time: 48.48 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 975.94, NNZs: 8192, Bias: -49.631724, T: 2160000, Avg. loss: 264.196574\n",
      "Total training time: 48.47 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 710.37, NNZs: 8192, Bias: -19.285408, T: 2115000, Avg. loss: 363.807763\n",
      "Total training time: 48.49 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 770.60, NNZs: 8192, Bias: -57.233056, T: 2160000, Avg. loss: 300.146046\n",
      "Total training time: 48.86 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 821.93, NNZs: 8192, Bias: -5.363502, T: 2205000, Avg. loss: 178.713522\n",
      "Total training time: 48.85 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 707.07, NNZs: 8192, Bias: -8.711751, T: 2115000, Avg. loss: 311.730730\n",
      "Total training time: 48.87 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 831.08, NNZs: 8192, Bias: -52.314637, T: 2205000, Avg. loss: 271.372177\n",
      "Total training time: 49.16 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 724.30, NNZs: 8192, Bias: -60.878150, T: 2160000, Avg. loss: 327.979119\n",
      "Total training time: 49.30 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 881.18, NNZs: 8192, Bias: -32.269332, T: 2205000, Avg. loss: 235.822386\n",
      "Total training time: 49.34 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 808.16, NNZs: 8192, Bias: -53.005268, T: 2205000, Avg. loss: 273.626245\n",
      "Total training time: 49.37 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 734.35, NNZs: 8192, Bias: 48.735884, T: 2205000, Avg. loss: 200.921842\n",
      "Total training time: 49.52 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 967.09, NNZs: 8192, Bias: -49.659126, T: 2205000, Avg. loss: 257.131708\n",
      "Total training time: 49.51 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 704.45, NNZs: 8192, Bias: -19.290080, T: 2160000, Avg. loss: 356.898811\n",
      "Total training time: 49.55 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 813.68, NNZs: 8192, Bias: -5.358979, T: 2250000, Avg. loss: 180.480037\n",
      "Total training time: 49.88 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 763.90, NNZs: 8192, Bias: -57.283390, T: 2205000, Avg. loss: 296.280142\n",
      "Total training time: 49.92 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 700.92, NNZs: 8192, Bias: -8.702381, T: 2160000, Avg. loss: 311.661066\n",
      "Total training time: 49.94 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 823.45, NNZs: 8192, Bias: -52.296719, T: 2250000, Avg. loss: 267.968199\n",
      "Total training time: 50.18 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 718.15, NNZs: 8192, Bias: -60.896413, T: 2205000, Avg. loss: 320.103947\n",
      "Total training time: 50.36 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 872.68, NNZs: 8192, Bias: -32.296257, T: 2250000, Avg. loss: 228.364225\n",
      "Total training time: 50.39 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 801.52, NNZs: 8192, Bias: -53.036797, T: 2250000, Avg. loss: 265.669339\n",
      "Total training time: 50.42 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 727.65, NNZs: 8192, Bias: 48.749410, T: 2250000, Avg. loss: 196.941188\n",
      "Total training time: 50.56 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 957.78, NNZs: 8192, Bias: -49.681646, T: 2250000, Avg. loss: 246.920887\n",
      "Total training time: 50.55 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 698.92, NNZs: 8192, Bias: -19.290054, T: 2205000, Avg. loss: 350.156538\n",
      "Total training time: 50.62 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 806.16, NNZs: 8192, Bias: -5.358997, T: 2295000, Avg. loss: 174.425754\n",
      "Total training time: 50.89 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 756.10, NNZs: 8192, Bias: -57.328284, T: 2250000, Avg. loss: 285.001500\n",
      "Total training time: 50.95 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 694.76, NNZs: 8192, Bias: -8.683997, T: 2205000, Avg. loss: 300.474664\n",
      "Total training time: 51.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 815.91, NNZs: 8192, Bias: -52.301110, T: 2295000, Avg. loss: 262.416174\n",
      "Total training time: 51.22 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 864.37, NNZs: 8192, Bias: -32.327065, T: 2295000, Avg. loss: 222.218550\n",
      "Total training time: 51.39 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 712.55, NNZs: 8192, Bias: -60.905345, T: 2250000, Avg. loss: 312.460851\n",
      "Total training time: 51.40 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 794.98, NNZs: 8192, Bias: -53.058813, T: 2295000, Avg. loss: 260.714824\n",
      "Total training time: 51.46 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 720.97, NNZs: 8192, Bias: 48.767122, T: 2295000, Avg. loss: 195.768065\n",
      "Total training time: 51.58 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 949.27, NNZs: 8192, Bias: -49.694896, T: 2295000, Avg. loss: 238.689151\n",
      "Total training time: 51.57 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 693.72, NNZs: 8192, Bias: -19.299048, T: 2250000, Avg. loss: 340.301338\n",
      "Total training time: 51.69 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 799.01, NNZs: 8192, Bias: -5.363338, T: 2340000, Avg. loss: 170.332445\n",
      "Total training time: 51.91 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 748.93, NNZs: 8192, Bias: -57.367893, T: 2295000, Avg. loss: 282.435804\n",
      "Total training time: 51.99 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 688.96, NNZs: 8192, Bias: -8.666042, T: 2250000, Avg. loss: 295.333738\n",
      "Total training time: 52.04 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 808.84, NNZs: 8192, Bias: -52.296747, T: 2340000, Avg. loss: 254.588663\n",
      "Total training time: 52.24 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 854.80, NNZs: 8192, Bias: -32.365897, T: 2340000, Avg. loss: 220.329714\n",
      "Total training time: 52.41 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 706.74, NNZs: 8192, Bias: -60.918529, T: 2295000, Avg. loss: 305.187400\n",
      "Total training time: 52.45 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 788.61, NNZs: 8192, Bias: -53.084711, T: 2340000, Avg. loss: 253.849519\n",
      "Total training time: 52.49 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 714.63, NNZs: 8192, Bias: 48.780092, T: 2340000, Avg. loss: 190.469226\n",
      "Total training time: 52.61 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 940.13, NNZs: 8192, Bias: -49.725088, T: 2340000, Avg. loss: 238.091272\n",
      "Total training time: 52.61 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 687.74, NNZs: 8192, Bias: -19.303426, T: 2295000, Avg. loss: 337.870668\n",
      "Total training time: 52.74 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 791.80, NNZs: 8192, Bias: -5.367615, T: 2385000, Avg. loss: 166.006300\n",
      "Total training time: 52.94 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 742.24, NNZs: 8192, Bias: -57.393843, T: 2340000, Avg. loss: 273.398793\n",
      "Total training time: 53.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 683.05, NNZs: 8192, Bias: -8.661716, T: 2295000, Avg. loss: 289.763541\n",
      "Total training time: 53.09 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 802.33, NNZs: 8192, Bias: -52.292452, T: 2385000, Avg. loss: 243.231970\n",
      "Total training time: 53.26 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 847.81, NNZs: 8192, Bias: -32.403958, T: 2385000, Avg. loss: 214.036359\n",
      "Total training time: 53.45 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 701.16, NNZs: 8192, Bias: -60.927127, T: 2340000, Avg. loss: 305.084363\n",
      "Total training time: 53.50 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 781.40, NNZs: 8192, Bias: -53.131148, T: 2385000, Avg. loss: 245.948836\n",
      "Total training time: 53.52 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 708.37, NNZs: 8192, Bias: 48.792761, T: 2385000, Avg. loss: 186.329007\n",
      "Total training time: 53.64 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 931.12, NNZs: 8192, Bias: -49.737796, T: 2385000, Avg. loss: 231.452831\n",
      "Total training time: 53.63 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 682.20, NNZs: 8192, Bias: -19.307680, T: 2340000, Avg. loss: 331.706883\n",
      "Total training time: 53.81 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 784.76, NNZs: 8192, Bias: -5.371661, T: 2430000, Avg. loss: 162.106561\n",
      "Total training time: 53.95 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 735.81, NNZs: 8192, Bias: -57.436209, T: 2385000, Avg. loss: 270.986870\n",
      "Total training time: 54.06 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 677.93, NNZs: 8192, Bias: -8.666042, T: 2340000, Avg. loss: 280.525236\n",
      "Total training time: 54.15 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 795.58, NNZs: 8192, Bias: -52.288213, T: 2430000, Avg. loss: 244.497898\n",
      "Total training time: 54.28 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 839.27, NNZs: 8192, Bias: -32.432925, T: 2430000, Avg. loss: 210.573784\n",
      "Total training time: 54.44 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 697.20, NNZs: 8192, Bias: -60.948221, T: 2385000, Avg. loss: 297.745155\n",
      "Total training time: 54.55 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 774.43, NNZs: 8192, Bias: -53.147682, T: 2430000, Avg. loss: 239.842358\n",
      "Total training time: 54.55 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 702.62, NNZs: 8192, Bias: 48.801044, T: 2430000, Avg. loss: 179.902534\n",
      "Total training time: 54.66 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 923.16, NNZs: 8192, Bias: -49.766873, T: 2430000, Avg. loss: 228.740343\n",
      "Total training time: 54.67 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 677.00, NNZs: 8192, Bias: -19.303487, T: 2385000, Avg. loss: 322.725854\n",
      "Total training time: 54.85 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 778.24, NNZs: 8192, Bias: -5.371611, T: 2475000, Avg. loss: 159.076800\n",
      "Total training time: 54.96 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 729.66, NNZs: 8192, Bias: -57.469447, T: 2430000, Avg. loss: 262.118827\n",
      "Total training time: 55.08 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 672.64, NNZs: 8192, Bias: -8.670320, T: 2385000, Avg. loss: 277.760287\n",
      "Total training time: 55.20 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 789.09, NNZs: 8192, Bias: -52.296430, T: 2475000, Avg. loss: 242.220370\n",
      "Total training time: 55.29 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 832.50, NNZs: 8192, Bias: -32.461343, T: 2475000, Avg. loss: 203.116403\n",
      "Total training time: 55.48 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 768.49, NNZs: 8192, Bias: -53.163951, T: 2475000, Avg. loss: 239.862315\n",
      "Total training time: 55.56 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 693.04, NNZs: 8192, Bias: -60.969041, T: 2430000, Avg. loss: 291.393965\n",
      "Total training time: 55.60 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 696.84, NNZs: 8192, Bias: 48.821593, T: 2475000, Avg. loss: 179.806011\n",
      "Total training time: 55.67 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 915.05, NNZs: 8192, Bias: -49.783222, T: 2475000, Avg. loss: 227.582760\n",
      "Total training time: 55.70 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 671.67, NNZs: 8192, Bias: -19.299259, T: 2430000, Avg. loss: 317.326579\n",
      "Total training time: 55.93 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 771.57, NNZs: 8192, Bias: -5.371643, T: 2520000, Avg. loss: 157.102783\n",
      "Total training time: 55.98 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 722.84, NNZs: 8192, Bias: -57.493921, T: 2475000, Avg. loss: 259.962297\n",
      "Total training time: 56.13 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 667.77, NNZs: 8192, Bias: -8.662090, T: 2430000, Avg. loss: 271.038465\n",
      "Total training time: 56.26 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 783.21, NNZs: 8192, Bias: -52.296348, T: 2520000, Avg. loss: 234.395179\n",
      "Total training time: 56.32 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 825.29, NNZs: 8192, Bias: -32.489340, T: 2520000, Avg. loss: 201.259302\n",
      "Total training time: 56.48 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 763.16, NNZs: 8192, Bias: -53.187932, T: 2520000, Avg. loss: 231.484152\n",
      "Total training time: 56.58 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 688.20, NNZs: 8192, Bias: -60.973161, T: 2475000, Avg. loss: 287.516972\n",
      "Total training time: 56.63 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 691.43, NNZs: 8192, Bias: 48.837554, T: 2520000, Avg. loss: 173.177922\n",
      "Total training time: 56.70 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 907.33, NNZs: 8192, Bias: -49.815226, T: 2520000, Avg. loss: 222.088581\n",
      "Total training time: 56.70 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 666.51, NNZs: 8192, Bias: -19.307374, T: 2475000, Avg. loss: 310.842745\n",
      "Total training time: 56.97 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 765.54, NNZs: 8192, Bias: -5.371596, T: 2565000, Avg. loss: 154.072440\n",
      "Total training time: 57.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 717.01, NNZs: 8192, Bias: -57.533891, T: 2520000, Avg. loss: 254.885542\n",
      "Total training time: 57.16 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 662.39, NNZs: 8192, Bias: -8.653941, T: 2475000, Avg. loss: 268.270299\n",
      "Total training time: 57.33 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 775.91, NNZs: 8192, Bias: -52.284412, T: 2565000, Avg. loss: 233.088761\n",
      "Total training time: 57.34 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 818.32, NNZs: 8192, Bias: -32.520702, T: 2565000, Avg. loss: 199.182900\n",
      "Total training time: 57.51 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 757.94, NNZs: 8192, Bias: -53.211509, T: 2565000, Avg. loss: 224.882156\n",
      "Total training time: 57.61 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 683.26, NNZs: 8192, Bias: -61.005272, T: 2520000, Avg. loss: 278.033068\n",
      "Total training time: 57.68 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 685.75, NNZs: 8192, Bias: 48.861105, T: 2565000, Avg. loss: 173.282807\n",
      "Total training time: 57.73 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 899.86, NNZs: 8192, Bias: -49.834897, T: 2565000, Avg. loss: 212.949181\n",
      "Total training time: 57.72 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 759.18, NNZs: 8192, Bias: -5.379290, T: 2610000, Avg. loss: 150.001709\n",
      "Total training time: 57.98 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 661.41, NNZs: 8192, Bias: -19.303356, T: 2520000, Avg. loss: 307.085221\n",
      "Total training time: 58.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 710.79, NNZs: 8192, Bias: -57.573069, T: 2565000, Avg. loss: 245.306979\n",
      "Total training time: 58.16 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 770.73, NNZs: 8192, Bias: -52.272749, T: 2610000, Avg. loss: 222.884811\n",
      "Total training time: 58.34 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 657.24, NNZs: 8192, Bias: -8.642014, T: 2520000, Avg. loss: 260.455725\n",
      "Total training time: 58.38 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 811.56, NNZs: 8192, Bias: -32.551614, T: 2610000, Avg. loss: 192.097593\n",
      "Total training time: 58.53 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 752.79, NNZs: 8192, Bias: -53.230884, T: 2610000, Avg. loss: 218.726802\n",
      "Total training time: 58.61 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 677.73, NNZs: 8192, Bias: -61.020934, T: 2565000, Avg. loss: 278.571924\n",
      "Total training time: 58.70 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 680.21, NNZs: 8192, Bias: 48.868820, T: 2610000, Avg. loss: 169.193470\n",
      "Total training time: 58.75 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 892.59, NNZs: 8192, Bias: -49.854205, T: 2610000, Avg. loss: 209.099911\n",
      "Total training time: 58.73 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 753.72, NNZs: 8192, Bias: -5.379279, T: 2655000, Avg. loss: 146.728531\n",
      "Total training time: 58.96 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 655.98, NNZs: 8192, Bias: -19.315117, T: 2565000, Avg. loss: 300.139346\n",
      "Total training time: 59.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 704.90, NNZs: 8192, Bias: -57.596263, T: 2610000, Avg. loss: 248.285734\n",
      "Total training time: 59.18 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 764.54, NNZs: 8192, Bias: -52.261358, T: 2655000, Avg. loss: 223.845853\n",
      "Total training time: 59.35 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 653.22, NNZs: 8192, Bias: -8.641975, T: 2565000, Avg. loss: 256.512581\n",
      "Total training time: 59.44 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 805.44, NNZs: 8192, Bias: -32.578235, T: 2655000, Avg. loss: 188.784516\n",
      "Total training time: 59.57 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 747.53, NNZs: 8192, Bias: -53.253735, T: 2655000, Avg. loss: 216.469672\n",
      "Total training time: 59.64 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 673.01, NNZs: 8192, Bias: -61.024790, T: 2610000, Avg. loss: 272.516004\n",
      "Total training time: 59.75 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 675.15, NNZs: 8192, Bias: 48.872592, T: 2655000, Avg. loss: 164.248060\n",
      "Total training time: 59.78 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 885.97, NNZs: 8192, Bias: -49.873172, T: 2655000, Avg. loss: 208.005766\n",
      "Total training time: 59.74 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 748.25, NNZs: 8192, Bias: -5.375529, T: 2700000, Avg. loss: 143.048127\n",
      "Total training time: 59.97 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 651.33, NNZs: 8192, Bias: -19.311265, T: 2610000, Avg. loss: 293.725753\n",
      "Total training time: 60.10 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 699.93, NNZs: 8192, Bias: -57.619063, T: 2655000, Avg. loss: 238.306081\n",
      "Total training time: 60.23 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 758.47, NNZs: 8192, Bias: -52.257640, T: 2700000, Avg. loss: 216.266981\n",
      "Total training time: 60.39 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 648.24, NNZs: 8192, Bias: -8.634253, T: 2610000, Avg. loss: 252.549930\n",
      "Total training time: 60.52 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 799.78, NNZs: 8192, Bias: -32.608164, T: 2700000, Avg. loss: 183.558529\n",
      "Total training time: 60.63 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 742.73, NNZs: 8192, Bias: -53.291021, T: 2700000, Avg. loss: 213.710255\n",
      "Total training time: 60.69 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 879.26, NNZs: 8192, Bias: -49.888060, T: 2700000, Avg. loss: 200.744178\n",
      "Total training time: 60.79 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 668.84, NNZs: 8192, Bias: -61.036157, T: 2655000, Avg. loss: 260.484932\n",
      "Total training time: 60.82 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 670.09, NNZs: 8192, Bias: 48.883825, T: 2700000, Avg. loss: 161.773693\n",
      "Total training time: 60.85 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 742.32, NNZs: 8192, Bias: -5.371835, T: 2745000, Avg. loss: 140.898049\n",
      "Total training time: 61.01 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 646.27, NNZs: 8192, Bias: -19.318851, T: 2655000, Avg. loss: 289.310947\n",
      "Total training time: 61.20 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 694.06, NNZs: 8192, Bias: -57.637780, T: 2700000, Avg. loss: 235.683112\n",
      "Total training time: 61.30 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 752.50, NNZs: 8192, Bias: -52.257590, T: 2745000, Avg. loss: 212.385176\n",
      "Total training time: 61.45 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 643.91, NNZs: 8192, Bias: -8.622821, T: 2655000, Avg. loss: 249.428490\n",
      "Total training time: 61.59 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 793.37, NNZs: 8192, Bias: -32.626607, T: 2745000, Avg. loss: 180.259186\n",
      "Total training time: 61.67 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 737.16, NNZs: 8192, Bias: -53.305630, T: 2745000, Avg. loss: 208.101144\n",
      "Total training time: 61.73 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 872.50, NNZs: 8192, Bias: -49.899081, T: 2745000, Avg. loss: 199.820346\n",
      "Total training time: 61.84 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 664.59, NNZs: 8192, Bias: 48.898542, T: 2745000, Avg. loss: 159.187901\n",
      "Total training time: 61.89 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 664.08, NNZs: 8192, Bias: -61.036198, T: 2700000, Avg. loss: 259.152117\n",
      "Total training time: 61.89 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 736.53, NNZs: 8192, Bias: -5.375471, T: 2790000, Avg. loss: 138.942656\n",
      "Total training time: 62.05 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 641.41, NNZs: 8192, Bias: -19.318841, T: 2700000, Avg. loss: 284.970334\n",
      "Total training time: 62.27 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 688.58, NNZs: 8192, Bias: -57.667204, T: 2745000, Avg. loss: 232.176771\n",
      "Total training time: 62.35 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 746.30, NNZs: 8192, Bias: -52.253990, T: 2790000, Avg. loss: 208.675854\n",
      "Total training time: 62.49 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 639.77, NNZs: 8192, Bias: -8.604191, T: 2700000, Avg. loss: 241.085357\n",
      "Total training time: 62.66 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 786.93, NNZs: 8192, Bias: -32.655590, T: 2790000, Avg. loss: 174.683776\n",
      "Total training time: 62.70 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 731.76, NNZs: 8192, Bias: -53.316508, T: 2790000, Avg. loss: 206.399219\n",
      "Total training time: 62.76 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 865.35, NNZs: 8192, Bias: -49.927971, T: 2790000, Avg. loss: 193.721817\n",
      "Total training time: 62.87 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 659.95, NNZs: 8192, Bias: 48.920220, T: 2790000, Avg. loss: 155.634229\n",
      "Total training time: 62.92 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 659.53, NNZs: 8192, Bias: -61.054637, T: 2745000, Avg. loss: 252.229351\n",
      "Total training time: 62.95 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 731.16, NNZs: 8192, Bias: -5.375550, T: 2835000, Avg. loss: 136.748508\n",
      "Total training time: 63.08 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 637.06, NNZs: 8192, Bias: -19.329886, T: 2745000, Avg. loss: 273.354727\n",
      "Total training time: 63.34 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 683.16, NNZs: 8192, Bias: -57.699696, T: 2790000, Avg. loss: 225.729027\n",
      "Total training time: 63.41 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 740.49, NNZs: 8192, Bias: -52.246879, T: 2835000, Avg. loss: 205.280037\n",
      "Total training time: 63.53 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 634.99, NNZs: 8192, Bias: -8.607950, T: 2745000, Avg. loss: 236.880545\n",
      "Total training time: 63.72 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 780.82, NNZs: 8192, Bias: -32.676969, T: 2835000, Avg. loss: 173.174931\n",
      "Total training time: 63.75 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 726.89, NNZs: 8192, Bias: -53.341408, T: 2835000, Avg. loss: 202.189893\n",
      "Total training time: 63.79 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 655.43, NNZs: 8192, Bias: 48.930753, T: 2835000, Avg. loss: 153.776681\n",
      "Total training time: 63.95 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 859.15, NNZs: 8192, Bias: -49.938635, T: 2835000, Avg. loss: 191.466659\n",
      "Total training time: 63.91 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 655.41, NNZs: 8192, Bias: -61.065522, T: 2790000, Avg. loss: 251.991728\n",
      "Total training time: 63.98 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 725.58, NNZs: 8192, Bias: -5.368602, T: 2880000, Avg. loss: 136.061843\n",
      "Total training time: 64.09 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 632.87, NNZs: 8192, Bias: -19.337140, T: 2790000, Avg. loss: 269.915184\n",
      "Total training time: 64.39 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 678.19, NNZs: 8192, Bias: -57.724576, T: 2835000, Avg. loss: 223.259029\n",
      "Total training time: 64.44 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 735.17, NNZs: 8192, Bias: -52.246866, T: 2880000, Avg. loss: 199.367751\n",
      "Total training time: 64.54 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 775.57, NNZs: 8192, Bias: -32.701512, T: 2880000, Avg. loss: 170.331256\n",
      "Total training time: 64.77 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 630.86, NNZs: 8192, Bias: -8.611548, T: 2790000, Avg. loss: 235.880376\n",
      "Total training time: 64.78 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 721.88, NNZs: 8192, Bias: -53.358854, T: 2880000, Avg. loss: 198.791109\n",
      "Total training time: 64.81 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 650.66, NNZs: 8192, Bias: 48.930677, T: 2880000, Avg. loss: 149.704559\n",
      "Total training time: 64.96 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 852.65, NNZs: 8192, Bias: -49.963130, T: 2880000, Avg. loss: 193.043895\n",
      "Total training time: 64.94 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 651.05, NNZs: 8192, Bias: -61.086949, T: 2835000, Avg. loss: 243.539596\n",
      "Total training time: 65.02 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 720.28, NNZs: 8192, Bias: -5.372043, T: 2925000, Avg. loss: 130.437342\n",
      "Total training time: 65.10 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 673.48, NNZs: 8192, Bias: -57.749032, T: 2880000, Avg. loss: 221.339206\n",
      "Total training time: 65.48 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 628.60, NNZs: 8192, Bias: -19.351293, T: 2835000, Avg. loss: 269.231999\n",
      "Total training time: 65.45 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 730.08, NNZs: 8192, Bias: -52.239992, T: 2925000, Avg. loss: 194.925607\n",
      "Total training time: 65.57 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 769.73, NNZs: 8192, Bias: -32.729141, T: 2925000, Avg. loss: 169.938877\n",
      "Total training time: 65.78 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 626.71, NNZs: 8192, Bias: -8.608007, T: 2835000, Avg. loss: 227.813089\n",
      "Total training time: 65.83 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 716.47, NNZs: 8192, Bias: -53.379503, T: 2925000, Avg. loss: 194.424076\n",
      "Total training time: 65.84 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 645.87, NNZs: 8192, Bias: 48.937529, T: 2925000, Avg. loss: 148.656391\n",
      "Total training time: 65.99 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 846.66, NNZs: 8192, Bias: -49.980375, T: 2925000, Avg. loss: 185.502383\n",
      "Total training time: 65.98 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 646.98, NNZs: 8192, Bias: -61.104492, T: 2880000, Avg. loss: 242.973155\n",
      "Total training time: 66.09 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 714.96, NNZs: 8192, Bias: -5.375449, T: 2970000, Avg. loss: 128.961907\n",
      "Total training time: 66.11 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 669.49, NNZs: 8192, Bias: -57.776525, T: 2925000, Avg. loss: 214.027799\n",
      "Total training time: 66.52 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 624.24, NNZs: 8192, Bias: -19.347776, T: 2880000, Avg. loss: 262.210027\n",
      "Total training time: 66.50 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 725.27, NNZs: 8192, Bias: -52.240007, T: 2970000, Avg. loss: 195.922093\n",
      "Total training time: 66.58 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 763.57, NNZs: 8192, Bias: -32.752942, T: 2970000, Avg. loss: 167.143754\n",
      "Total training time: 66.79 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 622.93, NNZs: 8192, Bias: -8.597491, T: 2880000, Avg. loss: 223.438907\n",
      "Total training time: 66.87 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 711.81, NNZs: 8192, Bias: -53.393073, T: 2970000, Avg. loss: 190.955019\n",
      "Total training time: 66.87 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 641.34, NNZs: 8192, Bias: 48.937523, T: 2970000, Avg. loss: 142.639802\n",
      "Total training time: 67.01 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 841.05, NNZs: 8192, Bias: -50.000734, T: 2970000, Avg. loss: 181.024362\n",
      "Total training time: 67.00 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 709.36, NNZs: 8192, Bias: -5.375473, T: 3015000, Avg. loss: 126.119824\n",
      "Total training time: 67.10 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 642.86, NNZs: 8192, Bias: -61.114930, T: 2925000, Avg. loss: 237.531435\n",
      "Total training time: 67.13 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 664.10, NNZs: 8192, Bias: -57.800214, T: 2970000, Avg. loss: 211.330048\n",
      "Total training time: 67.53 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 620.19, NNZs: 8192, Bias: -19.340918, T: 2925000, Avg. loss: 257.346070\n",
      "Total training time: 67.53 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 720.43, NNZs: 8192, Bias: -52.243349, T: 3015000, Avg. loss: 190.116922\n",
      "Total training time: 67.58 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 758.49, NNZs: 8192, Bias: -32.779715, T: 3015000, Avg. loss: 161.820199\n",
      "Total training time: 67.81 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 707.62, NNZs: 8192, Bias: -53.413166, T: 3015000, Avg. loss: 189.909343\n",
      "Total training time: 67.88 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 618.70, NNZs: 8192, Bias: -8.600911, T: 2925000, Avg. loss: 222.253354\n",
      "Total training time: 67.92 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 636.77, NNZs: 8192, Bias: 48.954238, T: 3015000, Avg. loss: 144.999498\n",
      "Total training time: 68.03 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 835.03, NNZs: 8192, Bias: -50.020797, T: 3015000, Avg. loss: 179.904663\n",
      "Total training time: 68.01 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 705.01, NNZs: 8192, Bias: -5.375494, T: 3060000, Avg. loss: 123.681777\n",
      "Total training time: 68.10 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 639.46, NNZs: 8192, Bias: -61.128509, T: 2970000, Avg. loss: 232.350896\n",
      "Total training time: 68.15 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 659.65, NNZs: 8192, Bias: -57.813523, T: 3015000, Avg. loss: 211.163042\n",
      "Total training time: 68.54 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 616.09, NNZs: 8192, Bias: -19.354473, T: 2970000, Avg. loss: 253.681183\n",
      "Total training time: 68.56 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 715.32, NNZs: 8192, Bias: -52.243342, T: 3060000, Avg. loss: 189.503036\n",
      "Total training time: 68.59 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 753.08, NNZs: 8192, Bias: -32.802745, T: 3060000, Avg. loss: 158.710602\n",
      "Total training time: 68.81 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 703.09, NNZs: 8192, Bias: -53.432916, T: 3060000, Avg. loss: 185.777087\n",
      "Total training time: 68.87 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 614.34, NNZs: 8192, Bias: -8.594097, T: 2970000, Avg. loss: 219.137489\n",
      "Total training time: 68.95 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 632.54, NNZs: 8192, Bias: 48.964101, T: 3060000, Avg. loss: 140.739822\n",
      "Total training time: 69.04 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 828.98, NNZs: 8192, Bias: -50.030637, T: 3060000, Avg. loss: 177.351142\n",
      "Total training time: 69.03 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 699.93, NNZs: 8192, Bias: -5.369064, T: 3105000, Avg. loss: 122.259533\n",
      "Total training time: 69.08 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 635.84, NNZs: 8192, Bias: -61.141893, T: 3015000, Avg. loss: 231.019133\n",
      "Total training time: 69.18 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 654.71, NNZs: 8192, Bias: -57.843219, T: 3060000, Avg. loss: 207.932264\n",
      "Total training time: 69.58 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 710.43, NNZs: 8192, Bias: -52.249814, T: 3105000, Avg. loss: 183.675050\n",
      "Total training time: 69.59 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 612.56, NNZs: 8192, Bias: -19.344445, T: 3015000, Avg. loss: 248.746457\n",
      "Total training time: 69.58 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 748.34, NNZs: 8192, Bias: -32.828680, T: 3105000, Avg. loss: 156.623614\n",
      "Total training time: 69.82 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 698.42, NNZs: 8192, Bias: -53.452373, T: 3105000, Avg. loss: 182.458690\n",
      "Total training time: 69.87 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 610.87, NNZs: 8192, Bias: -8.597442, T: 3015000, Avg. loss: 214.062573\n",
      "Total training time: 69.99 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 628.56, NNZs: 8192, Bias: 48.983534, T: 3105000, Avg. loss: 137.089654\n",
      "Total training time: 70.06 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 822.81, NNZs: 8192, Bias: -50.040359, T: 3105000, Avg. loss: 169.852698\n",
      "Total training time: 70.04 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 695.31, NNZs: 8192, Bias: -5.375485, T: 3150000, Avg. loss: 118.880004\n",
      "Total training time: 70.06 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 632.28, NNZs: 8192, Bias: -61.155108, T: 3060000, Avg. loss: 226.811840\n",
      "Total training time: 70.23 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 649.83, NNZs: 8192, Bias: -57.865965, T: 3105000, Avg. loss: 202.683977\n",
      "Total training time: 70.62 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 705.32, NNZs: 8192, Bias: -52.237006, T: 3150000, Avg. loss: 181.977484\n",
      "Total training time: 70.63 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 608.82, NNZs: 8192, Bias: -19.354370, T: 3060000, Avg. loss: 245.144848\n",
      "Total training time: 70.64 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 743.00, NNZs: 8192, Bias: -32.847860, T: 3150000, Avg. loss: 154.825425\n",
      "Total training time: 70.87 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 693.74, NNZs: 8192, Bias: -53.461955, T: 3150000, Avg. loss: 178.700349\n",
      "Total training time: 70.91 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 624.18, NNZs: 8192, Bias: 48.989951, T: 3150000, Avg. loss: 137.693672\n",
      "Total training time: 71.10 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 607.26, NNZs: 8192, Bias: -8.597458, T: 3060000, Avg. loss: 214.048034\n",
      "Total training time: 71.08 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 817.00, NNZs: 8192, Bias: -50.056331, T: 3150000, Avg. loss: 169.201806\n",
      "Total training time: 71.08 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 690.54, NNZs: 8192, Bias: -5.372314, T: 3195000, Avg. loss: 119.029816\n",
      "Total training time: 71.10 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 628.57, NNZs: 8192, Bias: -61.164841, T: 3105000, Avg. loss: 219.647463\n",
      "Total training time: 71.28 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 645.52, NNZs: 8192, Bias: -57.888367, T: 3150000, Avg. loss: 200.324248\n",
      "Total training time: 71.67 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 700.63, NNZs: 8192, Bias: -52.224477, T: 3195000, Avg. loss: 179.619402\n",
      "Total training time: 71.66 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 605.22, NNZs: 8192, Bias: -19.357573, T: 3105000, Avg. loss: 241.944791\n",
      "Total training time: 71.69 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 737.83, NNZs: 8192, Bias: -32.876218, T: 3195000, Avg. loss: 153.754460\n",
      "Total training time: 71.90 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 689.54, NNZs: 8192, Bias: -53.483991, T: 3195000, Avg. loss: 175.097549\n",
      "Total training time: 71.93 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 685.91, NNZs: 8192, Bias: -5.378490, T: 3240000, Avg. loss: 116.901738\n",
      "Total training time: 72.10 seconds.\n",
      "Norm: 620.35, NNZs: 8192, Bias: 48.999449, T: 3195000, Avg. loss: 134.001610\n",
      "Total training time: 72.14 seconds.\n",
      "-- Epoch 73\n",
      "-- Epoch 72\n",
      "Norm: 811.18, NNZs: 8192, Bias: -50.068893, T: 3195000, Avg. loss: 172.358078\n",
      "Total training time: 72.11 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 603.73, NNZs: 8192, Bias: -8.584481, T: 3105000, Avg. loss: 208.441399\n",
      "Total training time: 72.14 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 625.03, NNZs: 8192, Bias: -61.180798, T: 3150000, Avg. loss: 218.755845\n",
      "Total training time: 72.32 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 641.09, NNZs: 8192, Bias: -57.913535, T: 3195000, Avg. loss: 197.708891\n",
      "Total training time: 72.70 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 696.31, NNZs: 8192, Bias: -52.224470, T: 3240000, Avg. loss: 176.153286\n",
      "Total training time: 72.69 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 602.05, NNZs: 8192, Bias: -19.351136, T: 3150000, Avg. loss: 236.871881\n",
      "Total training time: 72.74 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 732.99, NNZs: 8192, Bias: -32.901078, T: 3240000, Avg. loss: 147.987713\n",
      "Total training time: 72.94 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 685.30, NNZs: 8192, Bias: -53.490220, T: 3240000, Avg. loss: 172.556389\n",
      "Total training time: 72.96 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 681.70, NNZs: 8192, Bias: -5.378489, T: 3285000, Avg. loss: 113.550919\n",
      "Total training time: 73.12 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 616.23, NNZs: 8192, Bias: 49.011888, T: 3240000, Avg. loss: 134.012557\n",
      "Total training time: 73.18 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 805.28, NNZs: 8192, Bias: -50.081328, T: 3240000, Avg. loss: 164.092845\n",
      "Total training time: 73.14 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 599.95, NNZs: 8192, Bias: -8.584459, T: 3150000, Avg. loss: 204.143011\n",
      "Total training time: 73.19 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 621.86, NNZs: 8192, Bias: -61.183966, T: 3195000, Avg. loss: 215.585625\n",
      "Total training time: 73.38 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 691.71, NNZs: 8192, Bias: -52.221417, T: 3285000, Avg. loss: 173.300855\n",
      "Total training time: 73.70 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 637.38, NNZs: 8192, Bias: -57.929061, T: 3240000, Avg. loss: 192.639927\n",
      "Total training time: 73.72 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 598.81, NNZs: 8192, Bias: -19.360602, T: 3195000, Avg. loss: 234.719579\n",
      "Total training time: 73.81 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 728.46, NNZs: 8192, Bias: -32.928636, T: 3285000, Avg. loss: 147.151081\n",
      "Total training time: 73.96 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 681.09, NNZs: 8192, Bias: -53.514709, T: 3285000, Avg. loss: 171.320917\n",
      "Total training time: 73.98 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 677.39, NNZs: 8192, Bias: -5.375459, T: 3330000, Avg. loss: 112.983237\n",
      "Total training time: 74.13 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 612.06, NNZs: 8192, Bias: 49.021055, T: 3285000, Avg. loss: 131.245276\n",
      "Total training time: 74.21 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 800.03, NNZs: 8192, Bias: -50.102780, T: 3285000, Avg. loss: 162.720383\n",
      "Total training time: 74.17 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 596.47, NNZs: 8192, Bias: -8.581295, T: 3195000, Avg. loss: 203.315575\n",
      "Total training time: 74.25 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 618.19, NNZs: 8192, Bias: -61.190244, T: 3240000, Avg. loss: 208.926965\n",
      "Total training time: 74.44 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 687.71, NNZs: 8192, Bias: -52.224463, T: 3330000, Avg. loss: 171.486643\n",
      "Total training time: 74.72 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 633.40, NNZs: 8192, Bias: -57.959680, T: 3285000, Avg. loss: 189.396976\n",
      "Total training time: 74.76 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 595.35, NNZs: 8192, Bias: -19.363734, T: 3240000, Avg. loss: 232.444645\n",
      "Total training time: 74.87 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 723.86, NNZs: 8192, Bias: -32.952831, T: 3330000, Avg. loss: 147.227982\n",
      "Total training time: 74.98 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 677.43, NNZs: 8192, Bias: -53.523764, T: 3330000, Avg. loss: 169.564793\n",
      "Total training time: 75.01 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 673.15, NNZs: 8192, Bias: -5.378446, T: 3375000, Avg. loss: 109.495853\n",
      "Total training time: 75.16 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 607.81, NNZs: 8192, Bias: 49.027123, T: 3330000, Avg. loss: 130.269686\n",
      "Total training time: 75.23 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 794.94, NNZs: 8192, Bias: -50.111837, T: 3330000, Avg. loss: 156.793506\n",
      "Total training time: 75.19 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 593.05, NNZs: 8192, Bias: -8.581360, T: 3240000, Avg. loss: 200.008392\n",
      "Total training time: 75.31 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 614.55, NNZs: 8192, Bias: -61.205564, T: 3285000, Avg. loss: 207.510031\n",
      "Total training time: 75.49 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 683.87, NNZs: 8192, Bias: -52.221471, T: 3375000, Avg. loss: 169.522359\n",
      "Total training time: 75.73 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 629.48, NNZs: 8192, Bias: -57.974764, T: 3330000, Avg. loss: 186.438144\n",
      "Total training time: 75.79 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 592.27, NNZs: 8192, Bias: -19.366781, T: 3285000, Avg. loss: 226.113447\n",
      "Total training time: 75.92 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 719.22, NNZs: 8192, Bias: -32.970705, T: 3375000, Avg. loss: 143.807654\n",
      "Total training time: 76.00 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 673.46, NNZs: 8192, Bias: -53.541664, T: 3375000, Avg. loss: 167.798186\n",
      "Total training time: 76.02 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 668.92, NNZs: 8192, Bias: -5.381375, T: 3420000, Avg. loss: 108.080912\n",
      "Total training time: 76.20 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 603.86, NNZs: 8192, Bias: 49.030132, T: 3375000, Avg. loss: 126.268434\n",
      "Total training time: 76.28 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 789.60, NNZs: 8192, Bias: -50.126691, T: 3375000, Avg. loss: 156.948948\n",
      "Total training time: 76.24 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 589.78, NNZs: 8192, Bias: -8.575278, T: 3285000, Avg. loss: 194.469424\n",
      "Total training time: 76.37 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 611.49, NNZs: 8192, Bias: -61.202600, T: 3330000, Avg. loss: 205.816288\n",
      "Total training time: 76.54 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 680.39, NNZs: 8192, Bias: -52.215577, T: 3420000, Avg. loss: 165.747966\n",
      "Total training time: 76.76 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 625.87, NNZs: 8192, Bias: -58.001597, T: 3375000, Avg. loss: 184.978151\n",
      "Total training time: 76.82 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 588.58, NNZs: 8192, Bias: -19.357766, T: 3330000, Avg. loss: 223.550909\n",
      "Total training time: 76.96 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 714.83, NNZs: 8192, Bias: -32.988360, T: 3420000, Avg. loss: 142.959967\n",
      "Total training time: 77.01 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 670.28, NNZs: 8192, Bias: -53.562271, T: 3420000, Avg. loss: 163.970237\n",
      "Total training time: 77.05 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 664.58, NNZs: 8192, Bias: -5.372677, T: 3465000, Avg. loss: 108.474538\n",
      "Total training time: 77.19 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 784.30, NNZs: 8192, Bias: -50.135496, T: 3420000, Avg. loss: 153.564911\n",
      "Total training time: 77.25 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 600.31, NNZs: 8192, Bias: 49.044858, T: 3420000, Avg. loss: 124.571398\n",
      "Total training time: 77.31 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 586.63, NNZs: 8192, Bias: -8.569265, T: 3330000, Avg. loss: 192.967830\n",
      "Total training time: 77.44 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 608.34, NNZs: 8192, Bias: -61.220515, T: 3375000, Avg. loss: 200.271523\n",
      "Total training time: 77.58 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 676.18, NNZs: 8192, Bias: -52.218507, T: 3465000, Avg. loss: 166.270473\n",
      "Total training time: 77.82 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 621.87, NNZs: 8192, Bias: -58.022085, T: 3420000, Avg. loss: 183.486424\n",
      "Total training time: 77.90 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 585.44, NNZs: 8192, Bias: -19.360789, T: 3375000, Avg. loss: 225.323215\n",
      "Total training time: 78.04 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 710.31, NNZs: 8192, Bias: -33.002935, T: 3465000, Avg. loss: 134.812610\n",
      "Total training time: 78.07 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 666.64, NNZs: 8192, Bias: -53.579673, T: 3465000, Avg. loss: 161.940863\n",
      "Total training time: 78.12 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 660.61, NNZs: 8192, Bias: -5.375545, T: 3510000, Avg. loss: 104.651687\n",
      "Total training time: 78.24 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 779.28, NNZs: 8192, Bias: -50.152963, T: 3465000, Avg. loss: 152.018182\n",
      "Total training time: 78.31 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 596.50, NNZs: 8192, Bias: 49.047731, T: 3465000, Avg. loss: 121.073678\n",
      "Total training time: 78.37 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 583.75, NNZs: 8192, Bias: -8.569284, T: 3375000, Avg. loss: 190.790603\n",
      "Total training time: 78.58 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 604.77, NNZs: 8192, Bias: -61.223478, T: 3420000, Avg. loss: 201.825198\n",
      "Total training time: 78.71 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 671.67, NNZs: 8192, Bias: -52.218514, T: 3510000, Avg. loss: 160.058471\n",
      "Total training time: 78.91 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 617.67, NNZs: 8192, Bias: -58.045316, T: 3465000, Avg. loss: 180.424853\n",
      "Total training time: 79.03 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 705.90, NNZs: 8192, Bias: -33.023014, T: 3510000, Avg. loss: 134.061575\n",
      "Total training time: 79.17 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 582.50, NNZs: 8192, Bias: -19.369616, T: 3420000, Avg. loss: 220.331931\n",
      "Total training time: 79.21 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 662.84, NNZs: 8192, Bias: -53.588240, T: 3510000, Avg. loss: 157.996905\n",
      "Total training time: 79.22 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 656.42, NNZs: 8192, Bias: -5.381203, T: 3555000, Avg. loss: 106.213193\n",
      "Total training time: 79.34 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 774.53, NNZs: 8192, Bias: -50.167274, T: 3510000, Avg. loss: 148.143166\n",
      "Total training time: 79.41 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 593.24, NNZs: 8192, Bias: 49.053468, T: 3510000, Avg. loss: 121.057744\n",
      "Total training time: 79.48 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 580.31, NNZs: 8192, Bias: -8.566321, T: 3420000, Avg. loss: 187.625318\n",
      "Total training time: 79.72 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 601.39, NNZs: 8192, Bias: -61.240911, T: 3465000, Avg. loss: 195.374827\n",
      "Total training time: 79.81 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 668.18, NNZs: 8192, Bias: -52.215679, T: 3555000, Avg. loss: 162.377517\n",
      "Total training time: 80.00 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 614.25, NNZs: 8192, Bias: -58.065380, T: 3510000, Avg. loss: 176.584394\n",
      "Total training time: 80.16 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 701.69, NNZs: 8192, Bias: -33.045660, T: 3555000, Avg. loss: 135.813896\n",
      "Total training time: 80.28 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 659.39, NNZs: 8192, Bias: -53.605191, T: 3555000, Avg. loss: 154.103550\n",
      "Total training time: 80.32 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 580.13, NNZs: 8192, Bias: -19.366688, T: 3465000, Avg. loss: 217.452238\n",
      "Total training time: 80.36 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 652.72, NNZs: 8192, Bias: -5.378419, T: 3600000, Avg. loss: 101.400607\n",
      "Total training time: 80.43 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 769.73, NNZs: 8192, Bias: -50.172899, T: 3555000, Avg. loss: 150.251195\n",
      "Total training time: 80.51 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 589.88, NNZs: 8192, Bias: 49.059110, T: 3555000, Avg. loss: 118.071810\n",
      "Total training time: 80.59 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 577.24, NNZs: 8192, Bias: -8.572097, T: 3465000, Avg. loss: 184.351321\n",
      "Total training time: 80.84 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 598.03, NNZs: 8192, Bias: -61.243788, T: 3510000, Avg. loss: 194.082516\n",
      "Total training time: 80.94 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 664.17, NNZs: 8192, Bias: -52.210108, T: 3600000, Avg. loss: 155.714197\n",
      "Total training time: 81.12 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 610.75, NNZs: 8192, Bias: -58.088016, T: 3555000, Avg. loss: 176.784064\n",
      "Total training time: 81.29 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 697.56, NNZs: 8192, Bias: -33.062422, T: 3600000, Avg. loss: 135.368179\n",
      "Total training time: 81.39 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 655.62, NNZs: 8192, Bias: -53.616355, T: 3600000, Avg. loss: 153.003581\n",
      "Total training time: 81.42 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 577.21, NNZs: 8192, Bias: -19.375283, T: 3510000, Avg. loss: 211.212061\n",
      "Total training time: 81.51 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 648.72, NNZs: 8192, Bias: -5.383982, T: 3645000, Avg. loss: 100.102494\n",
      "Total training time: 81.51 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 765.09, NNZs: 8192, Bias: -50.186860, T: 3600000, Avg. loss: 144.833911\n",
      "Total training time: 81.62 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 586.39, NNZs: 8192, Bias: 49.070285, T: 3600000, Avg. loss: 115.787612\n",
      "Total training time: 81.70 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 574.16, NNZs: 8192, Bias: -8.554860, T: 3510000, Avg. loss: 182.645479\n",
      "Total training time: 81.99 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 594.93, NNZs: 8192, Bias: -61.252248, T: 3555000, Avg. loss: 189.775320\n",
      "Total training time: 82.04 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 660.14, NNZs: 8192, Bias: -52.210128, T: 3645000, Avg. loss: 154.404748\n",
      "Total training time: 82.21 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 607.50, NNZs: 8192, Bias: -58.101994, T: 3600000, Avg. loss: 175.224445\n",
      "Total training time: 82.39 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 693.55, NNZs: 8192, Bias: -33.078971, T: 3645000, Avg. loss: 130.976501\n",
      "Total training time: 82.48 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 652.46, NNZs: 8192, Bias: -53.627383, T: 3645000, Avg. loss: 150.569390\n",
      "Total training time: 82.50 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 645.12, NNZs: 8192, Bias: -5.381225, T: 3690000, Avg. loss: 101.635193\n",
      "Total training time: 82.58 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 573.96, NNZs: 8192, Bias: -19.369656, T: 3555000, Avg. loss: 212.095960\n",
      "Total training time: 82.63 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 760.27, NNZs: 8192, Bias: -50.197876, T: 3645000, Avg. loss: 146.948324\n",
      "Total training time: 82.70 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 582.92, NNZs: 8192, Bias: 49.075771, T: 3645000, Avg. loss: 116.808031\n",
      "Total training time: 82.78 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 571.34, NNZs: 8192, Bias: -8.566140, T: 3555000, Avg. loss: 180.408402\n",
      "Total training time: 83.11 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 592.61, NNZs: 8192, Bias: -61.263403, T: 3600000, Avg. loss: 190.407519\n",
      "Total training time: 83.15 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 656.19, NNZs: 8192, Bias: -52.223747, T: 3690000, Avg. loss: 149.987180\n",
      "Total training time: 83.28 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 603.80, NNZs: 8192, Bias: -58.121324, T: 3645000, Avg. loss: 168.862684\n",
      "Total training time: 83.46 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 689.37, NNZs: 8192, Bias: -33.095336, T: 3690000, Avg. loss: 130.242370\n",
      "Total training time: 83.54 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 649.10, NNZs: 8192, Bias: -53.635562, T: 3690000, Avg. loss: 148.662525\n",
      "Total training time: 83.54 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 641.50, NNZs: 8192, Bias: -5.383921, T: 3735000, Avg. loss: 97.389713\n",
      "Total training time: 83.60 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 570.95, NNZs: 8192, Bias: -19.375293, T: 3600000, Avg. loss: 206.775586\n",
      "Total training time: 83.71 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 755.80, NNZs: 8192, Bias: -50.203340, T: 3690000, Avg. loss: 143.676905\n",
      "Total training time: 83.74 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 579.77, NNZs: 8192, Bias: 49.081190, T: 3690000, Avg. loss: 114.126766\n",
      "Total training time: 83.84 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 568.38, NNZs: 8192, Bias: -8.557741, T: 3600000, Avg. loss: 176.009256\n",
      "Total training time: 84.17 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 589.52, NNZs: 8192, Bias: -61.266135, T: 3645000, Avg. loss: 184.813145\n",
      "Total training time: 84.21 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 652.40, NNZs: 8192, Bias: -52.218363, T: 3735000, Avg. loss: 153.221581\n",
      "Total training time: 84.30 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 600.67, NNZs: 8192, Bias: -58.140410, T: 3690000, Avg. loss: 168.332771\n",
      "Total training time: 84.49 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 685.40, NNZs: 8192, Bias: -33.108814, T: 3735000, Avg. loss: 125.316614\n",
      "Total training time: 84.55 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 645.68, NNZs: 8192, Bias: -53.649032, T: 3735000, Avg. loss: 147.744796\n",
      "Total training time: 84.56 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 638.07, NNZs: 8192, Bias: -5.386571, T: 3780000, Avg. loss: 97.059354\n",
      "Total training time: 84.60 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 568.11, NNZs: 8192, Bias: -19.378048, T: 3645000, Avg. loss: 204.382019\n",
      "Total training time: 84.72 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 751.60, NNZs: 8192, Bias: -50.219536, T: 3735000, Avg. loss: 141.166738\n",
      "Total training time: 84.75 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 576.43, NNZs: 8192, Bias: 49.081198, T: 3735000, Avg. loss: 111.848044\n",
      "Total training time: 84.84 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 565.47, NNZs: 8192, Bias: -8.557777, T: 3645000, Avg. loss: 175.576057\n",
      "Total training time: 85.22 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 586.75, NNZs: 8192, Bias: -61.277054, T: 3690000, Avg. loss: 184.431286\n",
      "Total training time: 85.25 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 648.99, NNZs: 8192, Bias: -52.221025, T: 3780000, Avg. loss: 149.065325\n",
      "Total training time: 85.33 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 597.11, NNZs: 8192, Bias: -58.151196, T: 3735000, Avg. loss: 163.636099\n",
      "Total training time: 85.54 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 681.71, NNZs: 8192, Bias: -33.124818, T: 3780000, Avg. loss: 121.782645\n",
      "Total training time: 85.60 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 634.73, NNZs: 8192, Bias: -5.381298, T: 3825000, Avg. loss: 95.861208\n",
      "Total training time: 85.62 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 642.28, NNZs: 8192, Bias: -53.667637, T: 3780000, Avg. loss: 145.138400\n",
      "Total training time: 85.61 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 565.18, NNZs: 8192, Bias: -19.380740, T: 3690000, Avg. loss: 200.935365\n",
      "Total training time: 85.81 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 747.46, NNZs: 8192, Bias: -50.235514, T: 3780000, Avg. loss: 139.296164\n",
      "Total training time: 85.81 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 573.43, NNZs: 8192, Bias: 49.086516, T: 3780000, Avg. loss: 110.065333\n",
      "Total training time: 85.89 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 562.82, NNZs: 8192, Bias: -8.557774, T: 3690000, Avg. loss: 172.821292\n",
      "Total training time: 86.37 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 583.87, NNZs: 8192, Bias: -61.290496, T: 3735000, Avg. loss: 181.907178\n",
      "Total training time: 86.39 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 645.89, NNZs: 8192, Bias: -52.223642, T: 3825000, Avg. loss: 144.496506\n",
      "Total training time: 86.43 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 593.87, NNZs: 8192, Bias: -58.172505, T: 3780000, Avg. loss: 163.282927\n",
      "Total training time: 86.66 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 677.98, NNZs: 8192, Bias: -33.143233, T: 3825000, Avg. loss: 121.505359\n",
      "Total training time: 86.70 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 631.04, NNZs: 8192, Bias: -5.381308, T: 3870000, Avg. loss: 94.993652\n",
      "Total training time: 86.73 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 639.31, NNZs: 8192, Bias: -53.678150, T: 3825000, Avg. loss: 143.103778\n",
      "Total training time: 86.72 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 743.41, NNZs: 8192, Bias: -50.248633, T: 3825000, Avg. loss: 134.173681\n",
      "Total training time: 86.95 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 562.00, NNZs: 8192, Bias: -19.378027, T: 3735000, Avg. loss: 198.940656\n",
      "Total training time: 86.97 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 570.23, NNZs: 8192, Bias: 49.094430, T: 3825000, Avg. loss: 111.394677\n",
      "Total training time: 87.03 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 580.67, NNZs: 8192, Bias: -61.295813, T: 3780000, Avg. loss: 180.799304\n",
      "Total training time: 87.52 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 560.04, NNZs: 8192, Bias: -8.552385, T: 3735000, Avg. loss: 171.237759\n",
      "Total training time: 87.52 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 642.08, NNZs: 8192, Bias: -52.221039, T: 3870000, Avg. loss: 144.739227\n",
      "Total training time: 87.56 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 590.47, NNZs: 8192, Bias: -58.188286, T: 3825000, Avg. loss: 161.618280\n",
      "Total training time: 87.81 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 674.14, NNZs: 8192, Bias: -33.156237, T: 3870000, Avg. loss: 121.345227\n",
      "Total training time: 87.82 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 627.62, NNZs: 8192, Bias: -5.381302, T: 3915000, Avg. loss: 92.118686\n",
      "Total training time: 87.85 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 635.90, NNZs: 8192, Bias: -53.691132, T: 3870000, Avg. loss: 141.316992\n",
      "Total training time: 87.87 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 738.94, NNZs: 8192, Bias: -50.251204, T: 3870000, Avg. loss: 134.309931\n",
      "Total training time: 88.07 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 567.05, NNZs: 8192, Bias: 49.104849, T: 3870000, Avg. loss: 106.965309\n",
      "Total training time: 88.15 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 558.91, NNZs: 8192, Bias: -19.372714, T: 3780000, Avg. loss: 195.667208\n",
      "Total training time: 88.15 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 577.93, NNZs: 8192, Bias: -61.306333, T: 3825000, Avg. loss: 176.335635\n",
      "Total training time: 88.70 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 557.34, NNZs: 8192, Bias: -8.555017, T: 3780000, Avg. loss: 169.874179\n",
      "Total training time: 88.70 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 638.26, NNZs: 8192, Bias: -52.223595, T: 3915000, Avg. loss: 142.696812\n",
      "Total training time: 88.72 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 670.39, NNZs: 8192, Bias: -33.171665, T: 3915000, Avg. loss: 119.099306\n",
      "Total training time: 88.97 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 587.30, NNZs: 8192, Bias: -58.201276, T: 3870000, Avg. loss: 160.288914\n",
      "Total training time: 88.98 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 624.02, NNZs: 8192, Bias: -5.378766, T: 3960000, Avg. loss: 90.248411\n",
      "Total training time: 88.99 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 632.69, NNZs: 8192, Bias: -53.703954, T: 3915000, Avg. loss: 139.338852\n",
      "Total training time: 89.04 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 734.50, NNZs: 8192, Bias: -50.256342, T: 3915000, Avg. loss: 135.752261\n",
      "Total training time: 89.21 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 563.82, NNZs: 8192, Bias: 49.102320, T: 3915000, Avg. loss: 106.146414\n",
      "Total training time: 89.27 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 556.36, NNZs: 8192, Bias: -19.377987, T: 3825000, Avg. loss: 193.656598\n",
      "Total training time: 89.32 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 575.48, NNZs: 8192, Bias: -61.306360, T: 3870000, Avg. loss: 172.920177\n",
      "Total training time: 89.84 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 635.05, NNZs: 8192, Bias: -52.223564, T: 3960000, Avg. loss: 140.267422\n",
      "Total training time: 89.85 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 554.70, NNZs: 8192, Bias: -8.549727, T: 3825000, Avg. loss: 166.218596\n",
      "Total training time: 89.86 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 666.61, NNZs: 8192, Bias: -33.191979, T: 3960000, Avg. loss: 116.201402\n",
      "Total training time: 90.08 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 584.12, NNZs: 8192, Bias: -58.214142, T: 3915000, Avg. loss: 155.499310\n",
      "Total training time: 90.10 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 620.80, NNZs: 8192, Bias: -5.376259, T: 4005000, Avg. loss: 89.133630\n",
      "Total training time: 90.08 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 629.75, NNZs: 8192, Bias: -53.719163, T: 3960000, Avg. loss: 137.445722\n",
      "Total training time: 90.15 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 730.28, NNZs: 8192, Bias: -50.266514, T: 3960000, Avg. loss: 132.109048\n",
      "Total training time: 90.31 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 560.71, NNZs: 8192, Bias: 49.112502, T: 3960000, Avg. loss: 103.909286\n",
      "Total training time: 90.39 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 553.58, NNZs: 8192, Bias: -19.372802, T: 3870000, Avg. loss: 191.011445\n",
      "Total training time: 90.47 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 631.59, NNZs: 8192, Bias: -52.221043, T: 4005000, Avg. loss: 139.112446\n",
      "Total training time: 90.91 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 572.55, NNZs: 8192, Bias: -61.314105, T: 3915000, Avg. loss: 171.549733\n",
      "Total training time: 90.93 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 552.00, NNZs: 8192, Bias: -8.552307, T: 3870000, Avg. loss: 162.708261\n",
      "Total training time: 90.98 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 617.75, NNZs: 8192, Bias: -5.381225, T: 4050000, Avg. loss: 89.990670\n",
      "Total training time: 91.12 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 663.39, NNZs: 8192, Bias: -33.204519, T: 4005000, Avg. loss: 117.720084\n",
      "Total training time: 91.15 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 581.35, NNZs: 8192, Bias: -58.234467, T: 3960000, Avg. loss: 153.215265\n",
      "Total training time: 91.17 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 626.82, NNZs: 8192, Bias: -53.729163, T: 4005000, Avg. loss: 138.402223\n",
      "Total training time: 91.20 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 726.19, NNZs: 8192, Bias: -50.279066, T: 4005000, Avg. loss: 130.934439\n",
      "Total training time: 91.35 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 557.40, NNZs: 8192, Bias: 49.120065, T: 4005000, Avg. loss: 104.598628\n",
      "Total training time: 91.45 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 550.86, NNZs: 8192, Bias: -19.380521, T: 3915000, Avg. loss: 188.904492\n",
      "Total training time: 91.54 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 628.25, NNZs: 8192, Bias: -52.218567, T: 4050000, Avg. loss: 134.699832\n",
      "Total training time: 91.96 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 569.62, NNZs: 8192, Bias: -61.314121, T: 3960000, Avg. loss: 168.744519\n",
      "Total training time: 92.00 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 549.44, NNZs: 8192, Bias: -8.552270, T: 3915000, Avg. loss: 160.895519\n",
      "Total training time: 92.08 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 614.86, NNZs: 8192, Bias: -5.381205, T: 4095000, Avg. loss: 87.261087\n",
      "Total training time: 92.16 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 659.77, NNZs: 8192, Bias: -33.224380, T: 4050000, Avg. loss: 113.471502\n",
      "Total training time: 92.20 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 578.41, NNZs: 8192, Bias: -58.247045, T: 4005000, Avg. loss: 152.827835\n",
      "Total training time: 92.23 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 623.77, NNZs: 8192, Bias: -53.744057, T: 4050000, Avg. loss: 131.026276\n",
      "Total training time: 92.25 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 722.29, NNZs: 8192, Bias: -50.289000, T: 4050000, Avg. loss: 127.062420\n",
      "Total training time: 92.38 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 554.70, NNZs: 8192, Bias: 49.122554, T: 4050000, Avg. loss: 101.807550\n",
      "Total training time: 92.50 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 548.75, NNZs: 8192, Bias: -19.390678, T: 3960000, Avg. loss: 185.676282\n",
      "Total training time: 92.60 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 625.11, NNZs: 8192, Bias: -52.221001, T: 4095000, Avg. loss: 135.983621\n",
      "Total training time: 93.00 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 567.59, NNZs: 8192, Bias: -61.324193, T: 4005000, Avg. loss: 167.736179\n",
      "Total training time: 93.05 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 547.19, NNZs: 8192, Bias: -8.547207, T: 3960000, Avg. loss: 158.748328\n",
      "Total training time: 93.15 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 611.66, NNZs: 8192, Bias: -5.378760, T: 4140000, Avg. loss: 87.309088\n",
      "Total training time: 93.18 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 656.45, NNZs: 8192, Bias: -33.241541, T: 4095000, Avg. loss: 115.750518\n",
      "Total training time: 93.23 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 575.18, NNZs: 8192, Bias: -58.274320, T: 4050000, Avg. loss: 153.378405\n",
      "Total training time: 93.29 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 620.81, NNZs: 8192, Bias: -53.751407, T: 4095000, Avg. loss: 133.086642\n",
      "Total training time: 93.29 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 718.23, NNZs: 8192, Bias: -50.303734, T: 4095000, Avg. loss: 126.002720\n",
      "Total training time: 93.43 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 551.73, NNZs: 8192, Bias: 49.125012, T: 4095000, Avg. loss: 101.735710\n",
      "Total training time: 93.54 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 546.69, NNZs: 8192, Bias: -19.393167, T: 4005000, Avg. loss: 185.199393\n",
      "Total training time: 93.67 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 622.27, NNZs: 8192, Bias: -52.216148, T: 4140000, Avg. loss: 133.010547\n",
      "Total training time: 94.04 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 565.15, NNZs: 8192, Bias: -61.336612, T: 4050000, Avg. loss: 163.135094\n",
      "Total training time: 94.11 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 608.72, NNZs: 8192, Bias: -5.371583, T: 4185000, Avg. loss: 86.280505\n",
      "Total training time: 94.21 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 544.80, NNZs: 8192, Bias: -8.549689, T: 4005000, Avg. loss: 156.108431\n",
      "Total training time: 94.23 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 652.92, NNZs: 8192, Bias: -33.258515, T: 4140000, Avg. loss: 111.081382\n",
      "Total training time: 94.27 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 572.45, NNZs: 8192, Bias: -58.289022, T: 4095000, Avg. loss: 149.261646\n",
      "Total training time: 94.35 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 617.71, NNZs: 8192, Bias: -53.761113, T: 4140000, Avg. loss: 130.796814\n",
      "Total training time: 94.33 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 714.34, NNZs: 8192, Bias: -50.318306, T: 4140000, Avg. loss: 124.279180\n",
      "Total training time: 94.52 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 549.01, NNZs: 8192, Bias: 49.139602, T: 4140000, Avg. loss: 99.704033\n",
      "Total training time: 94.63 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 544.05, NNZs: 8192, Bias: -19.390671, T: 4050000, Avg. loss: 184.752266\n",
      "Total training time: 94.79 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 619.29, NNZs: 8192, Bias: -52.213736, T: 4185000, Avg. loss: 134.477543\n",
      "Total training time: 95.17 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 562.77, NNZs: 8192, Bias: -61.343985, T: 4095000, Avg. loss: 163.815639\n",
      "Total training time: 95.27 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 605.38, NNZs: 8192, Bias: -5.381053, T: 4230000, Avg. loss: 85.841389\n",
      "Total training time: 95.34 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 649.73, NNZs: 8192, Bias: -33.263269, T: 4185000, Avg. loss: 111.234922\n",
      "Total training time: 95.40 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 541.97, NNZs: 8192, Bias: -8.544713, T: 4050000, Avg. loss: 155.819401\n",
      "Total training time: 95.39 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 569.73, NNZs: 8192, Bias: -58.306007, T: 4140000, Avg. loss: 149.680738\n",
      "Total training time: 95.51 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 614.76, NNZs: 8192, Bias: -53.770731, T: 4185000, Avg. loss: 127.956049\n",
      "Total training time: 95.47 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 710.49, NNZs: 8192, Bias: -50.318287, T: 4185000, Avg. loss: 122.981362\n",
      "Total training time: 95.67 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 545.90, NNZs: 8192, Bias: 49.144411, T: 4185000, Avg. loss: 98.536903\n",
      "Total training time: 95.75 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 541.60, NNZs: 8192, Bias: -19.395580, T: 4095000, Avg. loss: 182.467524\n",
      "Total training time: 95.94 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 616.03, NNZs: 8192, Bias: -52.213727, T: 4230000, Avg. loss: 129.928735\n",
      "Total training time: 96.30 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 560.37, NNZs: 8192, Bias: -61.351272, T: 4140000, Avg. loss: 162.080107\n",
      "Total training time: 96.41 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 602.43, NNZs: 8192, Bias: -5.381012, T: 4275000, Avg. loss: 83.567991\n",
      "Total training time: 96.46 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 646.18, NNZs: 8192, Bias: -33.279902, T: 4230000, Avg. loss: 110.165377\n",
      "Total training time: 96.49 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 539.54, NNZs: 8192, Bias: -8.556959, T: 4095000, Avg. loss: 154.735801\n",
      "Total training time: 96.51 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 566.92, NNZs: 8192, Bias: -58.320389, T: 4185000, Avg. loss: 147.885971\n",
      "Total training time: 96.61 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 611.86, NNZs: 8192, Bias: -53.782648, T: 4230000, Avg. loss: 127.042873\n",
      "Total training time: 96.57 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 706.67, NNZs: 8192, Bias: -50.327788, T: 4230000, Avg. loss: 123.303587\n",
      "Total training time: 96.77 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 543.54, NNZs: 8192, Bias: 49.151524, T: 4230000, Avg. loss: 99.469604\n",
      "Total training time: 96.85 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 539.27, NNZs: 8192, Bias: -19.400391, T: 4140000, Avg. loss: 177.036587\n",
      "Total training time: 97.08 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 613.28, NNZs: 8192, Bias: -52.211377, T: 4275000, Avg. loss: 132.257569\n",
      "Total training time: 97.41 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 557.77, NNZs: 8192, Bias: -61.358502, T: 4185000, Avg. loss: 160.182904\n",
      "Total training time: 97.52 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 599.48, NNZs: 8192, Bias: -5.376343, T: 4320000, Avg. loss: 81.151162\n",
      "Total training time: 97.55 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 642.98, NNZs: 8192, Bias: -33.294016, T: 4275000, Avg. loss: 107.791135\n",
      "Total training time: 97.59 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 537.18, NNZs: 8192, Bias: -8.552084, T: 4140000, Avg. loss: 150.722466\n",
      "Total training time: 97.63 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 609.31, NNZs: 8192, Bias: -53.794401, T: 4275000, Avg. loss: 124.926627\n",
      "Total training time: 97.67 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 564.37, NNZs: 8192, Bias: -58.334638, T: 4230000, Avg. loss: 143.780449\n",
      "Total training time: 97.73 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 702.83, NNZs: 8192, Bias: -50.337215, T: 4275000, Avg. loss: 120.132125\n",
      "Total training time: 97.86 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 540.75, NNZs: 8192, Bias: 49.153889, T: 4275000, Avg. loss: 95.291698\n",
      "Total training time: 97.93 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 536.86, NNZs: 8192, Bias: -19.405166, T: 4185000, Avg. loss: 177.328126\n",
      "Total training time: 98.23 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 610.18, NNZs: 8192, Bias: -52.211355, T: 4320000, Avg. loss: 129.939298\n",
      "Total training time: 98.49 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 596.68, NNZs: 8192, Bias: -5.385542, T: 4365000, Avg. loss: 81.127525\n",
      "Total training time: 98.62 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 555.62, NNZs: 8192, Bias: -61.363253, T: 4230000, Avg. loss: 158.676374\n",
      "Total training time: 98.65 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 639.50, NNZs: 8192, Bias: -33.305641, T: 4320000, Avg. loss: 107.722426\n",
      "Total training time: 98.67 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 534.79, NNZs: 8192, Bias: -8.547253, T: 4185000, Avg. loss: 151.179474\n",
      "Total training time: 98.75 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 606.65, NNZs: 8192, Bias: -53.808364, T: 4320000, Avg. loss: 125.533346\n",
      "Total training time: 98.78 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 561.50, NNZs: 8192, Bias: -58.346346, T: 4275000, Avg. loss: 144.872255\n",
      "Total training time: 98.85 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 699.20, NNZs: 8192, Bias: -50.344223, T: 4320000, Avg. loss: 118.551493\n",
      "Total training time: 98.95 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 538.05, NNZs: 8192, Bias: 49.156245, T: 4320000, Avg. loss: 94.494584\n",
      "Total training time: 99.00 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 534.36, NNZs: 8192, Bias: -19.405151, T: 4230000, Avg. loss: 172.362950\n",
      "Total training time: 99.30 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 607.36, NNZs: 8192, Bias: -52.213656, T: 4365000, Avg. loss: 125.764924\n",
      "Total training time: 99.53 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 593.63, NNZs: 8192, Bias: -5.380963, T: 4410000, Avg. loss: 79.244702\n",
      "Total training time: 99.66 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 636.68, NNZs: 8192, Bias: -33.319481, T: 4365000, Avg. loss: 106.023656\n",
      "Total training time: 99.70 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 553.15, NNZs: 8192, Bias: -61.370322, T: 4275000, Avg. loss: 157.011348\n",
      "Total training time: 99.71 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 532.36, NNZs: 8192, Bias: -8.554370, T: 4230000, Avg. loss: 149.267731\n",
      "Total training time: 99.84 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 603.80, NNZs: 8192, Bias: -53.817585, T: 4365000, Avg. loss: 123.144779\n",
      "Total training time: 99.83 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 558.85, NNZs: 8192, Bias: -58.355653, T: 4320000, Avg. loss: 142.789657\n",
      "Total training time: 99.92 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 535.55, NNZs: 8192, Bias: 49.165451, T: 4365000, Avg. loss: 93.743420\n",
      "Total training time: 100.07 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 695.29, NNZs: 8192, Bias: -50.348816, T: 4365000, Avg. loss: 115.055144\n",
      "Total training time: 100.02 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 531.76, NNZs: 8192, Bias: -19.402803, T: 4275000, Avg. loss: 170.619407\n",
      "Total training time: 100.42 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 604.51, NNZs: 8192, Bias: -52.220487, T: 4410000, Avg. loss: 124.525239\n",
      "Total training time: 100.63 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 590.77, NNZs: 8192, Bias: -5.385487, T: 4455000, Avg. loss: 80.755167\n",
      "Total training time: 100.72 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 633.83, NNZs: 8192, Bias: -33.333169, T: 4410000, Avg. loss: 102.441776\n",
      "Total training time: 100.78 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 551.16, NNZs: 8192, Bias: -61.377315, T: 4320000, Avg. loss: 153.008640\n",
      "Total training time: 100.82 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 601.23, NNZs: 8192, Bias: -53.828965, T: 4410000, Avg. loss: 121.567453\n",
      "Total training time: 100.92 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 529.76, NNZs: 8192, Bias: -8.547348, T: 4275000, Avg. loss: 147.201418\n",
      "Total training time: 100.95 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 556.01, NNZs: 8192, Bias: -58.371774, T: 4365000, Avg. loss: 138.381364\n",
      "Total training time: 101.02 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 533.01, NNZs: 8192, Bias: 49.174551, T: 4410000, Avg. loss: 93.083671\n",
      "Total training time: 101.13 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 691.82, NNZs: 8192, Bias: -50.360241, T: 4410000, Avg. loss: 113.896315\n",
      "Total training time: 101.09 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 529.58, NNZs: 8192, Bias: -19.405141, T: 4320000, Avg. loss: 167.154140\n",
      "Total training time: 101.50 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 601.74, NNZs: 8192, Bias: -52.218233, T: 4455000, Avg. loss: 125.099991\n",
      "Total training time: 101.71 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 587.85, NNZs: 8192, Bias: -5.381050, T: 4500000, Avg. loss: 77.798424\n",
      "Total training time: 101.78 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 630.77, NNZs: 8192, Bias: -33.348971, T: 4455000, Avg. loss: 105.134923\n",
      "Total training time: 101.84 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 549.04, NNZs: 8192, Bias: -61.384257, T: 4365000, Avg. loss: 151.383493\n",
      "Total training time: 101.91 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 598.72, NNZs: 8192, Bias: -53.840246, T: 4455000, Avg. loss: 120.048452\n",
      "Total training time: 101.97 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 527.83, NNZs: 8192, Bias: -8.556669, T: 4320000, Avg. loss: 144.810682\n",
      "Total training time: 102.05 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 553.52, NNZs: 8192, Bias: -58.392245, T: 4410000, Avg. loss: 136.538933\n",
      "Total training time: 102.11 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 530.45, NNZs: 8192, Bias: 49.172316, T: 4455000, Avg. loss: 92.475036\n",
      "Total training time: 102.19 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 688.16, NNZs: 8192, Bias: -50.367003, T: 4455000, Avg. loss: 116.058695\n",
      "Total training time: 102.16 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 527.28, NNZs: 8192, Bias: -19.407466, T: 4365000, Avg. loss: 168.513803\n",
      "Total training time: 102.57 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 598.63, NNZs: 8192, Bias: -52.213765, T: 4500000, Avg. loss: 124.659950\n",
      "Total training time: 102.75 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 584.91, NNZs: 8192, Bias: -5.385485, T: 4545000, Avg. loss: 78.367730\n",
      "Total training time: 102.80 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 627.74, NNZs: 8192, Bias: -33.364589, T: 4500000, Avg. loss: 100.197476\n",
      "Total training time: 102.90 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 546.93, NNZs: 8192, Bias: -61.386564, T: 4410000, Avg. loss: 148.768326\n",
      "Total training time: 102.96 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 596.32, NNZs: 8192, Bias: -53.855886, T: 4500000, Avg. loss: 117.606509\n",
      "Total training time: 102.99 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 525.69, NNZs: 8192, Bias: -8.556691, T: 4365000, Avg. loss: 143.954957\n",
      "Total training time: 103.13 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 551.01, NNZs: 8192, Bias: -58.408016, T: 4455000, Avg. loss: 135.165929\n",
      "Total training time: 103.15 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 528.14, NNZs: 8192, Bias: 49.181258, T: 4500000, Avg. loss: 90.724552\n",
      "Total training time: 103.23 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 684.75, NNZs: 8192, Bias: -50.375958, T: 4500000, Avg. loss: 113.356261\n",
      "Total training time: 103.19 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 524.97, NNZs: 8192, Bias: -19.412027, T: 4410000, Avg. loss: 168.169316\n",
      "Total training time: 103.65 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 595.89, NNZs: 8192, Bias: -52.215983, T: 4545000, Avg. loss: 121.229426\n",
      "Total training time: 103.82 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 582.13, NNZs: 8192, Bias: -5.385504, T: 4590000, Avg. loss: 76.976601\n",
      "Total training time: 103.84 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 624.76, NNZs: 8192, Bias: -33.382251, T: 4545000, Avg. loss: 99.896760\n",
      "Total training time: 103.97 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 544.67, NNZs: 8192, Bias: -61.393325, T: 4455000, Avg. loss: 150.233082\n",
      "Total training time: 104.05 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 593.56, NNZs: 8192, Bias: -53.866936, T: 4545000, Avg. loss: 117.526080\n",
      "Total training time: 104.07 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 548.59, NNZs: 8192, Bias: -58.419136, T: 4500000, Avg. loss: 132.954102\n",
      "Total training time: 104.24 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 523.71, NNZs: 8192, Bias: -8.554433, T: 4410000, Avg. loss: 141.020329\n",
      "Total training time: 104.25 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 681.33, NNZs: 8192, Bias: -50.387024, T: 4545000, Avg. loss: 112.013199\n",
      "Total training time: 104.27 seconds.\n",
      "Norm: 525.60, NNZs: 8192, Bias: 49.183451, T: 4545000, Avg. loss: 91.536063\n",
      "Total training time: 104.32 seconds.\n",
      "-- Epoch 102\n",
      "-- Epoch 102\n",
      "Norm: 523.00, NNZs: 8192, Bias: -19.409738, T: 4455000, Avg. loss: 163.212630\n",
      "Total training time: 104.75 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 593.21, NNZs: 8192, Bias: -52.215947, T: 4590000, Avg. loss: 120.686673\n",
      "Total training time: 104.87 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 579.25, NNZs: 8192, Bias: -5.389839, T: 4635000, Avg. loss: 75.607797\n",
      "Total training time: 104.90 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 621.85, NNZs: 8192, Bias: -33.395369, T: 4590000, Avg. loss: 100.659702\n",
      "Total training time: 105.04 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 542.58, NNZs: 8192, Bias: -61.395546, T: 4500000, Avg. loss: 147.416682\n",
      "Total training time: 105.14 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 591.07, NNZs: 8192, Bias: -53.875681, T: 4590000, Avg. loss: 116.770274\n",
      "Total training time: 105.15 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 546.13, NNZs: 8192, Bias: -58.430188, T: 4545000, Avg. loss: 133.137759\n",
      "Total training time: 105.36 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 521.50, NNZs: 8192, Bias: -8.558950, T: 4455000, Avg. loss: 140.036086\n",
      "Total training time: 105.37 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 677.93, NNZs: 8192, Bias: -50.393600, T: 4590000, Avg. loss: 108.436238\n",
      "Total training time: 105.36 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 523.15, NNZs: 8192, Bias: 49.187835, T: 4590000, Avg. loss: 90.110596\n",
      "Total training time: 105.41 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 520.90, NNZs: 8192, Bias: -19.409749, T: 4500000, Avg. loss: 162.407768\n",
      "Total training time: 105.84 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 590.41, NNZs: 8192, Bias: -52.215915, T: 4635000, Avg. loss: 116.901589\n",
      "Total training time: 105.93 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 576.50, NNZs: 8192, Bias: -5.387698, T: 4680000, Avg. loss: 74.909563\n",
      "Total training time: 105.95 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 618.93, NNZs: 8192, Bias: -33.404065, T: 4635000, Avg. loss: 98.747688\n",
      "Total training time: 106.09 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 540.46, NNZs: 8192, Bias: -61.399962, T: 4545000, Avg. loss: 144.502682\n",
      "Total training time: 106.21 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 588.79, NNZs: 8192, Bias: -53.886505, T: 4635000, Avg. loss: 115.678883\n",
      "Total training time: 106.22 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 543.75, NNZs: 8192, Bias: -58.447696, T: 4590000, Avg. loss: 131.854843\n",
      "Total training time: 106.43 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 674.65, NNZs: 8192, Bias: -50.402260, T: 4635000, Avg. loss: 110.209669\n",
      "Total training time: 106.41 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 520.80, NNZs: 8192, Bias: 49.192164, T: 4635000, Avg. loss: 87.430546\n",
      "Total training time: 106.48 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 519.14, NNZs: 8192, Bias: -8.552252, T: 4500000, Avg. loss: 140.547153\n",
      "Total training time: 106.46 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 519.06, NNZs: 8192, Bias: -19.403142, T: 4545000, Avg. loss: 159.815819\n",
      "Total training time: 106.95 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 587.82, NNZs: 8192, Bias: -52.218043, T: 4680000, Avg. loss: 115.051597\n",
      "Total training time: 107.01 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 573.79, NNZs: 8192, Bias: -5.391961, T: 4725000, Avg. loss: 74.083069\n",
      "Total training time: 107.01 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 616.09, NNZs: 8192, Bias: -33.423393, T: 4680000, Avg. loss: 97.164634\n",
      "Total training time: 107.16 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 538.44, NNZs: 8192, Bias: -61.406518, T: 4590000, Avg. loss: 144.444297\n",
      "Total training time: 107.30 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 586.40, NNZs: 8192, Bias: -53.899365, T: 4680000, Avg. loss: 114.589238\n",
      "Total training time: 107.29 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 541.31, NNZs: 8192, Bias: -58.456346, T: 4635000, Avg. loss: 131.534705\n",
      "Total training time: 107.50 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 518.68, NNZs: 8192, Bias: 49.192165, T: 4680000, Avg. loss: 87.296165\n",
      "Total training time: 107.53 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 671.25, NNZs: 8192, Bias: -50.406561, T: 4680000, Avg. loss: 108.853083\n",
      "Total training time: 107.49 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 516.79, NNZs: 8192, Bias: -8.556661, T: 4545000, Avg. loss: 137.303122\n",
      "Total training time: 107.55 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 516.84, NNZs: 8192, Bias: -19.411905, T: 4590000, Avg. loss: 160.047635\n",
      "Total training time: 108.02 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 585.08, NNZs: 8192, Bias: -52.220152, T: 4725000, Avg. loss: 116.242943\n",
      "Total training time: 108.06 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 571.21, NNZs: 8192, Bias: -5.391942, T: 4770000, Avg. loss: 73.455144\n",
      "Total training time: 108.05 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 613.57, NNZs: 8192, Bias: -33.436128, T: 4725000, Avg. loss: 94.527783\n",
      "Total training time: 108.21 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 536.69, NNZs: 8192, Bias: -61.413030, T: 4635000, Avg. loss: 142.105624\n",
      "Total training time: 108.34 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 583.86, NNZs: 8192, Bias: -53.905766, T: 4725000, Avg. loss: 110.549903\n",
      "Total training time: 108.33 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 538.54, NNZs: 8192, Bias: -58.464923, T: 4680000, Avg. loss: 129.447791\n",
      "Total training time: 108.54 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 516.43, NNZs: 8192, Bias: 49.200686, T: 4725000, Avg. loss: 85.852819\n",
      "Total training time: 108.56 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 667.91, NNZs: 8192, Bias: -50.419305, T: 4725000, Avg. loss: 106.691979\n",
      "Total training time: 108.52 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 514.53, NNZs: 8192, Bias: -8.558838, T: 4590000, Avg. loss: 134.171336\n",
      "Total training time: 108.62 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 568.70, NNZs: 8192, Bias: -5.391941, T: 4815000, Avg. loss: 72.619640\n",
      "Total training time: 109.06 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 582.27, NNZs: 8192, Bias: -52.213843, T: 4770000, Avg. loss: 114.480208\n",
      "Total training time: 109.09 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 514.98, NNZs: 8192, Bias: -19.416224, T: 4635000, Avg. loss: 156.186999\n",
      "Total training time: 109.07 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 610.66, NNZs: 8192, Bias: -33.446642, T: 4770000, Avg. loss: 93.498896\n",
      "Total training time: 109.23 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 534.62, NNZs: 8192, Bias: -61.419450, T: 4680000, Avg. loss: 140.139167\n",
      "Total training time: 109.37 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 581.39, NNZs: 8192, Bias: -53.916315, T: 4770000, Avg. loss: 110.550174\n",
      "Total training time: 109.35 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 536.40, NNZs: 8192, Bias: -58.475568, T: 4725000, Avg. loss: 127.428211\n",
      "Total training time: 109.57 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 665.21, NNZs: 8192, Bias: -50.425602, T: 4770000, Avg. loss: 104.757613\n",
      "Total training time: 109.53 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 514.14, NNZs: 8192, Bias: 49.207023, T: 4770000, Avg. loss: 85.526112\n",
      "Total training time: 109.59 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 512.24, NNZs: 8192, Bias: -8.560975, T: 4635000, Avg. loss: 135.801762\n",
      "Total training time: 109.65 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 566.34, NNZs: 8192, Bias: -5.398117, T: 4860000, Avg. loss: 73.341113\n",
      "Total training time: 110.08 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 579.73, NNZs: 8192, Bias: -52.215942, T: 4815000, Avg. loss: 110.993521\n",
      "Total training time: 110.12 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 512.88, NNZs: 8192, Bias: -19.418332, T: 4680000, Avg. loss: 154.958892\n",
      "Total training time: 110.12 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 608.17, NNZs: 8192, Bias: -33.457075, T: 4815000, Avg. loss: 93.984554\n",
      "Total training time: 110.26 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 579.07, NNZs: 8192, Bias: -53.926761, T: 4815000, Avg. loss: 111.427317\n",
      "Total training time: 110.41 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 532.67, NNZs: 8192, Bias: -61.425828, T: 4725000, Avg. loss: 141.368745\n",
      "Total training time: 110.44 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 511.73, NNZs: 8192, Bias: 49.213247, T: 4815000, Avg. loss: 85.529805\n",
      "Total training time: 110.61 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 533.80, NNZs: 8192, Bias: -58.484025, T: 4770000, Avg. loss: 128.159081\n",
      "Total training time: 110.61 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 662.03, NNZs: 8192, Bias: -50.425612, T: 4815000, Avg. loss: 106.708207\n",
      "Total training time: 110.58 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 510.31, NNZs: 8192, Bias: -8.558814, T: 4680000, Avg. loss: 132.276765\n",
      "Total training time: 110.70 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 563.96, NNZs: 8192, Bias: -5.398079, T: 4905000, Avg. loss: 70.293182\n",
      "Total training time: 111.09 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 577.53, NNZs: 8192, Bias: -52.218019, T: 4860000, Avg. loss: 112.837417\n",
      "Total training time: 111.15 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 510.90, NNZs: 8192, Bias: -19.418344, T: 4725000, Avg. loss: 154.922923\n",
      "Total training time: 111.18 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 605.36, NNZs: 8192, Bias: -33.475668, T: 4860000, Avg. loss: 91.043946\n",
      "Total training time: 111.26 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 577.00, NNZs: 8192, Bias: -53.941218, T: 4860000, Avg. loss: 110.667622\n",
      "Total training time: 111.43 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 530.48, NNZs: 8192, Bias: -61.432146, T: 4770000, Avg. loss: 137.177578\n",
      "Total training time: 111.48 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 509.53, NNZs: 8192, Bias: 49.217383, T: 4860000, Avg. loss: 82.963187\n",
      "Total training time: 111.63 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 531.61, NNZs: 8192, Bias: -58.496552, T: 4815000, Avg. loss: 124.863540\n",
      "Total training time: 111.65 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 658.95, NNZs: 8192, Bias: -50.440111, T: 4860000, Avg. loss: 105.141591\n",
      "Total training time: 111.62 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 508.42, NNZs: 8192, Bias: -8.552437, T: 4725000, Avg. loss: 132.251514\n",
      "Total training time: 111.76 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 561.42, NNZs: 8192, Bias: -5.398056, T: 4950000, Avg. loss: 69.581066\n",
      "Total training time: 112.15 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 574.90, NNZs: 8192, Bias: -52.215986, T: 4905000, Avg. loss: 111.905057\n",
      "Total training time: 112.21 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 508.88, NNZs: 8192, Bias: -19.420485, T: 4770000, Avg. loss: 152.448613\n",
      "Total training time: 112.27 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 602.70, NNZs: 8192, Bias: -33.485928, T: 4905000, Avg. loss: 92.366456\n",
      "Total training time: 112.32 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 574.49, NNZs: 8192, Bias: -53.951472, T: 4905000, Avg. loss: 108.374200\n",
      "Total training time: 112.50 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 528.81, NNZs: 8192, Bias: -61.440488, T: 4815000, Avg. loss: 137.294929\n",
      "Total training time: 112.58 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 507.14, NNZs: 8192, Bias: 49.223512, T: 4905000, Avg. loss: 82.159711\n",
      "Total training time: 112.71 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 655.82, NNZs: 8192, Bias: -50.444196, T: 4905000, Avg. loss: 101.900754\n",
      "Total training time: 112.69 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 529.42, NNZs: 8192, Bias: -58.508958, T: 4860000, Avg. loss: 122.936696\n",
      "Total training time: 112.74 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 506.43, NNZs: 8192, Bias: -8.554529, T: 4770000, Avg. loss: 129.366862\n",
      "Total training time: 112.88 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 559.03, NNZs: 8192, Bias: -5.398056, T: 4995000, Avg. loss: 68.861246\n",
      "Total training time: 113.20 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 572.41, NNZs: 8192, Bias: -52.213953, T: 4950000, Avg. loss: 110.186299\n",
      "Total training time: 113.29 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 506.93, NNZs: 8192, Bias: -19.426742, T: 4815000, Avg. loss: 151.192249\n",
      "Total training time: 113.39 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 600.18, NNZs: 8192, Bias: -33.502151, T: 4950000, Avg. loss: 91.074915\n",
      "Total training time: 113.42 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 572.11, NNZs: 8192, Bias: -53.955521, T: 4950000, Avg. loss: 107.254801\n",
      "Total training time: 113.59 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 526.54, NNZs: 8192, Bias: -61.442560, T: 4860000, Avg. loss: 136.380819\n",
      "Total training time: 113.67 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 504.92, NNZs: 8192, Bias: 49.231638, T: 4950000, Avg. loss: 82.192394\n",
      "Total training time: 113.81 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 652.93, NNZs: 8192, Bias: -50.456361, T: 4950000, Avg. loss: 101.945649\n",
      "Total training time: 113.78 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 527.29, NNZs: 8192, Bias: -58.519197, T: 4905000, Avg. loss: 122.782273\n",
      "Total training time: 113.84 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 504.39, NNZs: 8192, Bias: -8.552425, T: 4815000, Avg. loss: 128.074972\n",
      "Total training time: 114.01 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 556.93, NNZs: 8192, Bias: -5.400067, T: 5040000, Avg. loss: 67.632521\n",
      "Total training time: 114.30 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 569.72, NNZs: 8192, Bias: -52.213951, T: 4995000, Avg. loss: 111.121515\n",
      "Total training time: 114.40 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 597.84, NNZs: 8192, Bias: -33.508190, T: 4995000, Avg. loss: 89.199868\n",
      "Total training time: 114.52 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 504.84, NNZs: 8192, Bias: -19.434960, T: 4860000, Avg. loss: 148.856654\n",
      "Total training time: 114.50 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 569.82, NNZs: 8192, Bias: -53.963577, T: 4995000, Avg. loss: 105.058586\n",
      "Total training time: 114.67 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 524.66, NNZs: 8192, Bias: -61.452808, T: 4905000, Avg. loss: 133.367864\n",
      "Total training time: 114.79 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 649.85, NNZs: 8192, Bias: -50.458372, T: 4995000, Avg. loss: 99.388104\n",
      "Total training time: 114.87 seconds.\n",
      "Norm: 502.78, NNZs: 8192, Bias: 49.239649, T: 4995000, Avg. loss: 80.844217-- Epoch 112\n",
      "\n",
      "Total training time: 114.92 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 525.35, NNZs: 8192, Bias: -58.533405, T: 4950000, Avg. loss: 119.583903\n",
      "Total training time: 114.95 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 502.41, NNZs: 8192, Bias: -8.552416, T: 4860000, Avg. loss: 124.990062\n",
      "Total training time: 115.15 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 554.66, NNZs: 8192, Bias: -5.400071, T: 5085000, Avg. loss: 68.036698\n",
      "Total training time: 115.38 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 567.26, NNZs: 8192, Bias: -52.211947, T: 5040000, Avg. loss: 108.342015\n",
      "Total training time: 115.48 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 595.46, NNZs: 8192, Bias: -33.518180, T: 5040000, Avg. loss: 89.113993\n",
      "Total training time: 115.62 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 503.09, NNZs: 8192, Bias: -19.432911, T: 4905000, Avg. loss: 147.281438\n",
      "Total training time: 115.63 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 567.85, NNZs: 8192, Bias: -53.971541, T: 5040000, Avg. loss: 103.152459\n",
      "Total training time: 115.80 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 522.80, NNZs: 8192, Bias: -61.456850, T: 4950000, Avg. loss: 130.005456\n",
      "Total training time: 115.97 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 646.95, NNZs: 8192, Bias: -50.470333, T: 5040000, Avg. loss: 99.146764\n",
      "Total training time: 116.04 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 500.71, NNZs: 8192, Bias: 49.243632, T: 5040000, Avg. loss: 81.140259\n",
      "Total training time: 116.09 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 523.04, NNZs: 8192, Bias: -58.541450, T: 4995000, Avg. loss: 120.379982\n",
      "Total training time: 116.11 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 500.54, NNZs: 8192, Bias: -8.550358, T: 4905000, Avg. loss: 126.210467\n",
      "Total training time: 116.32 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 552.36, NNZs: 8192, Bias: -5.403985, T: 5130000, Avg. loss: 67.397799\n",
      "Total training time: 116.51 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 565.09, NNZs: 8192, Bias: -52.211946, T: 5085000, Avg. loss: 105.674978\n",
      "Total training time: 116.59 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 592.93, NNZs: 8192, Bias: -33.533982, T: 5085000, Avg. loss: 88.822498\n",
      "Total training time: 116.73 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 501.19, NNZs: 8192, Bias: -19.438990, T: 4950000, Avg. loss: 147.128590\n",
      "Total training time: 116.77 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 565.57, NNZs: 8192, Bias: -53.975496, T: 5085000, Avg. loss: 104.779178\n",
      "Total training time: 116.88 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 520.84, NNZs: 8192, Bias: -61.458843, T: 4995000, Avg. loss: 130.100577\n",
      "Total training time: 117.05 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 643.88, NNZs: 8192, Bias: -50.470348, T: 5085000, Avg. loss: 99.703537\n",
      "Total training time: 117.10 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 498.73, NNZs: 8192, Bias: 49.249564, T: 5085000, Avg. loss: 78.907370\n",
      "Total training time: 117.17 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 520.87, NNZs: 8192, Bias: -58.553423, T: 5040000, Avg. loss: 119.461485\n",
      "Total training time: 117.17 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 498.70, NNZs: 8192, Bias: -8.550363, T: 4950000, Avg. loss: 125.307369\n",
      "Total training time: 117.40 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 550.23, NNZs: 8192, Bias: -5.396223, T: 5175000, Avg. loss: 64.763811\n",
      "Total training time: 117.53 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 562.67, NNZs: 8192, Bias: -52.211954, T: 5130000, Avg. loss: 104.342523\n",
      "Total training time: 117.62 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 590.53, NNZs: 8192, Bias: -33.545723, T: 5130000, Avg. loss: 86.818904\n",
      "Total training time: 117.77 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 499.19, NNZs: 8192, Bias: -19.444999, T: 4995000, Avg. loss: 146.485097\n",
      "Total training time: 117.85 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 563.54, NNZs: 8192, Bias: -53.987251, T: 5130000, Avg. loss: 102.315494\n",
      "Total training time: 117.94 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 519.22, NNZs: 8192, Bias: -61.460854, T: 5040000, Avg. loss: 127.359267\n",
      "Total training time: 118.12 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 640.98, NNZs: 8192, Bias: -50.480150, T: 5130000, Avg. loss: 97.083212\n",
      "Total training time: 118.14 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 496.95, NNZs: 8192, Bias: 49.255433, T: 5130000, Avg. loss: 78.773967\n",
      "Total training time: 118.23 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 518.88, NNZs: 8192, Bias: -58.565270, T: 5085000, Avg. loss: 117.052638\n",
      "Total training time: 118.26 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 496.84, NNZs: 8192, Bias: -8.552371, T: 4995000, Avg. loss: 123.889151\n",
      "Total training time: 118.48 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 547.76, NNZs: 8192, Bias: -5.394304, T: 5220000, Avg. loss: 67.289143\n",
      "Total training time: 118.58 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 560.64, NNZs: 8192, Bias: -52.219711, T: 5175000, Avg. loss: 103.154573\n",
      "Total training time: 118.68 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 588.12, NNZs: 8192, Bias: -33.555429, T: 5175000, Avg. loss: 84.807493\n",
      "Total training time: 118.81 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 497.47, NNZs: 8192, Bias: -19.445015, T: 5040000, Avg. loss: 142.629297\n",
      "Total training time: 118.91 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 561.39, NNZs: 8192, Bias: -53.993087, T: 5175000, Avg. loss: 100.823488\n",
      "Total training time: 118.97 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 517.25, NNZs: 8192, Bias: -61.470717, T: 5085000, Avg. loss: 129.578680\n",
      "Total training time: 119.16 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 637.99, NNZs: 8192, Bias: -50.489853, T: 5175000, Avg. loss: 97.029929\n",
      "Total training time: 119.15 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 494.90, NNZs: 8192, Bias: 49.255426, T: 5175000, Avg. loss: 77.788799\n",
      "Total training time: 119.26 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 516.81, NNZs: 8192, Bias: -58.577018, T: 5130000, Avg. loss: 118.094379\n",
      "Total training time: 119.29 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 494.87, NNZs: 8192, Bias: -8.554351, T: 5040000, Avg. loss: 123.014231\n",
      "Total training time: 119.52 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 545.52, NNZs: 8192, Bias: -5.396244, T: 5265000, Avg. loss: 66.941575\n",
      "Total training time: 119.59 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 558.38, NNZs: 8192, Bias: -52.219686, T: 5220000, Avg. loss: 102.921080\n",
      "Total training time: 119.70 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 585.67, NNZs: 8192, Bias: -33.566975, T: 5220000, Avg. loss: 84.474468\n",
      "Total training time: 119.82 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 495.45, NNZs: 8192, Bias: -19.446993, T: 5085000, Avg. loss: 143.715478\n",
      "Total training time: 119.96 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 559.41, NNZs: 8192, Bias: -54.002717, T: 5220000, Avg. loss: 100.424398\n",
      "Total training time: 119.99 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 515.61, NNZs: 8192, Bias: -61.476597, T: 5130000, Avg. loss: 127.444862\n",
      "Total training time: 120.19 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 635.20, NNZs: 8192, Bias: -50.491790, T: 5220000, Avg. loss: 95.513202\n",
      "Total training time: 120.19 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 493.04, NNZs: 8192, Bias: 49.255437, T: 5220000, Avg. loss: 77.950310\n",
      "Total training time: 120.28 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 514.90, NNZs: 8192, Bias: -58.588658, T: 5175000, Avg. loss: 114.371200\n",
      "Total training time: 120.33 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 492.87, NNZs: 8192, Bias: -8.546461, T: 5085000, Avg. loss: 121.317147\n",
      "Total training time: 120.59 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 543.47, NNZs: 8192, Bias: -5.403813, T: 5310000, Avg. loss: 64.690465\n",
      "Total training time: 120.61 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 556.13, NNZs: 8192, Bias: -52.215870, T: 5265000, Avg. loss: 102.782751\n",
      "Total training time: 120.74 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 583.36, NNZs: 8192, Bias: -33.574603, T: 5265000, Avg. loss: 83.161983\n",
      "Total training time: 120.86 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 493.73, NNZs: 8192, Bias: -19.454793, T: 5130000, Avg. loss: 140.280947\n",
      "Total training time: 121.05 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 557.43, NNZs: 8192, Bias: -54.010342, T: 5265000, Avg. loss: 99.115874\n",
      "Total training time: 121.04 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 632.64, NNZs: 8192, Bias: -50.501339, T: 5265000, Avg. loss: 94.165774Norm: 513.86, NNZs: 8192, Bias: -61.480484, T: 5175000, Avg. loss: 126.369942\n",
      "\n",
      "Total training time: 121.26 seconds.\n",
      "-- Epoch 116\n",
      "Total training time: 121.24 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 491.04, NNZs: 8192, Bias: 49.261156, T: 5265000, Avg. loss: 76.709434\n",
      "Total training time: 121.33 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 512.65, NNZs: 8192, Bias: -58.596347, T: 5220000, Avg. loss: 113.939367\n",
      "Total training time: 121.39 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 541.10, NNZs: 8192, Bias: -5.403813, T: 5355000, Avg. loss: 65.441638\n",
      "Total training time: 121.66 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 491.07, NNZs: 8192, Bias: -8.552338, T: 5130000, Avg. loss: 120.598686\n",
      "Total training time: 121.68 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 554.02, NNZs: 8192, Bias: -52.217750, T: 5310000, Avg. loss: 99.078182\n",
      "Total training time: 121.83 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 581.00, NNZs: 8192, Bias: -33.584064, T: 5310000, Avg. loss: 84.637935\n",
      "Total training time: 121.95 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 555.48, NNZs: 8192, Bias: -54.016013, T: 5310000, Avg. loss: 98.401111\n",
      "Total training time: 122.17 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 492.36, NNZs: 8192, Bias: -19.448951, T: 5175000, Avg. loss: 137.985901\n",
      "Total training time: 122.21 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 629.80, NNZs: 8192, Bias: -50.508905, T: 5310000, Avg. loss: 93.815722\n",
      "Total training time: 122.36 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 512.05, NNZs: 8192, Bias: -61.482418, T: 5220000, Avg. loss: 126.739777\n",
      "Total training time: 122.42 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 489.01, NNZs: 8192, Bias: 49.274401, T: 5310000, Avg. loss: 75.678644\n",
      "Total training time: 122.46 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 510.88, NNZs: 8192, Bias: -58.611595, T: 5265000, Avg. loss: 113.096255\n",
      "Total training time: 122.54 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 538.93, NNZs: 8192, Bias: -5.403819, T: 5400000, Avg. loss: 62.569005\n",
      "Total training time: 122.72 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 489.27, NNZs: 8192, Bias: -8.552336, T: 5175000, Avg. loss: 118.538512\n",
      "Total training time: 122.78 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 551.82, NNZs: 8192, Bias: -52.215882, T: 5355000, Avg. loss: 100.653667\n",
      "Total training time: 122.88 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 578.68, NNZs: 8192, Bias: -33.597171, T: 5355000, Avg. loss: 82.913517\n",
      "Total training time: 122.97 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 553.57, NNZs: 8192, Bias: -54.025382, T: 5355000, Avg. loss: 97.358391\n",
      "Total training time: 123.21 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 490.53, NNZs: 8192, Bias: -19.454716, T: 5220000, Avg. loss: 138.499560\n",
      "Total training time: 123.27 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 627.02, NNZs: 8192, Bias: -50.514519, T: 5355000, Avg. loss: 93.401370\n",
      "Total training time: 123.41 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 510.51, NNZs: 8192, Bias: -61.497663, T: 5265000, Avg. loss: 123.171273\n",
      "Total training time: 123.48 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 486.95, NNZs: 8192, Bias: 49.274411, T: 5355000, Avg. loss: 77.056221\n",
      "Total training time: 123.53 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 509.04, NNZs: 8192, Bias: -58.621043, T: 5310000, Avg. loss: 112.117283\n",
      "Total training time: 123.60 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 536.90, NNZs: 8192, Bias: -5.400135, T: 5445000, Avg. loss: 63.481800\n",
      "Total training time: 123.75 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 487.45, NNZs: 8192, Bias: -8.550427, T: 5220000, Avg. loss: 120.241837\n",
      "Total training time: 123.87 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 549.61, NNZs: 8192, Bias: -52.217743, T: 5400000, Avg. loss: 99.306266\n",
      "Total training time: 123.91 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 576.44, NNZs: 8192, Bias: -33.606474, T: 5400000, Avg. loss: 82.199785\n",
      "Total training time: 124.02 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 551.48, NNZs: 8192, Bias: -54.030962, T: 5400000, Avg. loss: 96.507952\n",
      "Total training time: 124.28 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 488.88, NNZs: 8192, Bias: -19.450905, T: 5265000, Avg. loss: 136.152995\n",
      "Total training time: 124.37 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 624.56, NNZs: 8192, Bias: -50.521953, T: 5400000, Avg. loss: 90.839364\n",
      "Total training time: 124.48 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 508.75, NNZs: 8192, Bias: -61.495774, T: 5310000, Avg. loss: 122.598249\n",
      "Total training time: 124.59 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 485.18, NNZs: 8192, Bias: 49.279982, T: 5400000, Avg. loss: 73.803238\n",
      "Total training time: 124.62 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 507.19, NNZs: 8192, Bias: -58.624796, T: 5355000, Avg. loss: 112.637041\n",
      "Total training time: 124.72 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 534.82, NNZs: 8192, Bias: -5.401972, T: 5490000, Avg. loss: 60.817577\n",
      "Total training time: 124.84 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 485.62, NNZs: 8192, Bias: -8.556148, T: 5265000, Avg. loss: 115.964114\n",
      "Total training time: 125.01 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 547.44, NNZs: 8192, Bias: -52.215904, T: 5445000, Avg. loss: 97.091764\n",
      "Total training time: 125.03 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 574.17, NNZs: 8192, Bias: -33.617535, T: 5445000, Avg. loss: 81.852737\n",
      "Total training time: 125.13 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 549.48, NNZs: 8192, Bias: -54.040170, T: 5445000, Avg. loss: 94.276690\n",
      "Total training time: 125.36 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 487.35, NNZs: 8192, Bias: -19.462259, T: 5310000, Avg. loss: 135.063852\n",
      "Total training time: 125.48 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 622.17, NNZs: 8192, Bias: -50.527487, T: 5445000, Avg. loss: 92.822514\n",
      "Total training time: 125.56 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 483.37, NNZs: 8192, Bias: 49.283678, T: 5445000, Avg. loss: 73.273464\n",
      "Total training time: 125.67 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 507.00, NNZs: 8192, Bias: -61.505161, T: 5355000, Avg. loss: 121.519228\n",
      "Total training time: 125.66 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 505.29, NNZs: 8192, Bias: -58.641523, T: 5400000, Avg. loss: 109.236154\n",
      "Total training time: 125.78 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 532.79, NNZs: 8192, Bias: -5.403773, T: 5535000, Avg. loss: 60.874886\n",
      "Total training time: 125.87 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 545.46, NNZs: 8192, Bias: -52.212243, T: 5490000, Avg. loss: 98.395087\n",
      "Total training time: 126.08 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 483.83, NNZs: 8192, Bias: -8.554253, T: 5310000, Avg. loss: 114.842864\n",
      "Total training time: 126.10 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 571.80, NNZs: 8192, Bias: -33.624854, T: 5490000, Avg. loss: 80.583674\n",
      "Total training time: 126.20 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 547.79, NNZs: 8192, Bias: -54.043826, T: 5490000, Avg. loss: 94.836469\n",
      "Total training time: 126.44 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 485.42, NNZs: 8192, Bias: -19.458485, T: 5355000, Avg. loss: 133.232389\n",
      "Total training time: 126.57 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 619.53, NNZs: 8192, Bias: -50.532976, T: 5490000, Avg. loss: 88.941091\n",
      "Total training time: 126.63 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 481.26, NNZs: 8192, Bias: 49.281851, T: 5490000, Avg. loss: 72.456232\n",
      "Total training time: 126.74 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 505.46, NNZs: 8192, Bias: -61.512597, T: 5400000, Avg. loss: 120.441715\n",
      "Total training time: 126.76 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 503.36, NNZs: 8192, Bias: -58.648884, T: 5445000, Avg. loss: 108.460543\n",
      "Total training time: 126.87 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 530.76, NNZs: 8192, Bias: -5.405558, T: 5580000, Avg. loss: 61.073842\n",
      "Total training time: 126.94 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 543.45, NNZs: 8192, Bias: -52.212249, T: 5535000, Avg. loss: 96.560244\n",
      "Total training time: 127.16 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 482.16, NNZs: 8192, Bias: -8.554236, T: 5355000, Avg. loss: 114.591669\n",
      "Total training time: 127.22 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 569.54, NNZs: 8192, Bias: -33.637550, T: 5535000, Avg. loss: 79.463443\n",
      "Total training time: 127.28 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 545.67, NNZs: 8192, Bias: -54.056517, T: 5535000, Avg. loss: 93.868738\n",
      "Total training time: 127.50 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 483.74, NNZs: 8192, Bias: -19.462200, T: 5400000, Avg. loss: 133.606490\n",
      "Total training time: 127.65 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 616.92, NNZs: 8192, Bias: -50.540232, T: 5535000, Avg. loss: 89.208656\n",
      "Total training time: 127.70 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 479.47, NNZs: 8192, Bias: 49.292746, T: 5535000, Avg. loss: 73.487278\n",
      "Total training time: 127.82 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 503.67, NNZs: 8192, Bias: -61.514432, T: 5445000, Avg. loss: 119.470584\n",
      "Total training time: 127.86 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 501.34, NNZs: 8192, Bias: -58.659856, T: 5490000, Avg. loss: 107.698258\n",
      "Total training time: 127.95 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 528.76, NNZs: 8192, Bias: -5.407337, T: 5625000, Avg. loss: 60.518877\n",
      "Total training time: 128.00 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 541.26, NNZs: 8192, Bias: -52.214040, T: 5580000, Avg. loss: 95.467745\n",
      "Total training time: 128.24 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 480.62, NNZs: 8192, Bias: -8.552375, T: 5400000, Avg. loss: 112.020396\n",
      "Total training time: 128.35 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 567.11, NNZs: 8192, Bias: -33.646540, T: 5580000, Avg. loss: 78.856147\n",
      "Total training time: 128.37 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 543.72, NNZs: 8192, Bias: -54.061913, T: 5580000, Avg. loss: 92.598602\n",
      "Total training time: 128.57 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 482.38, NNZs: 8192, Bias: -19.469578, T: 5445000, Avg. loss: 132.039654\n",
      "Total training time: 128.78 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 614.43, NNZs: 8192, Bias: -50.545641, T: 5580000, Avg. loss: 87.967256\n",
      "Total training time: 128.78 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 477.85, NNZs: 8192, Bias: 49.298132, T: 5580000, Avg. loss: 70.055802\n",
      "Total training time: 128.93 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 502.29, NNZs: 8192, Bias: -61.519897, T: 5490000, Avg. loss: 116.988489\n",
      "Total training time: 128.96 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 499.38, NNZs: 8192, Bias: -58.670726, T: 5535000, Avg. loss: 107.445861\n",
      "Total training time: 129.04 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 526.63, NNZs: 8192, Bias: -5.407328, T: 5670000, Avg. loss: 60.560286\n",
      "Total training time: 129.06 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 539.23, NNZs: 8192, Bias: -52.210470, T: 5625000, Avg. loss: 94.976615\n",
      "Total training time: 129.31 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 564.98, NNZs: 8192, Bias: -33.655464, T: 5625000, Avg. loss: 78.561785\n",
      "Total training time: 129.40 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 479.01, NNZs: 8192, Bias: -8.554225, T: 5445000, Avg. loss: 112.727666\n",
      "Total training time: 129.43 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 541.82, NNZs: 8192, Bias: -54.070829, T: 5625000, Avg. loss: 92.479095\n",
      "Total training time: 129.62 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 480.78, NNZs: 8192, Bias: -19.464083, T: 5490000, Avg. loss: 132.073001\n",
      "Total training time: 129.84 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 611.74, NNZs: 8192, Bias: -50.552791, T: 5625000, Avg. loss: 88.501574\n",
      "Total training time: 129.85 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 476.18, NNZs: 8192, Bias: 49.299924, T: 5625000, Avg. loss: 70.779337\n",
      "Total training time: 129.97 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 500.77, NNZs: 8192, Bias: -61.521706, T: 5535000, Avg. loss: 117.881672\n",
      "Total training time: 130.03 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 497.42, NNZs: 8192, Bias: -58.676122, T: 5580000, Avg. loss: 106.158588\n",
      "Total training time: 130.09 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 524.64, NNZs: 8192, Bias: -5.409078, T: 5715000, Avg. loss: 60.588025\n",
      "Total training time: 130.09 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 537.33, NNZs: 8192, Bias: -52.212240, T: 5670000, Avg. loss: 94.680394\n",
      "Total training time: 130.35 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 562.90, NNZs: 8192, Bias: -33.666092, T: 5670000, Avg. loss: 77.014826\n",
      "Total training time: 130.45 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 477.39, NNZs: 8192, Bias: -8.550559, T: 5490000, Avg. loss: 111.814394\n",
      "Total training time: 130.52 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 540.02, NNZs: 8192, Bias: -54.072596, T: 5670000, Avg. loss: 91.004622\n",
      "Total training time: 130.69 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 609.24, NNZs: 8192, Bias: -50.558108, T: 5670000, Avg. loss: 87.314000\n",
      "Total training time: 130.91 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 479.16, NNZs: 8192, Bias: -19.465908, T: 5535000, Avg. loss: 129.792460\n",
      "Total training time: 130.93 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 474.59, NNZs: 8192, Bias: 49.299935, T: 5670000, Avg. loss: 70.337590\n",
      "Total training time: 131.02 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 498.92, NNZs: 8192, Bias: -61.525308, T: 5580000, Avg. loss: 115.647169\n",
      "Total training time: 131.11 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 495.61, NNZs: 8192, Bias: -58.686847, T: 5625000, Avg. loss: 104.192240\n",
      "Total training time: 131.17 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 522.69, NNZs: 8192, Bias: -5.405594, T: 5760000, Avg. loss: 60.020024\n",
      "Total training time: 131.14 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 535.22, NNZs: 8192, Bias: -52.208733, T: 5715000, Avg. loss: 93.324637\n",
      "Total training time: 131.40 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 560.73, NNZs: 8192, Bias: -33.680144, T: 5715000, Avg. loss: 76.562111\n",
      "Total training time: 131.47 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 475.76, NNZs: 8192, Bias: -8.554158, T: 5535000, Avg. loss: 111.094820\n",
      "Total training time: 131.59 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 538.12, NNZs: 8192, Bias: -54.081390, T: 5715000, Avg. loss: 88.575691\n",
      "Total training time: 131.73 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 606.71, NNZs: 8192, Bias: -50.561626, T: 5715000, Avg. loss: 86.795141\n",
      "Total training time: 131.96 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 477.62, NNZs: 8192, Bias: -19.465915, T: 5580000, Avg. loss: 128.573330\n",
      "Total training time: 131.99 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 472.88, NNZs: 8192, Bias: 49.312228, T: 5715000, Avg. loss: 69.870604\n",
      "Total training time: 132.07 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 497.25, NNZs: 8192, Bias: -61.525318, T: 5625000, Avg. loss: 115.065585\n",
      "Total training time: 132.16 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 520.84, NNZs: 8192, Bias: -5.403851, T: 5805000, Avg. loss: 57.933990\n",
      "Total training time: 132.16 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 493.86, NNZs: 8192, Bias: -58.697486, T: 5670000, Avg. loss: 104.080517\n",
      "Total training time: 132.22 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 533.17, NNZs: 8192, Bias: -52.210496, T: 5760000, Avg. loss: 93.545732\n",
      "Total training time: 132.44 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 558.60, NNZs: 8192, Bias: -33.687116, T: 5760000, Avg. loss: 75.128299\n",
      "Total training time: 132.48 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 474.18, NNZs: 8192, Bias: -8.545160, T: 5580000, Avg. loss: 109.175001\n",
      "Total training time: 132.65 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 536.38, NNZs: 8192, Bias: -54.090101, T: 5760000, Avg. loss: 88.063550\n",
      "Total training time: 132.76 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 604.27, NNZs: 8192, Bias: -50.566867, T: 5760000, Avg. loss: 85.186381\n",
      "Total training time: 132.99 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 475.93, NNZs: 8192, Bias: -19.467705, T: 5625000, Avg. loss: 128.205928\n",
      "Total training time: 133.05 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 471.05, NNZs: 8192, Bias: 49.315687, T: 5760000, Avg. loss: 69.803235\n",
      "Total training time: 133.10 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 518.86, NNZs: 8192, Bias: -5.403845, T: 5850000, Avg. loss: 58.106535\n",
      "Total training time: 133.16 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 495.66, NNZs: 8192, Bias: -61.532411, T: 5670000, Avg. loss: 112.348196\n",
      "Total training time: 133.22 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 492.07, NNZs: 8192, Bias: -58.708028, T: 5715000, Avg. loss: 104.800726\n",
      "Total training time: 133.26 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 531.46, NNZs: 8192, Bias: -52.213958, T: 5805000, Avg. loss: 89.418738\n",
      "Total training time: 133.48 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 556.86, NNZs: 8192, Bias: -33.697489, T: 5805000, Avg. loss: 75.020704\n",
      "Total training time: 133.51 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 472.64, NNZs: 8192, Bias: -8.548736, T: 5625000, Avg. loss: 109.401267\n",
      "Total training time: 133.72 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 534.35, NNZs: 8192, Bias: -54.095290, T: 5805000, Avg. loss: 88.670383\n",
      "Total training time: 133.80 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 601.98, NNZs: 8192, Bias: -50.575505, T: 5805000, Avg. loss: 84.034850\n",
      "Total training time: 134.04 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 469.51, NNZs: 8192, Bias: 49.320869, T: 5805000, Avg. loss: 67.830524\n",
      "Total training time: 134.15 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 474.47, NNZs: 8192, Bias: -19.467717, T: 5670000, Avg. loss: 127.973996\n",
      "Total training time: 134.12 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 516.96, NNZs: 8192, Bias: -5.405553, T: 5895000, Avg. loss: 56.340681\n",
      "Total training time: 134.19 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 494.21, NNZs: 8192, Bias: -61.537671, T: 5715000, Avg. loss: 112.705469\n",
      "Total training time: 134.29 seconds.\n",
      "Norm: 490.21, NNZs: 8192, Bias: -58.721967, T: 5760000, Avg. loss: 102.583506\n",
      "Total training time: 134.31 seconds.\n",
      "-- Epoch 128\n",
      "-- Epoch 129\n",
      "Norm: 529.28, NNZs: 8192, Bias: -52.210516, T: 5850000, Avg. loss: 92.230696\n",
      "Total training time: 134.51 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 554.90, NNZs: 8192, Bias: -33.704345, T: 5850000, Avg. loss: 75.357560\n",
      "Total training time: 134.55 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 471.26, NNZs: 8192, Bias: -8.550512, T: 5670000, Avg. loss: 107.759917\n",
      "Total training time: 134.78 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 532.50, NNZs: 8192, Bias: -54.098722, T: 5850000, Avg. loss: 88.269187\n",
      "Total training time: 134.82 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 599.63, NNZs: 8192, Bias: -50.578922, T: 5850000, Avg. loss: 83.549375\n",
      "Total training time: 135.06 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 467.91, NNZs: 8192, Bias: 49.322574, T: 5850000, Avg. loss: 67.753794\n",
      "Total training time: 135.18 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 473.10, NNZs: 8192, Bias: -19.469476, T: 5715000, Avg. loss: 125.619168\n",
      "Total training time: 135.17 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 515.16, NNZs: 8192, Bias: -5.403851, T: 5940000, Avg. loss: 57.388839\n",
      "Total training time: 135.19 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 488.46, NNZs: 8192, Bias: -58.727154, T: 5805000, Avg. loss: 100.689317\n",
      "Total training time: 135.35 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 492.57, NNZs: 8192, Bias: -61.541176, T: 5760000, Avg. loss: 112.084213\n",
      "Total training time: 135.34 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 527.54, NNZs: 8192, Bias: -52.208824, T: 5895000, Avg. loss: 90.184291\n",
      "Total training time: 135.53 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 553.27, NNZs: 8192, Bias: -33.711152, T: 5895000, Avg. loss: 73.998042\n",
      "Total training time: 135.60 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 469.64, NNZs: 8192, Bias: -8.552272, T: 5715000, Avg. loss: 107.399756\n",
      "Total training time: 135.84 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 530.60, NNZs: 8192, Bias: -54.108934, T: 5895000, Avg. loss: 86.995694\n",
      "Total training time: 135.86 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 597.23, NNZs: 8192, Bias: -50.585706, T: 5895000, Avg. loss: 84.104997\n",
      "Total training time: 136.08 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 466.29, NNZs: 8192, Bias: 49.324295, T: 5895000, Avg. loss: 68.092730\n",
      "Total training time: 136.22 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 513.26, NNZs: 8192, Bias: -5.405535, T: 5985000, Avg. loss: 55.634179\n",
      "Total training time: 136.21 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 471.55, NNZs: 8192, Bias: -19.469480, T: 5760000, Avg. loss: 123.286946\n",
      "Total training time: 136.25 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 491.08, NNZs: 8192, Bias: -61.549841, T: 5805000, Avg. loss: 111.240713\n",
      "Total training time: 136.41 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 486.88, NNZs: 8192, Bias: -58.735728, T: 5850000, Avg. loss: 100.415075\n",
      "Total training time: 136.43 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 525.78, NNZs: 8192, Bias: -52.210526, T: 5940000, Avg. loss: 89.149125\n",
      "Total training time: 136.58 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 551.35, NNZs: 8192, Bias: -33.717908, T: 5940000, Avg. loss: 73.257821\n",
      "Total training time: 136.66 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 528.96, NNZs: 8192, Bias: -54.112305, T: 5940000, Avg. loss: 84.181864\n",
      "Total training time: 136.91 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 468.04, NNZs: 8192, Bias: -8.548794, T: 5760000, Avg. loss: 104.927796\n",
      "Total training time: 136.95 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 595.00, NNZs: 8192, Bias: -50.585689, T: 5940000, Avg. loss: 83.683916\n",
      "Total training time: 137.15 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 464.84, NNZs: 8192, Bias: 49.326005, T: 5940000, Avg. loss: 66.339205\n",
      "Total training time: 137.29 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 511.36, NNZs: 8192, Bias: -5.403870, T: 6030000, Avg. loss: 55.021637\n",
      "Total training time: 137.25 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 470.31, NNZs: 8192, Bias: -19.478116, T: 5805000, Avg. loss: 123.865901\n",
      "Total training time: 137.34 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 489.61, NNZs: 8192, Bias: -61.553273, T: 5850000, Avg. loss: 109.860367\n",
      "Total training time: 137.49 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 485.23, NNZs: 8192, Bias: -58.745942, T: 5895000, Avg. loss: 99.710726\n",
      "Total training time: 137.51 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 524.09, NNZs: 8192, Bias: -52.208871, T: 5985000, Avg. loss: 87.594611\n",
      "Total training time: 137.64 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 549.38, NNZs: 8192, Bias: -33.727982, T: 5985000, Avg. loss: 71.582690\n",
      "Total training time: 137.71 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 527.28, NNZs: 8192, Bias: -54.120690, T: 5985000, Avg. loss: 86.404933\n",
      "Total training time: 137.96 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 466.47, NNZs: 8192, Bias: -8.550545, T: 5805000, Avg. loss: 104.864329\n",
      "Total training time: 138.02 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 592.84, NNZs: 8192, Bias: -50.594065, T: 5985000, Avg. loss: 81.473413\n",
      "Total training time: 138.21 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 509.50, NNZs: 8192, Bias: -5.405532, T: 6075000, Avg. loss: 55.057910\n",
      "Total training time: 138.31 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 463.27, NNZs: 8192, Bias: 49.339419, T: 5985000, Avg. loss: 66.254874\n",
      "Total training time: 138.36 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 468.93, NNZs: 8192, Bias: -19.474691, T: 5850000, Avg. loss: 122.426028\n",
      "Total training time: 138.41 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 483.60, NNZs: 8192, Bias: -58.745927, T: 5940000, Avg. loss: 97.303438\n",
      "Total training time: 138.57 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 488.11, NNZs: 8192, Bias: -61.558394, T: 5895000, Avg. loss: 106.483942\n",
      "Total training time: 138.55 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 522.12, NNZs: 8192, Bias: -52.210536, T: 6030000, Avg. loss: 88.171433\n",
      "Total training time: 138.70 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 547.40, NNZs: 8192, Bias: -33.737964, T: 6030000, Avg. loss: 72.120284\n",
      "Total training time: 138.78 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 525.53, NNZs: 8192, Bias: -54.125674, T: 6030000, Avg. loss: 84.017508\n",
      "Total training time: 139.05 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 465.00, NNZs: 8192, Bias: -8.552260, T: 5850000, Avg. loss: 104.508337\n",
      "Total training time: 139.17 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 590.71, NNZs: 8192, Bias: -50.597399, T: 6030000, Avg. loss: 80.166431\n",
      "Total training time: 139.32 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 507.62, NNZs: 8192, Bias: -5.402259, T: 6120000, Avg. loss: 56.041291\n",
      "Total training time: 139.40 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 461.58, NNZs: 8192, Bias: 49.339410, T: 6030000, Avg. loss: 64.708357\n",
      "Total training time: 139.47 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 467.60, NNZs: 8192, Bias: -19.476393, T: 5895000, Avg. loss: 120.637265\n",
      "Total training time: 139.54 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 481.89, NNZs: 8192, Bias: -58.759348, T: 5985000, Avg. loss: 97.240058\n",
      "Total training time: 139.68 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 486.78, NNZs: 8192, Bias: -61.568524, T: 5940000, Avg. loss: 108.135119\n",
      "Total training time: 139.68 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 520.51, NNZs: 8192, Bias: -52.212202, T: 6075000, Avg. loss: 88.356737\n",
      "Total training time: 139.78 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 545.60, NNZs: 8192, Bias: -33.746231, T: 6075000, Avg. loss: 72.791043\n",
      "Total training time: 139.86 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 523.75, NNZs: 8192, Bias: -54.127340, T: 6075000, Avg. loss: 86.867899\n",
      "Total training time: 140.12 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 463.62, NNZs: 8192, Bias: -8.552254, T: 5895000, Avg. loss: 103.380696\n",
      "Total training time: 140.27 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 588.50, NNZs: 8192, Bias: -50.604018, T: 6075000, Avg. loss: 80.801102\n",
      "Total training time: 140.39 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 505.78, NNZs: 8192, Bias: -5.405529, T: 6165000, Avg. loss: 53.826873\n",
      "Total training time: 140.44 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 459.96, NNZs: 8192, Bias: 49.339397, T: 6075000, Avg. loss: 64.624660\n",
      "Total training time: 140.53 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 466.20, NNZs: 8192, Bias: -19.479778, T: 5940000, Avg. loss: 120.192334\n",
      "Total training time: 140.63 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 480.35, NNZs: 8192, Bias: -58.765999, T: 6030000, Avg. loss: 97.105495\n",
      "Total training time: 140.76 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 485.51, NNZs: 8192, Bias: -61.568514, T: 5985000, Avg. loss: 106.168012\n",
      "Total training time: 140.77 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 518.63, NNZs: 8192, Bias: -52.213852, T: 6120000, Avg. loss: 86.360000\n",
      "Total training time: 140.84 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 543.78, NNZs: 8192, Bias: -33.757712, T: 6120000, Avg. loss: 71.253238\n",
      "Total training time: 140.94 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 522.19, NNZs: 8192, Bias: -54.138834, T: 6120000, Avg. loss: 82.646663\n",
      "Total training time: 141.20 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 462.08, NNZs: 8192, Bias: -8.550570, T: 5940000, Avg. loss: 103.461250\n",
      "Total training time: 141.37 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 586.37, NNZs: 8192, Bias: -50.610577, T: 6120000, Avg. loss: 79.260051\n",
      "Total training time: 141.46 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 503.98, NNZs: 8192, Bias: -5.403915, T: 6210000, Avg. loss: 52.985885\n",
      "Total training time: 141.51 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 458.50, NNZs: 8192, Bias: 49.342679, T: 6120000, Avg. loss: 64.498503\n",
      "Total training time: 141.61 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 464.79, NNZs: 8192, Bias: -19.478081, T: 5985000, Avg. loss: 119.435089\n",
      "Total training time: 141.73 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 478.77, NNZs: 8192, Bias: -58.772613, T: 6075000, Avg. loss: 96.526095\n",
      "Total training time: 141.83 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 484.13, NNZs: 8192, Bias: -61.571846, T: 6030000, Avg. loss: 106.359174\n",
      "Total training time: 141.87 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 516.82, NNZs: 8192, Bias: -52.218725, T: 6165000, Avg. loss: 83.433357\n",
      "Total training time: 141.90 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 541.90, NNZs: 8192, Bias: -33.767483, T: 6165000, Avg. loss: 70.762438\n",
      "Total training time: 141.99 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 520.49, NNZs: 8192, Bias: -54.148605, T: 6165000, Avg. loss: 80.990811\n",
      "Total training time: 142.26 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 460.69, NNZs: 8192, Bias: -8.547236, T: 5985000, Avg. loss: 101.031643\n",
      "Total training time: 142.47 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 584.22, NNZs: 8192, Bias: -50.612206, T: 6165000, Avg. loss: 77.000942\n",
      "Total training time: 142.52 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 502.15, NNZs: 8192, Bias: -5.407135, T: 6255000, Avg. loss: 52.578877\n",
      "Total training time: 142.56 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 456.93, NNZs: 8192, Bias: 49.345946, T: 6165000, Avg. loss: 64.226472\n",
      "Total training time: 142.66 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 463.49, NNZs: 8192, Bias: -19.481411, T: 6030000, Avg. loss: 117.199386\n",
      "Total training time: 142.82 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 477.10, NNZs: 8192, Bias: -58.784089, T: 6120000, Avg. loss: 96.942646\n",
      "Total training time: 142.91 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 482.75, NNZs: 8192, Bias: -61.570206, T: 6075000, Avg. loss: 106.886565\n",
      "Total training time: 142.93 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 515.13, NNZs: 8192, Bias: -52.217100, T: 6210000, Avg. loss: 83.387415\n",
      "Total training time: 142.95 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 539.95, NNZs: 8192, Bias: -33.777181, T: 6210000, Avg. loss: 69.758719\n",
      "Total training time: 143.03 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 518.98, NNZs: 8192, Bias: -54.148603, T: 6210000, Avg. loss: 81.476173\n",
      "Total training time: 143.29 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 459.14, NNZs: 8192, Bias: -8.557230, T: 6030000, Avg. loss: 100.088112\n",
      "Total training time: 143.52 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 582.10, NNZs: 8192, Bias: -50.617057, T: 6210000, Avg. loss: 79.619043\n",
      "Total training time: 143.56 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 500.36, NNZs: 8192, Bias: -5.402355, T: 6300000, Avg. loss: 51.755072\n",
      "Total training time: 143.58 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 455.38, NNZs: 8192, Bias: 49.349186, T: 6210000, Avg. loss: 63.537692\n",
      "Total training time: 143.71 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 462.01, NNZs: 8192, Bias: -19.481399, T: 6075000, Avg. loss: 116.890927\n",
      "Total training time: 143.90 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 475.53, NNZs: 8192, Bias: -58.792223, T: 6165000, Avg. loss: 93.864474\n",
      "Total training time: 143.96 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 513.45, NNZs: 8192, Bias: -52.217099, T: 6255000, Avg. loss: 83.586327\n",
      "Total training time: 144.00 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 481.40, NNZs: 8192, Bias: -61.575144, T: 6120000, Avg. loss: 104.019525\n",
      "Total training time: 144.01 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 538.15, NNZs: 8192, Bias: -33.783600, T: 6255000, Avg. loss: 67.678638\n",
      "Total training time: 144.09 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 517.24, NNZs: 8192, Bias: -54.156622, T: 6255000, Avg. loss: 81.573144\n",
      "Total training time: 144.42 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 580.05, NNZs: 8192, Bias: -50.618665, T: 6255000, Avg. loss: 75.733043\n",
      "Total training time: 144.68 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 457.70, NNZs: 8192, Bias: -8.553942, T: 6075000, Avg. loss: 100.272058\n",
      "Total training time: 144.70 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 498.74, NNZs: 8192, Bias: -5.403944, T: 6345000, Avg. loss: 51.559713\n",
      "Total training time: 144.69 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 453.75, NNZs: 8192, Bias: 49.354003, T: 6255000, Avg. loss: 63.003544\n",
      "Total training time: 144.84 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 460.60, NNZs: 8192, Bias: -19.483040, T: 6120000, Avg. loss: 114.718814\n",
      "Total training time: 145.12 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 473.81, NNZs: 8192, Bias: -58.797072, T: 6210000, Avg. loss: 94.837322\n",
      "Total training time: 145.20 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 479.99, NNZs: 8192, Bias: -61.578396, T: 6165000, Avg. loss: 102.938622\n",
      "Total training time: 145.23 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 511.72, NNZs: 8192, Bias: -52.217099, T: 6300000, Avg. loss: 84.634908\n",
      "Total training time: 145.24 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 536.28, NNZs: 8192, Bias: -33.789985, T: 6300000, Avg. loss: 67.626327\n",
      "Total training time: 145.26 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 515.60, NNZs: 8192, Bias: -54.161402, T: 6300000, Avg. loss: 80.885696\n",
      "Total training time: 145.58 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 577.99, NNZs: 8192, Bias: -50.625038, T: 6300000, Avg. loss: 77.134430\n",
      "Total training time: 145.81 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 497.05, NNZs: 8192, Bias: -5.407076, T: 6390000, Avg. loss: 52.284994\n",
      "Total training time: 145.83 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 456.47, NNZs: 8192, Bias: -8.557214, T: 6120000, Avg. loss: 97.561685\n",
      "Total training time: 145.88 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 452.42, NNZs: 8192, Bias: 49.352417, T: 6300000, Avg. loss: 62.048306\n",
      "Total training time: 145.96 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 459.07, NNZs: 8192, Bias: -19.481420, T: 6165000, Avg. loss: 116.678406\n",
      "Total training time: 146.27 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 472.30, NNZs: 8192, Bias: -58.806705, T: 6255000, Avg. loss: 94.332398\n",
      "Total training time: 146.32 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 534.56, NNZs: 8192, Bias: -33.797898, T: 6345000, Avg. loss: 69.133714\n",
      "Total training time: 146.35 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 509.96, NNZs: 8192, Bias: -52.213940, T: 6345000, Avg. loss: 82.388272\n",
      "Total training time: 146.36 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 478.47, NNZs: 8192, Bias: -61.578390, T: 6210000, Avg. loss: 103.844813\n",
      "Total training time: 146.37 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 513.95, NNZs: 8192, Bias: -54.170896, T: 6345000, Avg. loss: 79.075486\n",
      "Total training time: 146.70 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 495.41, NNZs: 8192, Bias: -5.405512, T: 6435000, Avg. loss: 51.872115\n",
      "Total training time: 146.94 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 575.87, NNZs: 8192, Bias: -50.632932, T: 6345000, Avg. loss: 75.215015\n",
      "Total training time: 146.93 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 455.09, NNZs: 8192, Bias: -8.555593, T: 6165000, Avg. loss: 99.777828\n",
      "Total training time: 147.05 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 450.98, NNZs: 8192, Bias: 49.363505, T: 6345000, Avg. loss: 60.506071\n",
      "Total training time: 147.09 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 470.84, NNZs: 8192, Bias: -58.817859, T: 6300000, Avg. loss: 92.323723\n",
      "Total training time: 147.48 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 457.67, NNZs: 8192, Bias: -19.483040, T: 6210000, Avg. loss: 115.289218\n",
      "Total training time: 147.45 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 532.70, NNZs: 8192, Bias: -33.807327, T: 6390000, Avg. loss: 66.268182\n",
      "Total training time: 147.50 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 508.16, NNZs: 8192, Bias: -52.218661, T: 6390000, Avg. loss: 81.307076\n",
      "Total training time: 147.52 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 477.23, NNZs: 8192, Bias: -61.584795, T: 6255000, Avg. loss: 100.809543\n",
      "Total training time: 147.55 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 512.30, NNZs: 8192, Bias: -54.177159, T: 6390000, Avg. loss: 79.600150\n",
      "Total training time: 147.83 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 493.68, NNZs: 8192, Bias: -5.405511, T: 6480000, Avg. loss: 51.212450\n",
      "Total training time: 148.07 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 573.91, NNZs: 8192, Bias: -50.637642, T: 6390000, Avg. loss: 76.816962\n",
      "Total training time: 148.10 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 449.38, NNZs: 8192, Bias: 49.368199, T: 6390000, Avg. loss: 61.449258\n",
      "Total training time: 148.26 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 453.68, NNZs: 8192, Bias: -8.557208, T: 6210000, Avg. loss: 97.547753\n",
      "Total training time: 148.25 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 531.14, NNZs: 8192, Bias: -33.816684, T: 6435000, Avg. loss: 66.628629\n",
      "Total training time: 148.66 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 469.43, NNZs: 8192, Bias: -58.828929, T: 6345000, Avg. loss: 92.726996\n",
      "Total training time: 148.67 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 456.32, NNZs: 8192, Bias: -19.486238, T: 6255000, Avg. loss: 114.434682\n",
      "Total training time: 148.65 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 506.28, NNZs: 8192, Bias: -52.218648, T: 6435000, Avg. loss: 81.242851\n",
      "Total training time: 148.68 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 475.93, NNZs: 8192, Bias: -61.587983, T: 6300000, Avg. loss: 101.399724\n",
      "Total training time: 148.74 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 510.72, NNZs: 8192, Bias: -54.181821, T: 6435000, Avg. loss: 79.377753\n",
      "Total training time: 148.96 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 491.93, NNZs: 8192, Bias: -5.403972, T: 6525000, Avg. loss: 50.671367\n",
      "Total training time: 149.18 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 571.93, NNZs: 8192, Bias: -50.642309, T: 6435000, Avg. loss: 74.121032\n",
      "Total training time: 149.21 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 447.92, NNZs: 8192, Bias: 49.368215, T: 6435000, Avg. loss: 60.994164\n",
      "Total training time: 149.38 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 452.33, NNZs: 8192, Bias: -8.557220, T: 6255000, Avg. loss: 97.766425\n",
      "Total training time: 149.37 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 529.41, NNZs: 8192, Bias: -33.822883, T: 6480000, Avg. loss: 65.184890\n",
      "Total training time: 149.71 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 467.93, NNZs: 8192, Bias: -58.836769, T: 6390000, Avg. loss: 92.065587\n",
      "Total training time: 149.75 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 455.02, NNZs: 8192, Bias: -19.486239, T: 6300000, Avg. loss: 113.204870\n",
      "Total training time: 149.72 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 504.59, NNZs: 8192, Bias: -52.221743, T: 6480000, Avg. loss: 80.722449\n",
      "Total training time: 149.74 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 474.58, NNZs: 8192, Bias: -61.594310, T: 6345000, Avg. loss: 99.974675\n",
      "Total training time: 149.81 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 509.26, NNZs: 8192, Bias: -54.186474, T: 6480000, Avg. loss: 77.625611\n",
      "Total training time: 150.00 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 490.09, NNZs: 8192, Bias: -5.408546, T: 6570000, Avg. loss: 50.061266\n",
      "Total training time: 150.21 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 569.89, NNZs: 8192, Bias: -50.646950, T: 6480000, Avg. loss: 74.056569\n",
      "Total training time: 150.26 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 446.58, NNZs: 8192, Bias: 49.372847, T: 6480000, Avg. loss: 58.988222\n",
      "Total training time: 150.44 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 450.91, NNZs: 8192, Bias: -8.555632, T: 6300000, Avg. loss: 96.479679\n",
      "Total training time: 150.49 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 527.74, NNZs: 8192, Bias: -33.830568, T: 6525000, Avg. loss: 66.562434\n",
      "Total training time: 150.83 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 466.43, NNZs: 8192, Bias: -58.846111, T: 6435000, Avg. loss: 90.971819\n",
      "Total training time: 150.89 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 502.93, NNZs: 8192, Bias: -52.221732, T: 6525000, Avg. loss: 81.082070\n",
      "Total training time: 150.87 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 453.66, NNZs: 8192, Bias: -19.487824, T: 6345000, Avg. loss: 110.952844\n",
      "Total training time: 150.88 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 473.29, NNZs: 8192, Bias: -61.597440, T: 6390000, Avg. loss: 100.520173\n",
      "Total training time: 150.99 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 507.74, NNZs: 8192, Bias: -54.192629, T: 6525000, Avg. loss: 77.212028\n",
      "Total training time: 151.17 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 488.52, NNZs: 8192, Bias: -5.402488, T: 6615000, Avg. loss: 49.885073\n",
      "Total training time: 151.40 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 567.90, NNZs: 8192, Bias: -50.654629, T: 6525000, Avg. loss: 74.447812\n",
      "Total training time: 151.45 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 445.15, NNZs: 8192, Bias: 49.374383, T: 6525000, Avg. loss: 58.904684\n",
      "Total training time: 151.61 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 449.44, NNZs: 8192, Bias: -8.554057, T: 6345000, Avg. loss: 95.607323\n",
      "Total training time: 151.71 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 526.09, NNZs: 8192, Bias: -33.836674, T: 6570000, Avg. loss: 64.056199\n",
      "Total training time: 152.00 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 501.08, NNZs: 8192, Bias: -52.218683, T: 6570000, Avg. loss: 81.314709\n",
      "Total training time: 152.03 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 464.96, NNZs: 8192, Bias: -58.852297, T: 6480000, Avg. loss: 89.503443\n",
      "Total training time: 152.06 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 452.36, NNZs: 8192, Bias: -19.487822, T: 6390000, Avg. loss: 110.134286\n",
      "Total training time: 152.04 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 471.97, NNZs: 8192, Bias: -61.598990, T: 6435000, Avg. loss: 98.391926\n",
      "Total training time: 152.16 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 506.26, NNZs: 8192, Bias: -54.198729, T: 6570000, Avg. loss: 78.182249\n",
      "Total training time: 152.33 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 486.94, NNZs: 8192, Bias: -5.407008, T: 6660000, Avg. loss: 49.037563\n",
      "Total training time: 152.52 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 565.91, NNZs: 8192, Bias: -50.654622, T: 6570000, Avg. loss: 73.422496\n",
      "Total training time: 152.56 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 443.77, NNZs: 8192, Bias: 49.374377, T: 6570000, Avg. loss: 58.662998\n",
      "Total training time: 152.70 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 448.13, NNZs: 8192, Bias: -8.554048, T: 6390000, Avg. loss: 95.447287\n",
      "Total training time: 152.83 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 524.48, NNZs: 8192, Bias: -33.844262, T: 6615000, Avg. loss: 65.256653\n",
      "Total training time: 153.05 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 499.49, NNZs: 8192, Bias: -52.218687, T: 6615000, Avg. loss: 79.847562\n",
      "Total training time: 153.10 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 463.52, NNZs: 8192, Bias: -58.856919, T: 6525000, Avg. loss: 89.082331\n",
      "Total training time: 153.14 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 451.17, NNZs: 8192, Bias: -19.487804, T: 6435000, Avg. loss: 109.843171\n",
      "Total training time: 153.14 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 470.59, NNZs: 8192, Bias: -61.603621, T: 6480000, Avg. loss: 98.941852\n",
      "Total training time: 153.24 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 504.70, NNZs: 8192, Bias: -54.204787, T: 6615000, Avg. loss: 76.635037\n",
      "Total training time: 153.40 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 485.47, NNZs: 8192, Bias: -5.407009, T: 6705000, Avg. loss: 48.657238\n",
      "Total training time: 153.58 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 563.91, NNZs: 8192, Bias: -50.660678, T: 6615000, Avg. loss: 72.934962\n",
      "Total training time: 153.64 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 442.39, NNZs: 8192, Bias: 49.377415, T: 6615000, Avg. loss: 58.904985\n",
      "Total training time: 153.76 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 446.84, NNZs: 8192, Bias: -8.554058, T: 6435000, Avg. loss: 94.683432\n",
      "Total training time: 153.93 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 522.56, NNZs: 8192, Bias: -33.851799, T: 6660000, Avg. loss: 64.119625\n",
      "Total training time: 154.13 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 497.93, NNZs: 8192, Bias: -52.217192, T: 6660000, Avg. loss: 78.839151\n",
      "Total training time: 154.19 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 462.10, NNZs: 8192, Bias: -58.867614, T: 6570000, Avg. loss: 88.364725\n",
      "Total training time: 154.26 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 449.87, NNZs: 8192, Bias: -19.483158, T: 6480000, Avg. loss: 108.133328\n",
      "Total training time: 154.26 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 469.15, NNZs: 8192, Bias: -61.605154, T: 6525000, Avg. loss: 97.945825\n",
      "Total training time: 154.37 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 503.11, NNZs: 8192, Bias: -54.204786, T: 6660000, Avg. loss: 76.335603\n",
      "Total training time: 154.47 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 483.83, NNZs: 8192, Bias: -5.408486, T: 6750000, Avg. loss: 49.417582\n",
      "Total training time: 154.66 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 561.89, NNZs: 8192, Bias: -50.660666, T: 6660000, Avg. loss: 72.752612\n",
      "Total training time: 154.72 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 441.00, NNZs: 8192, Bias: 49.383434, T: 6660000, Avg. loss: 58.775805\n",
      "Total training time: 154.84 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 445.63, NNZs: 8192, Bias: -8.557153, T: 6480000, Avg. loss: 93.153386\n",
      "Total training time: 155.04 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 520.91, NNZs: 8192, Bias: -33.859280, T: 6705000, Avg. loss: 63.357696\n",
      "Total training time: 155.20 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 496.19, NNZs: 8192, Bias: -52.215705, T: 6705000, Avg. loss: 77.396283\n",
      "Total training time: 155.25 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 460.64, NNZs: 8192, Bias: -58.872174, T: 6615000, Avg. loss: 88.024418\n",
      "Total training time: 155.33 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 448.72, NNZs: 8192, Bias: -19.492407, T: 6525000, Avg. loss: 108.095616\n",
      "Total training time: 155.35 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 468.06, NNZs: 8192, Bias: -61.609736, T: 6570000, Avg. loss: 94.329862\n",
      "Total training time: 155.45 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 501.84, NNZs: 8192, Bias: -54.212266, T: 6705000, Avg. loss: 74.482201\n",
      "Total training time: 155.53 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 482.29, NNZs: 8192, Bias: -5.405532, T: 6795000, Avg. loss: 48.325853\n",
      "Total training time: 155.71 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 559.90, NNZs: 8192, Bias: -50.663670, T: 6705000, Avg. loss: 71.547917\n",
      "Total training time: 155.77 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 439.54, NNZs: 8192, Bias: 49.383429, T: 6705000, Avg. loss: 57.929275\n",
      "Total training time: 155.91 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 444.36, NNZs: 8192, Bias: -8.554065, T: 6525000, Avg. loss: 91.913581\n",
      "Total training time: 156.14 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 519.28, NNZs: 8192, Bias: -33.869689, T: 6750000, Avg. loss: 63.237112\n",
      "Total training time: 156.28 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 494.60, NNZs: 8192, Bias: -52.215711, T: 6750000, Avg. loss: 76.314124\n",
      "Total training time: 156.35 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 459.31, NNZs: 8192, Bias: -58.878223, T: 6660000, Avg. loss: 86.051263\n",
      "Total training time: 156.43 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 447.44, NNZs: 8192, Bias: -19.493930, T: 6570000, Avg. loss: 107.238620\n",
      "Total training time: 156.47 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 466.76, NNZs: 8192, Bias: -61.617319, T: 6615000, Avg. loss: 95.823985\n",
      "Total training time: 156.56 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 500.36, NNZs: 8192, Bias: -54.218209, T: 6750000, Avg. loss: 74.654188\n",
      "Total training time: 156.64 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 480.74, NNZs: 8192, Bias: -5.406998, T: 6840000, Avg. loss: 46.963283\n",
      "Total training time: 156.78 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 558.07, NNZs: 8192, Bias: -50.669621, T: 6750000, Avg. loss: 69.718678\n",
      "Total training time: 156.84 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 438.21, NNZs: 8192, Bias: 49.386402, T: 6750000, Avg. loss: 57.132417\n",
      "Total training time: 156.98 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 443.23, NNZs: 8192, Bias: -8.555589, T: 6570000, Avg. loss: 92.769084\n",
      "Total training time: 157.25 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 517.71, NNZs: 8192, Bias: -33.875599, T: 6795000, Avg. loss: 62.542668\n",
      "Total training time: 157.33 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 493.08, NNZs: 8192, Bias: -52.218659, T: 6795000, Avg. loss: 76.341463\n",
      "Total training time: 157.41 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 457.98, NNZs: 8192, Bias: -58.888692, T: 6705000, Avg. loss: 86.980226\n",
      "Total training time: 157.51 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 446.42, NNZs: 8192, Bias: -19.490905, T: 6615000, Avg. loss: 107.722533\n",
      "Total training time: 157.56 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 465.59, NNZs: 8192, Bias: -61.620324, T: 6660000, Avg. loss: 95.252773\n",
      "Total training time: 157.63 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 498.90, NNZs: 8192, Bias: -54.225595, T: 6795000, Avg. loss: 75.892283\n",
      "Total training time: 157.68 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 479.14, NNZs: 8192, Bias: -5.402622, T: 6885000, Avg. loss: 48.258008\n",
      "Total training time: 157.81 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 556.16, NNZs: 8192, Bias: -50.675532, T: 6795000, Avg. loss: 70.337380\n",
      "Total training time: 157.87 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 436.84, NNZs: 8192, Bias: 49.389350, T: 6795000, Avg. loss: 57.391551\n",
      "Total training time: 158.00 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 442.05, NNZs: 8192, Bias: -8.555588, T: 6615000, Avg. loss: 90.696911\n",
      "Total training time: 158.35 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 516.21, NNZs: 8192, Bias: -33.882937, T: 6840000, Avg. loss: 60.260437\n",
      "Total training time: 158.39 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 491.77, NNZs: 8192, Bias: -52.220128, T: 6840000, Avg. loss: 74.135272\n",
      "Total training time: 158.45 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 456.66, NNZs: 8192, Bias: -58.896120, T: 6750000, Avg. loss: 86.148951\n",
      "Total training time: 158.57 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 445.21, NNZs: 8192, Bias: -19.490917, T: 6660000, Avg. loss: 105.685257\n",
      "Total training time: 158.63 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 464.13, NNZs: 8192, Bias: -61.621818, T: 6705000, Avg. loss: 95.333768\n",
      "Total training time: 158.70 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 497.36, NNZs: 8192, Bias: -54.235852, T: 6840000, Avg. loss: 72.025787\n",
      "Total training time: 158.74 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 477.62, NNZs: 8192, Bias: -5.406961, T: 6930000, Avg. loss: 47.758042\n",
      "Total training time: 158.85 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 554.23, NNZs: 8192, Bias: -50.676987, T: 6840000, Avg. loss: 70.869135\n",
      "Total training time: 158.93 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 435.40, NNZs: 8192, Bias: 49.393743, T: 6840000, Avg. loss: 56.589443\n",
      "Total training time: 159.08 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 514.76, NNZs: 8192, Bias: -33.888769, T: 6885000, Avg. loss: 61.865620\n",
      "Total training time: 159.44 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 440.73, NNZs: 8192, Bias: -8.558608, T: 6660000, Avg. loss: 89.412643\n",
      "Total training time: 159.44 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 490.27, NNZs: 8192, Bias: -52.217214, T: 6885000, Avg. loss: 75.152902\n",
      "Total training time: 159.48 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 455.32, NNZs: 8192, Bias: -58.899076, T: 6795000, Avg. loss: 85.672556\n",
      "Total training time: 159.63 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 444.03, NNZs: 8192, Bias: -19.495397, T: 6705000, Avg. loss: 105.229349\n",
      "Total training time: 159.70 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 462.95, NNZs: 8192, Bias: -61.623309, T: 6750000, Avg. loss: 94.955261\n",
      "Total training time: 159.76 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 495.88, NNZs: 8192, Bias: -54.240221, T: 6885000, Avg. loss: 73.736691\n",
      "Total training time: 159.77 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 476.07, NNZs: 8192, Bias: -5.408405, T: 6975000, Avg. loss: 46.632326\n",
      "Total training time: 159.89 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 552.35, NNZs: 8192, Bias: -50.681360, T: 6885000, Avg. loss: 69.817419\n",
      "Total training time: 159.99 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 434.12, NNZs: 8192, Bias: 49.392268, T: 6885000, Avg. loss: 56.262179\n",
      "Total training time: 160.13 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 513.25, NNZs: 8192, Bias: -33.893113, T: 6930000, Avg. loss: 60.852120\n",
      "Total training time: 160.48 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 488.89, NNZs: 8192, Bias: -52.214333, T: 6930000, Avg. loss: 73.610797\n",
      "Total training time: 160.52 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 439.65, NNZs: 8192, Bias: -8.560098, T: 6705000, Avg. loss: 88.460560\n",
      "Total training time: 160.51 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 453.85, NNZs: 8192, Bias: -58.909339, T: 6840000, Avg. loss: 85.051368\n",
      "Total training time: 160.69 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 442.90, NNZs: 8192, Bias: -19.495390, T: 6750000, Avg. loss: 103.101930\n",
      "Total training time: 160.76 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 461.81, NNZs: 8192, Bias: -61.623323, T: 6795000, Avg. loss: 92.631326\n",
      "Total training time: 160.81 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 494.61, NNZs: 8192, Bias: -54.246008, T: 6930000, Avg. loss: 71.439092\n",
      "Total training time: 160.79 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 474.52, NNZs: 8192, Bias: -5.411270, T: 7020000, Avg. loss: 47.330188\n",
      "Total training time: 160.90 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 550.39, NNZs: 8192, Bias: -50.687152, T: 6930000, Avg. loss: 69.137011\n",
      "Total training time: 161.02 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 432.88, NNZs: 8192, Bias: 49.395167, T: 6930000, Avg. loss: 55.778740\n",
      "Total training time: 161.16 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 511.63, NNZs: 8192, Bias: -33.900312, T: 6975000, Avg. loss: 60.742834\n",
      "Total training time: 161.51 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 487.28, NNZs: 8192, Bias: -52.215778, T: 6975000, Avg. loss: 74.312603\n",
      "Total training time: 161.55 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 438.40, NNZs: 8192, Bias: -8.561589, T: 6750000, Avg. loss: 90.578090\n",
      "Total training time: 161.58 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 452.64, NNZs: 8192, Bias: -58.918070, T: 6885000, Avg. loss: 83.073853\n",
      "Total training time: 161.73 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 441.61, NNZs: 8192, Bias: -19.493911, T: 6795000, Avg. loss: 103.864266\n",
      "Total training time: 161.82 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 493.21, NNZs: 8192, Bias: -54.248880, T: 6975000, Avg. loss: 72.088402\n",
      "Total training time: 161.82 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 460.56, NNZs: 8192, Bias: -61.629196, T: 6840000, Avg. loss: 91.047687\n",
      "Total training time: 161.86 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 473.00, NNZs: 8192, Bias: -5.409848, T: 7065000, Avg. loss: 46.613483\n",
      "Total training time: 161.91 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 548.69, NNZs: 8192, Bias: -50.690031, T: 6975000, Avg. loss: 69.243036\n",
      "Total training time: 162.05 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 431.68, NNZs: 8192, Bias: 49.400911, T: 6975000, Avg. loss: 55.170901\n",
      "Total training time: 162.19 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 510.09, NNZs: 8192, Bias: -33.911737, T: 7020000, Avg. loss: 60.364221\n",
      "Total training time: 162.52 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 485.87, NNZs: 8192, Bias: -52.217205, T: 7020000, Avg. loss: 74.671311\n",
      "Total training time: 162.56 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 437.19, NNZs: 8192, Bias: -8.561583, T: 6795000, Avg. loss: 87.835814\n",
      "Total training time: 162.64 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 451.44, NNZs: 8192, Bias: -58.923855, T: 6930000, Avg. loss: 83.433862\n",
      "Total training time: 162.77 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 440.56, NNZs: 8192, Bias: -19.496837, T: 6840000, Avg. loss: 103.300083\n",
      "Total training time: 162.85 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 491.81, NNZs: 8192, Bias: -54.251739, T: 7020000, Avg. loss: 70.656344\n",
      "Total training time: 162.84 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 459.35, NNZs: 8192, Bias: -61.629191, T: 6885000, Avg. loss: 90.638929\n",
      "Total training time: 162.90 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 471.50, NNZs: 8192, Bias: -5.409848, T: 7110000, Avg. loss: 45.621009\n",
      "Total training time: 162.93 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 546.79, NNZs: 8192, Bias: -50.692880, T: 7020000, Avg. loss: 66.389891\n",
      "Total training time: 163.09 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 430.44, NNZs: 8192, Bias: 49.400910, T: 7020000, Avg. loss: 54.336076\n",
      "Total training time: 163.24 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 508.59, NNZs: 8192, Bias: -33.917398, T: 7065000, Avg. loss: 61.027486\n",
      "Total training time: 163.59 seconds.\n",
      "Convergence after 157 epochs took 163.60 seconds\n",
      "Norm: 484.67, NNZs: 8192, Bias: -52.215785, T: 7065000, Avg. loss: 73.921580\n",
      "Total training time: 163.65 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 435.82, NNZs: 8192, Bias: -8.563052, T: 6840000, Avg. loss: 87.939928\n",
      "Total training time: 163.74 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 450.21, NNZs: 8192, Bias: -58.932482, T: 6975000, Avg. loss: 82.821790\n",
      "Total training time: 163.85 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 490.61, NNZs: 8192, Bias: -54.257419, T: 7065000, Avg. loss: 69.454277\n",
      "Total training time: 163.88 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 439.36, NNZs: 8192, Bias: -19.492468, T: 6885000, Avg. loss: 102.676015\n",
      "Total training time: 163.93 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 458.20, NNZs: 8192, Bias: -61.637889, T: 6930000, Avg. loss: 90.397831\n",
      "Total training time: 163.98 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 470.12, NNZs: 8192, Bias: -5.407046, T: 7155000, Avg. loss: 45.327684\n",
      "Total training time: 163.97 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 545.03, NNZs: 8192, Bias: -50.694306, T: 7065000, Avg. loss: 67.668386\n",
      "Total training time: 164.13 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 429.18, NNZs: 8192, Bias: 49.408011, T: 7065000, Avg. loss: 55.238855\n",
      "Total training time: 164.26 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 483.27, NNZs: 8192, Bias: -52.214367, T: 7110000, Avg. loss: 72.781814\n",
      "Total training time: 164.59 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 434.76, NNZs: 8192, Bias: -8.561598, T: 6885000, Avg. loss: 85.840426\n",
      "Total training time: 164.70 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 448.86, NNZs: 8192, Bias: -58.939624, T: 7020000, Avg. loss: 83.705503\n",
      "Total training time: 164.79 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 489.24, NNZs: 8192, Bias: -54.263069, T: 7110000, Avg. loss: 69.885420\n",
      "Total training time: 164.79 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 438.12, NNZs: 8192, Bias: -19.495364, T: 6930000, Avg. loss: 100.665626\n",
      "Total training time: 164.86 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 468.68, NNZs: 8192, Bias: -5.408452, T: 7200000, Avg. loss: 45.321944\n",
      "Total training time: 164.90 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 457.01, NNZs: 8192, Bias: -61.637889, T: 6975000, Avg. loss: 90.929351\n",
      "Total training time: 164.92 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 543.35, NNZs: 8192, Bias: -50.695728, T: 7110000, Avg. loss: 65.219910\n",
      "Total training time: 165.03 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 427.95, NNZs: 8192, Bias: 49.409422, T: 7110000, Avg. loss: 54.669143\n",
      "Total training time: 165.16 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 481.93, NNZs: 8192, Bias: -52.214360, T: 7155000, Avg. loss: 71.867269\n",
      "Total training time: 165.52 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 433.53, NNZs: 8192, Bias: -8.560159, T: 6930000, Avg. loss: 85.931708\n",
      "Total training time: 165.62 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 447.69, NNZs: 8192, Bias: -58.946718, T: 7065000, Avg. loss: 80.567072\n",
      "Total training time: 165.71 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 488.05, NNZs: 8192, Bias: -54.267280, T: 7155000, Avg. loss: 68.698798\n",
      "Total training time: 165.69 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 467.33, NNZs: 8192, Bias: -5.409846, T: 7245000, Avg. loss: 45.232548\n",
      "Total training time: 165.78 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 436.90, NNZs: 8192, Bias: -19.501110, T: 6975000, Avg. loss: 100.101625\n",
      "Total training time: 165.79 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 455.74, NNZs: 8192, Bias: -61.643611, T: 7020000, Avg. loss: 90.569767\n",
      "Total training time: 165.83 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 541.66, NNZs: 8192, Bias: -50.699935, T: 7155000, Avg. loss: 67.921801\n",
      "Total training time: 165.93 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 426.67, NNZs: 8192, Bias: 49.410820, T: 7155000, Avg. loss: 53.566730\n",
      "Total training time: 166.06 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 480.49, NNZs: 8192, Bias: -52.214360, T: 7200000, Avg. loss: 72.038432\n",
      "Total training time: 166.42 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 432.44, NNZs: 8192, Bias: -8.561596, T: 6975000, Avg. loss: 84.578046\n",
      "Total training time: 166.56 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 446.48, NNZs: 8192, Bias: -58.952357, T: 7110000, Avg. loss: 80.488668\n",
      "Total training time: 166.64 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 486.63, NNZs: 8192, Bias: -54.274241, T: 7200000, Avg. loss: 69.683839\n",
      "Total training time: 166.62 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 465.88, NNZs: 8192, Bias: -5.411227, T: 7290000, Avg. loss: 44.864132\n",
      "Total training time: 166.69 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 435.85, NNZs: 8192, Bias: -19.501109, T: 7020000, Avg. loss: 100.651753\n",
      "Total training time: 166.75 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 454.72, NNZs: 8192, Bias: -61.645027, T: 7065000, Avg. loss: 89.648324\n",
      "Total training time: 166.78 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 539.89, NNZs: 8192, Bias: -50.708282, T: 7200000, Avg. loss: 65.608481\n",
      "Total training time: 166.86 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 425.53, NNZs: 8192, Bias: 49.412207, T: 7200000, Avg. loss: 53.085423\n",
      "Total training time: 166.99 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 479.20, NNZs: 8192, Bias: -52.219899, T: 7245000, Avg. loss: 70.465618\n",
      "Total training time: 167.33 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 431.23, NNZs: 8192, Bias: -8.565884, T: 7020000, Avg. loss: 85.513542\n",
      "Total training time: 167.49 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 445.23, NNZs: 8192, Bias: -58.960762, T: 7155000, Avg. loss: 80.652757\n",
      "Total training time: 167.56 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 485.15, NNZs: 8192, Bias: -54.281159, T: 7245000, Avg. loss: 68.647873\n",
      "Total training time: 167.52 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 464.43, NNZs: 8192, Bias: -5.416695, T: 7335000, Avg. loss: 44.274098\n",
      "Total training time: 167.57 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 434.69, NNZs: 8192, Bias: -19.502519, T: 7065000, Avg. loss: 99.355965\n",
      "Total training time: 167.69 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 453.53, NNZs: 8192, Bias: -61.647848, T: 7110000, Avg. loss: 90.170906\n",
      "Total training time: 167.72 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 538.22, NNZs: 8192, Bias: -50.705515, T: 7245000, Avg. loss: 66.124308\n",
      "Total training time: 167.77 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 424.35, NNZs: 8192, Bias: 49.414958, T: 7245000, Avg. loss: 53.085036\n",
      "Total training time: 167.88 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 477.86, NNZs: 8192, Bias: -52.215758, T: 7290000, Avg. loss: 70.960896\n",
      "Total training time: 168.22 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 430.08, NNZs: 8192, Bias: -8.564461, T: 7065000, Avg. loss: 84.023471\n",
      "Total training time: 168.40 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 483.98, NNZs: 8192, Bias: -54.285278, T: 7290000, Avg. loss: 67.869536\n",
      "Total training time: 168.40 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 443.97, NNZs: 8192, Bias: -58.967725, T: 7200000, Avg. loss: 79.066053\n",
      "Total training time: 168.46 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 463.06, NNZs: 8192, Bias: -5.416687, T: 7380000, Avg. loss: 43.680737\n",
      "Total training time: 168.45 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 433.60, NNZs: 8192, Bias: -19.501103, T: 7110000, Avg. loss: 99.315336\n",
      "Total training time: 168.60 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 452.38, NNZs: 8192, Bias: -61.650658, T: 7155000, Avg. loss: 87.164103\n",
      "Total training time: 168.66 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 536.53, NNZs: 8192, Bias: -50.711030, T: 7290000, Avg. loss: 65.121792\n",
      "Total training time: 168.68 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 423.28, NNZs: 8192, Bias: 49.420449, T: 7290000, Avg. loss: 52.393564\n",
      "Total training time: 168.80 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 476.57, NNZs: 8192, Bias: -52.217124, T: 7335000, Avg. loss: 67.604148\n",
      "Total training time: 169.16 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 482.64, NNZs: 8192, Bias: -54.292110, T: 7335000, Avg. loss: 68.356701\n",
      "Total training time: 169.33 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 428.99, NNZs: 8192, Bias: -8.564469, T: 7110000, Avg. loss: 84.211661\n",
      "Total training time: 169.37 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 442.79, NNZs: 8192, Bias: -58.971880, T: 7245000, Avg. loss: 79.905291\n",
      "Total training time: 169.40 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 461.67, NNZs: 8192, Bias: -5.418041, T: 7425000, Avg. loss: 43.310238\n",
      "Total training time: 169.37 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 432.39, NNZs: 8192, Bias: -19.498302, T: 7155000, Avg. loss: 97.499742\n",
      "Total training time: 169.54 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 451.20, NNZs: 8192, Bias: -61.653438, T: 7200000, Avg. loss: 87.561970\n",
      "Total training time: 169.57 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 534.96, NNZs: 8192, Bias: -50.713762, T: 7335000, Avg. loss: 63.980750\n",
      "Total training time: 169.58 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 422.16, NNZs: 8192, Bias: 49.420443, T: 7335000, Avg. loss: 51.681161\n",
      "Total training time: 169.70 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 475.26, NNZs: 8192, Bias: -52.218469, T: 7380000, Avg. loss: 70.636406\n",
      "Total training time: 170.06 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 481.23, NNZs: 8192, Bias: -54.294834, T: 7380000, Avg. loss: 67.554795\n",
      "Total training time: 170.23 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 460.38, NNZs: 8192, Bias: -5.418038, T: 7470000, Avg. loss: 43.430204\n",
      "Total training time: 170.25 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 441.60, NNZs: 8192, Bias: -58.978760, T: 7290000, Avg. loss: 79.805125\n",
      "Total training time: 170.32 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 427.76, NNZs: 8192, Bias: -8.567265, T: 7155000, Avg. loss: 83.271797\n",
      "Total training time: 170.31 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 431.22, NNZs: 8192, Bias: -19.502477, T: 7200000, Avg. loss: 97.319470\n",
      "Total training time: 170.47 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 450.06, NNZs: 8192, Bias: -61.656210, T: 7245000, Avg. loss: 86.047004\n",
      "Total training time: 170.49 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 533.17, NNZs: 8192, Bias: -50.716477, T: 7380000, Avg. loss: 63.487104\n",
      "Total training time: 170.49 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 420.88, NNZs: 8192, Bias: 49.425880, T: 7380000, Avg. loss: 52.006788\n",
      "Total training time: 170.63 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 473.86, NNZs: 8192, Bias: -52.219817, T: 7425000, Avg. loss: 69.553184\n",
      "Total training time: 170.96 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 459.01, NNZs: 8192, Bias: -5.419375, T: 7515000, Avg. loss: 42.930890\n",
      "Total training time: 171.12 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 480.05, NNZs: 8192, Bias: -54.297539, T: 7425000, Avg. loss: 67.304089\n",
      "Total training time: 171.11 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 440.42, NNZs: 8192, Bias: -58.985599, T: 7335000, Avg. loss: 77.987821\n",
      "Total training time: 171.21 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 426.64, NNZs: 8192, Bias: -8.572832, T: 7200000, Avg. loss: 83.606756\n",
      "Total training time: 171.24 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 430.25, NNZs: 8192, Bias: -19.503864, T: 7245000, Avg. loss: 96.217922\n",
      "Total training time: 171.37 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 531.36, NNZs: 8192, Bias: -50.719179, T: 7425000, Avg. loss: 64.328209\n",
      "Total training time: 171.36 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 449.08, NNZs: 8192, Bias: -61.658957, T: 7290000, Avg. loss: 84.683413\n",
      "Total training time: 171.39 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 419.71, NNZs: 8192, Bias: 49.427223, T: 7425000, Avg. loss: 51.916914\n",
      "Total training time: 171.53 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 472.53, NNZs: 8192, Bias: -52.219816, T: 7470000, Avg. loss: 69.829600\n",
      "Total training time: 171.85 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 457.67, NNZs: 8192, Bias: -5.419370, T: 7560000, Avg. loss: 42.622692\n",
      "Total training time: 171.99 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 478.81, NNZs: 8192, Bias: -54.306945, T: 7470000, Avg. loss: 66.907530\n",
      "Total training time: 172.01 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 439.23, NNZs: 8192, Bias: -58.989683, T: 7380000, Avg. loss: 77.050069\n",
      "Total training time: 172.10 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 425.55, NNZs: 8192, Bias: -8.574213, T: 7245000, Avg. loss: 81.424450\n",
      "Total training time: 172.19 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 529.87, NNZs: 8192, Bias: -50.720521, T: 7470000, Avg. loss: 63.163564\n",
      "Total training time: 172.28 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 448.07, NNZs: 8192, Bias: -61.663062, T: 7335000, Avg. loss: 85.613107\n",
      "Total training time: 172.33 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 429.11, NNZs: 8192, Bias: -19.506619, T: 7290000, Avg. loss: 96.031885\n",
      "Total training time: 172.32 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 418.56, NNZs: 8192, Bias: 49.427229, T: 7470000, Avg. loss: 51.827850\n",
      "Total training time: 172.48 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 471.13, NNZs: 8192, Bias: -52.218483, T: 7515000, Avg. loss: 67.977570\n",
      "Total training time: 172.78 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 456.34, NNZs: 8192, Bias: -5.422006, T: 7605000, Avg. loss: 42.741528\n",
      "Total training time: 172.92 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 477.57, NNZs: 8192, Bias: -54.313619, T: 7515000, Avg. loss: 65.200905\n",
      "Total training time: 172.95 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 438.09, NNZs: 8192, Bias: -58.997782, T: 7425000, Avg. loss: 76.777697\n",
      "Total training time: 173.03 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 424.45, NNZs: 8192, Bias: -8.574208, T: 7290000, Avg. loss: 81.567443\n",
      "Total training time: 173.16 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 528.14, NNZs: 8192, Bias: -50.723193, T: 7515000, Avg. loss: 61.640035\n",
      "Total training time: 173.20 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 447.05, NNZs: 8192, Bias: -61.664420, T: 7380000, Avg. loss: 84.392887\n",
      "Total training time: 173.27 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 428.05, NNZs: 8192, Bias: -19.505249, T: 7335000, Avg. loss: 94.831984\n",
      "Total training time: 173.29 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 417.32, NNZs: 8192, Bias: 49.429903, T: 7515000, Avg. loss: 51.573392\n",
      "Total training time: 173.42 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 469.78, NNZs: 8192, Bias: -52.219816, T: 7560000, Avg. loss: 68.182247\n",
      "Total training time: 173.72 seconds.\n",
      "Convergence after 168 epochs took 173.72 seconds\n",
      "Norm: 454.95, NNZs: 8192, Bias: -5.420695, T: 7650000, Avg. loss: 42.080320\n",
      "Total training time: 173.83 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 476.45, NNZs: 8192, Bias: -54.314946, T: 7560000, Avg. loss: 65.592981\n",
      "Total training time: 173.86 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 436.88, NNZs: 8192, Bias: -59.004497, T: 7470000, Avg. loss: 77.372137\n",
      "Total training time: 173.95 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 423.40, NNZs: 8192, Bias: -8.574199, T: 7335000, Avg. loss: 80.069935\n",
      "Total training time: 174.04 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 526.43, NNZs: 8192, Bias: -50.729827, T: 7560000, Avg. loss: 63.331794\n",
      "Total training time: 174.05 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 445.94, NNZs: 8192, Bias: -61.667125, T: 7425000, Avg. loss: 83.096801\n",
      "Total training time: 174.11 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 426.97, NNZs: 8192, Bias: -19.506621, T: 7380000, Avg. loss: 95.077859\n",
      "Total training time: 174.13 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 416.14, NNZs: 8192, Bias: 49.432558, T: 7560000, Avg. loss: 50.223740\n",
      "Total training time: 174.24 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 453.80, NNZs: 8192, Bias: -5.422009, T: 7695000, Avg. loss: 41.600748\n",
      "Total training time: 174.57 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 475.23, NNZs: 8192, Bias: -54.318899, T: 7605000, Avg. loss: 64.959292\n",
      "Total training time: 174.62 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 435.77, NNZs: 8192, Bias: -59.011172, T: 7515000, Avg. loss: 76.049638\n",
      "Total training time: 174.72 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 422.38, NNZs: 8192, Bias: -8.572839, T: 7380000, Avg. loss: 81.304514\n",
      "Total training time: 174.81 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 524.85, NNZs: 8192, Bias: -50.731134, T: 7605000, Avg. loss: 61.286556\n",
      "Total training time: 174.79 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 444.80, NNZs: 8192, Bias: -61.667124, T: 7470000, Avg. loss: 83.806020\n",
      "Total training time: 174.90 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 425.93, NNZs: 8192, Bias: -19.507970, T: 7425000, Avg. loss: 94.849802\n",
      "Total training time: 174.91 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 414.97, NNZs: 8192, Bias: 49.435200, T: 7605000, Avg. loss: 49.984409\n",
      "Total training time: 174.97 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 452.52, NNZs: 8192, Bias: -5.425900, T: 7740000, Avg. loss: 41.429374\n",
      "Total training time: 175.31 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 473.99, NNZs: 8192, Bias: -54.324138, T: 7650000, Avg. loss: 64.912671\n",
      "Total training time: 175.37 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 434.57, NNZs: 8192, Bias: -59.017800, T: 7560000, Avg. loss: 76.212188\n",
      "Total training time: 175.47 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 523.28, NNZs: 8192, Bias: -50.732435, T: 7650000, Avg. loss: 61.947821\n",
      "Total training time: 175.51 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 421.32, NNZs: 8192, Bias: -8.574189, T: 7425000, Avg. loss: 79.959797\n",
      "Total training time: 175.56 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 443.70, NNZs: 8192, Bias: -61.671137, T: 7515000, Avg. loss: 83.727336\n",
      "Total training time: 175.64 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 424.90, NNZs: 8192, Bias: -19.510649, T: 7470000, Avg. loss: 92.429369\n",
      "Total training time: 175.66 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 413.80, NNZs: 8192, Bias: 49.440444, T: 7650000, Avg. loss: 50.427359\n",
      "Total training time: 175.71 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 451.33, NNZs: 8192, Bias: -5.423331, T: 7785000, Avg. loss: 40.578492\n",
      "Total training time: 176.04 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 472.65, NNZs: 8192, Bias: -54.330653, T: 7695000, Avg. loss: 64.635408\n",
      "Total training time: 176.11 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 433.40, NNZs: 8192, Bias: -59.023068, T: 7605000, Avg. loss: 75.306266\n",
      "Total training time: 176.21 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 521.69, NNZs: 8192, Bias: -50.737640, T: 7695000, Avg. loss: 59.914797\n",
      "Total training time: 176.28 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 420.29, NNZs: 8192, Bias: -8.578206, T: 7470000, Avg. loss: 79.264982\n",
      "Total training time: 176.32 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 442.57, NNZs: 8192, Bias: -61.676445, T: 7560000, Avg. loss: 82.324419\n",
      "Total training time: 176.40 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 412.71, NNZs: 8192, Bias: 49.440449, T: 7695000, Avg. loss: 48.975344\n",
      "Total training time: 176.46 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 423.89, NNZs: 8192, Bias: -19.515983, T: 7515000, Avg. loss: 93.245368\n",
      "Total training time: 176.43 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 450.00, NNZs: 8192, Bias: -5.423331, T: 7830000, Avg. loss: 41.258528\n",
      "Total training time: 176.76 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 471.36, NNZs: 8192, Bias: -54.330660, T: 7740000, Avg. loss: 62.765615\n",
      "Total training time: 176.84 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 432.15, NNZs: 8192, Bias: -59.025682, T: 7650000, Avg. loss: 74.212646\n",
      "Total training time: 176.95 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 520.22, NNZs: 8192, Bias: -50.740230, T: 7740000, Avg. loss: 60.241996\n",
      "Total training time: 177.00 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 419.33, NNZs: 8192, Bias: -8.575535, T: 7515000, Avg. loss: 78.573545\n",
      "Total training time: 177.09 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 441.55, NNZs: 8192, Bias: -61.679077, T: 7605000, Avg. loss: 81.549988\n",
      "Total training time: 177.15 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 411.63, NNZs: 8192, Bias: 49.443045, T: 7740000, Avg. loss: 48.981161\n",
      "Total training time: 177.20 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 422.90, NNZs: 8192, Bias: -19.513326, T: 7560000, Avg. loss: 91.764152\n",
      "Total training time: 177.18 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 448.73, NNZs: 8192, Bias: -5.425870, T: 7875000, Avg. loss: 40.629300\n",
      "Total training time: 177.49 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 470.25, NNZs: 8192, Bias: -54.340966, T: 7785000, Avg. loss: 62.967307\n",
      "Total training time: 177.58 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 431.03, NNZs: 8192, Bias: -59.030904, T: 7695000, Avg. loss: 74.401045\n",
      "Total training time: 177.69 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 518.71, NNZs: 8192, Bias: -50.744094, T: 7785000, Avg. loss: 60.007452\n",
      "Total training time: 177.73 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 418.28, NNZs: 8192, Bias: -8.576860, T: 7560000, Avg. loss: 78.440138\n",
      "Total training time: 177.83 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 440.45, NNZs: 8192, Bias: -61.685642, T: 7650000, Avg. loss: 81.800085\n",
      "Total training time: 177.89 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 410.46, NNZs: 8192, Bias: 49.446916, T: 7785000, Avg. loss: 48.769634\n",
      "Total training time: 177.92 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 421.76, NNZs: 8192, Bias: -19.517281, T: 7605000, Avg. loss: 91.723891\n",
      "Total training time: 177.92 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 447.37, NNZs: 8192, Bias: -5.429663, T: 7920000, Avg. loss: 40.230974\n",
      "Total training time: 178.23 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 468.88, NNZs: 8192, Bias: -54.346086, T: 7830000, Avg. loss: 61.803529\n",
      "Total training time: 178.33 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 429.95, NNZs: 8192, Bias: -59.039975, T: 7740000, Avg. loss: 74.015572\n",
      "Total training time: 178.45 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 517.25, NNZs: 8192, Bias: -50.747936, T: 7830000, Avg. loss: 59.980095\n",
      "Total training time: 178.50 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 417.12, NNZs: 8192, Bias: -8.579498, T: 7605000, Avg. loss: 78.453279\n",
      "Total training time: 178.60 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 409.33, NNZs: 8192, Bias: 49.449477, T: 7830000, Avg. loss: 48.183103\n",
      "Total training time: 178.67 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 439.43, NNZs: 8192, Bias: -61.688240, T: 7695000, Avg. loss: 80.175503\n",
      "Total training time: 178.66 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 420.76, NNZs: 8192, Bias: -19.519895, T: 7650000, Avg. loss: 90.598243\n",
      "Total training time: 178.70 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 446.06, NNZs: 8192, Bias: -5.429667, T: 7965000, Avg. loss: 40.026231\n",
      "Total training time: 178.94 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 467.68, NNZs: 8192, Bias: -54.349908, T: 7875000, Avg. loss: 61.480086\n",
      "Total training time: 179.06 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 428.81, NNZs: 8192, Bias: -59.046408, T: 7785000, Avg. loss: 73.014000\n",
      "Total training time: 179.19 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 515.87, NNZs: 8192, Bias: -50.750479, T: 7875000, Avg. loss: 58.534810\n",
      "Total training time: 179.24 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 416.23, NNZs: 8192, Bias: -8.578178, T: 7650000, Avg. loss: 77.357898\n",
      "Total training time: 179.37 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 408.22, NNZs: 8192, Bias: 49.454560, T: 7875000, Avg. loss: 48.200662\n",
      "Total training time: 179.42 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 438.44, NNZs: 8192, Bias: -61.690836, T: 7740000, Avg. loss: 80.468034\n",
      "Total training time: 179.42 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 419.78, NNZs: 8192, Bias: -19.517280, T: 7695000, Avg. loss: 90.481240\n",
      "Total training time: 179.47 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 444.88, NNZs: 8192, Bias: -5.428413, T: 8010000, Avg. loss: 39.724508\n",
      "Total training time: 179.68 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 466.44, NNZs: 8192, Bias: -54.354972, T: 7920000, Avg. loss: 60.873173\n",
      "Total training time: 179.81 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 427.69, NNZs: 8192, Bias: -59.052802, T: 7830000, Avg. loss: 71.992366\n",
      "Total training time: 179.94 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 514.19, NNZs: 8192, Bias: -50.754280, T: 7920000, Avg. loss: 59.722286\n",
      "Total training time: 179.97 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 415.22, NNZs: 8192, Bias: -8.576855, T: 7695000, Avg. loss: 77.308573\n",
      "Total training time: 180.12 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 407.09, NNZs: 8192, Bias: 49.454562, T: 7920000, Avg. loss: 47.736060\n",
      "Total training time: 180.16 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 437.39, NNZs: 8192, Bias: -61.690841, T: 7785000, Avg. loss: 81.578167\n",
      "Total training time: 180.15 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 418.55, NNZs: 8192, Bias: -19.519869, T: 7740000, Avg. loss: 89.057411\n",
      "Total training time: 180.23 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 443.70, NNZs: 8192, Bias: -5.430905, T: 8055000, Avg. loss: 38.832200\n",
      "Total training time: 180.42 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 465.30, NNZs: 8192, Bias: -54.357487, T: 7965000, Avg. loss: 60.573858\n",
      "Total training time: 180.56 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 426.45, NNZs: 8192, Bias: -59.054079, T: 7875000, Avg. loss: 73.510666\n",
      "Total training time: 180.69 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 512.67, NNZs: 8192, Bias: -50.759316, T: 7965000, Avg. loss: 59.599216\n",
      "Total training time: 180.72 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 405.97, NNZs: 8192, Bias: 49.454555, T: 7965000, Avg. loss: 48.173190\n",
      "Total training time: 180.93 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 436.39, NNZs: 8192, Bias: -61.694692, T: 7830000, Avg. loss: 79.246168\n",
      "Total training time: 180.92 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 414.30, NNZs: 8192, Bias: -8.575552, T: 7740000, Avg. loss: 76.893935\n",
      "Total training time: 180.92 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 417.57, NNZs: 8192, Bias: -19.521161, T: 7785000, Avg. loss: 88.993745\n",
      "Total training time: 180.97 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 442.53, NNZs: 8192, Bias: -5.427194, T: 8100000, Avg. loss: 39.944089\n",
      "Total training time: 181.14 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 464.32, NNZs: 8192, Bias: -54.362498, T: 8010000, Avg. loss: 60.787000\n",
      "Total training time: 181.32 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 425.40, NNZs: 8192, Bias: -59.060425, T: 7920000, Avg. loss: 71.304513\n",
      "Total training time: 181.46 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 511.21, NNZs: 8192, Bias: -50.761813, T: 8010000, Avg. loss: 58.142333\n",
      "Total training time: 181.47 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 404.93, NNZs: 8192, Bias: 49.459560, T: 8010000, Avg. loss: 47.424595\n",
      "Total training time: 181.71 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 435.38, NNZs: 8192, Bias: -61.698510, T: 7875000, Avg. loss: 78.197958\n",
      "Total training time: 181.69 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 413.18, NNZs: 8192, Bias: -8.576846, T: 7785000, Avg. loss: 75.027661\n",
      "Total training time: 181.72 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 416.55, NNZs: 8192, Bias: -19.518602, T: 7830000, Avg. loss: 88.882800\n",
      "Total training time: 181.75 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 441.38, NNZs: 8192, Bias: -5.429655, T: 8145000, Avg. loss: 38.609855\n",
      "Total training time: 181.87 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 463.25, NNZs: 8192, Bias: -54.366238, T: 8055000, Avg. loss: 60.622957\n",
      "Total training time: 182.07 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 424.49, NNZs: 8192, Bias: -59.070490, T: 7965000, Avg. loss: 70.322478\n",
      "Total training time: 182.22 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 509.85, NNZs: 8192, Bias: -50.766791, T: 8055000, Avg. loss: 58.675728\n",
      "Total training time: 182.21 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 403.90, NNZs: 8192, Bias: 49.460799, T: 8055000, Avg. loss: 46.445941\n",
      "Total training time: 182.46 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 434.33, NNZs: 8192, Bias: -61.698514, T: 7920000, Avg. loss: 78.349168\n",
      "Total training time: 182.45 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 412.30, NNZs: 8192, Bias: -8.575570, T: 7830000, Avg. loss: 74.252344\n",
      "Total training time: 182.49 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 415.72, NNZs: 8192, Bias: -19.518611, T: 7875000, Avg. loss: 88.223774\n",
      "Total training time: 182.51 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 440.29, NNZs: 8192, Bias: -5.430879, T: 8190000, Avg. loss: 38.421424\n",
      "Total training time: 182.61 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 462.05, NNZs: 8192, Bias: -54.371193, T: 8100000, Avg. loss: 59.291538\n",
      "Total training time: 182.82 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 423.37, NNZs: 8192, Bias: -59.074248, T: 8010000, Avg. loss: 71.891036\n",
      "Total training time: 182.98 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 508.40, NNZs: 8192, Bias: -50.766787, T: 8100000, Avg. loss: 58.536170\n",
      "Total training time: 182.97 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 402.82, NNZs: 8192, Bias: 49.459561, T: 8100000, Avg. loss: 46.792545\n",
      "Total training time: 183.23 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 433.37, NNZs: 8192, Bias: -61.701034, T: 7965000, Avg. loss: 79.253000\n",
      "Total training time: 183.21 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 411.37, NNZs: 8192, Bias: -8.576848, T: 7875000, Avg. loss: 75.181791\n",
      "Total training time: 183.28 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 414.70, NNZs: 8192, Bias: -19.523690, T: 7920000, Avg. loss: 89.341834\n",
      "Total training time: 183.27 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 439.08, NNZs: 8192, Bias: -5.432094, T: 8235000, Avg. loss: 38.122359\n",
      "Total training time: 183.35 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 460.96, NNZs: 8192, Bias: -54.377344, T: 8145000, Avg. loss: 60.540149\n",
      "Total training time: 183.56 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 422.32, NNZs: 8192, Bias: -59.079221, T: 8055000, Avg. loss: 70.992374\n",
      "Total training time: 183.72 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 506.97, NNZs: 8192, Bias: -50.769253, T: 8145000, Avg. loss: 57.820485\n",
      "Total training time: 183.69 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 401.69, NNZs: 8192, Bias: 49.462027, T: 8145000, Avg. loss: 46.279055\n",
      "Total training time: 183.98 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 432.45, NNZs: 8192, Bias: -61.703532, T: 8010000, Avg. loss: 78.249658\n",
      "Total training time: 183.95 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 413.71, NNZs: 8192, Bias: -19.523693, T: 7965000, Avg. loss: 86.769533\n",
      "Total training time: 184.02 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 410.28, NNZs: 8192, Bias: -8.580642, T: 7920000, Avg. loss: 73.950518\n",
      "Total training time: 184.06 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 437.97, NNZs: 8192, Bias: -5.430882, T: 8280000, Avg. loss: 38.052594\n",
      "Total training time: 184.08 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 459.87, NNZs: 8192, Bias: -54.379785, T: 8190000, Avg. loss: 58.667824\n",
      "Total training time: 184.28 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 421.19, NNZs: 8192, Bias: -59.082929, T: 8100000, Avg. loss: 70.019463\n",
      "Total training time: 184.46 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 505.60, NNZs: 8192, Bias: -50.769258, T: 8190000, Avg. loss: 56.027191\n",
      "Total training time: 184.42 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 431.47, NNZs: 8192, Bias: -61.709751, T: 8055000, Avg. loss: 77.225051\n",
      "Total training time: 184.68 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 400.69, NNZs: 8192, Bias: 49.463254, T: 8190000, Avg. loss: 46.019934\n",
      "Total training time: 184.71 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 412.74, NNZs: 8192, Bias: -19.526204, T: 8010000, Avg. loss: 86.250908\n",
      "Total training time: 184.78 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 436.89, NNZs: 8192, Bias: -5.429680, T: 8325000, Avg. loss: 37.148530\n",
      "Total training time: 184.79 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 409.30, NNZs: 8192, Bias: -8.576865, T: 7965000, Avg. loss: 73.504012\n",
      "Total training time: 184.81 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 458.58, NNZs: 8192, Bias: -54.384650, T: 8235000, Avg. loss: 59.488718\n",
      "Total training time: 185.00 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 504.17, NNZs: 8192, Bias: -50.771692, T: 8235000, Avg. loss: 55.991653\n",
      "Total training time: 185.17 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 420.20, NNZs: 8192, Bias: -59.089083, T: 8145000, Avg. loss: 68.854317\n",
      "Total training time: 185.22 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 399.66, NNZs: 8192, Bias: 49.468119, T: 8235000, Avg. loss: 45.802097\n",
      "Total training time: 185.48 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 430.48, NNZs: 8192, Bias: -61.708512, T: 8100000, Avg. loss: 76.690774\n",
      "Total training time: 185.46 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 411.75, NNZs: 8192, Bias: -19.529945, T: 8055000, Avg. loss: 86.647780\n",
      "Total training time: 185.57 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 435.69, NNZs: 8192, Bias: -5.429690, T: 8370000, Avg. loss: 37.467814\n",
      "Total training time: 185.57 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 408.45, NNZs: 8192, Bias: -8.580629, T: 8010000, Avg. loss: 74.174514\n",
      "Total training time: 185.59 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 457.58, NNZs: 8192, Bias: -54.388283, T: 8280000, Avg. loss: 58.192141\n",
      "Total training time: 185.80 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 502.76, NNZs: 8192, Bias: -50.774115, T: 8280000, Avg. loss: 56.333140\n",
      "Total training time: 185.96 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 419.13, NNZs: 8192, Bias: -59.095194, T: 8190000, Avg. loss: 69.106043\n",
      "Total training time: 186.00 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 398.74, NNZs: 8192, Bias: 49.472966, T: 8280000, Avg. loss: 45.894487\n",
      "Total training time: 186.27 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 429.68, NNZs: 8192, Bias: -61.713433, T: 8145000, Avg. loss: 75.339717\n",
      "Total training time: 186.24 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 434.55, NNZs: 8192, Bias: -5.432074, T: 8415000, Avg. loss: 38.337937\n",
      "Total training time: 186.32 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 410.77, NNZs: 8192, Bias: -19.528707, T: 8100000, Avg. loss: 84.563138\n",
      "Total training time: 186.35 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 407.46, NNZs: 8192, Bias: -8.579386, T: 8055000, Avg. loss: 73.391529\n",
      "Total training time: 186.37 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 456.50, NNZs: 8192, Bias: -54.390693, T: 8325000, Avg. loss: 57.521596\n",
      "Total training time: 186.54 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 501.43, NNZs: 8192, Bias: -50.775317, T: 8325000, Avg. loss: 55.585462\n",
      "Total training time: 186.70 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 418.00, NNZs: 8192, Bias: -59.098842, T: 8235000, Avg. loss: 68.231309\n",
      "Total training time: 186.76 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 397.68, NNZs: 8192, Bias: 49.471765, T: 8325000, Avg. loss: 45.168754\n",
      "Total training time: 187.00 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 428.66, NNZs: 8192, Bias: -61.715884, T: 8190000, Avg. loss: 75.663751\n",
      "Total training time: 186.98 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 433.45, NNZs: 8192, Bias: -5.430889, T: 8460000, Avg. loss: 36.798266\n",
      "Total training time: 187.03 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 409.84, NNZs: 8192, Bias: -19.531162, T: 8145000, Avg. loss: 85.107974\n",
      "Total training time: 187.11 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 406.48, NNZs: 8192, Bias: -8.583098, T: 8100000, Avg. loss: 73.644739\n",
      "Total training time: 187.12 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 455.36, NNZs: 8192, Bias: -54.396681, T: 8370000, Avg. loss: 57.456962\n",
      "Total training time: 187.30 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 500.03, NNZs: 8192, Bias: -50.782507, T: 8370000, Avg. loss: 55.921136\n",
      "Total training time: 187.43 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 417.15, NNZs: 8192, Bias: -59.100056, T: 8280000, Avg. loss: 68.297183\n",
      "Total training time: 187.50 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 396.64, NNZs: 8192, Bias: 49.472967, T: 8370000, Avg. loss: 44.413536\n",
      "Total training time: 187.74 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 427.81, NNZs: 8192, Bias: -61.720753, T: 8235000, Avg. loss: 75.435285\n",
      "Total training time: 187.73 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 432.33, NNZs: 8192, Bias: -5.428541, T: 8505000, Avg. loss: 37.829735\n",
      "Total training time: 187.74 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 408.98, NNZs: 8192, Bias: -19.532386, T: 8190000, Avg. loss: 83.821825\n",
      "Total training time: 187.85 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 405.56, NNZs: 8192, Bias: -8.583088, T: 8145000, Avg. loss: 72.861332\n",
      "Total training time: 187.88 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 454.31, NNZs: 8192, Bias: -54.397878, T: 8415000, Avg. loss: 57.891144\n",
      "Total training time: 188.02 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 498.59, NNZs: 8192, Bias: -50.786072, T: 8415000, Avg. loss: 55.069073\n",
      "Total training time: 188.15 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 416.15, NNZs: 8192, Bias: -59.107286, T: 8325000, Avg. loss: 67.912772\n",
      "Total training time: 188.24 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 395.69, NNZs: 8192, Bias: 49.476542, T: 8415000, Avg. loss: 45.170991\n",
      "Total training time: 188.49 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 427.01, NNZs: 8192, Bias: -61.721964, T: 8280000, Avg. loss: 73.695287\n",
      "Total training time: 188.49 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 431.27, NNZs: 8192, Bias: -5.434399, T: 8550000, Avg. loss: 36.346670\n",
      "Total training time: 188.48 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 404.73, NNZs: 8192, Bias: -8.585531, T: 8190000, Avg. loss: 71.863834\n",
      "Total training time: 188.61 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 408.14, NNZs: 8192, Bias: -19.533602, T: 8235000, Avg. loss: 83.690270\n",
      "Total training time: 188.60 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 453.14, NNZs: 8192, Bias: -54.403808, T: 8460000, Avg. loss: 57.705064\n",
      "Total training time: 188.75 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 497.25, NNZs: 8192, Bias: -50.786067, T: 8460000, Avg. loss: 55.330395\n",
      "Total training time: 188.88 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 415.09, NNZs: 8192, Bias: -59.112072, T: 8370000, Avg. loss: 67.688399\n",
      "Total training time: 188.97 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 394.69, NNZs: 8192, Bias: 49.477728, T: 8460000, Avg. loss: 44.520493\n",
      "Total training time: 189.21 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 430.07, NNZs: 8192, Bias: -5.430894, T: 8595000, Avg. loss: 36.282306\n",
      "Total training time: 189.19 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 426.10, NNZs: 8192, Bias: -61.724369, T: 8325000, Avg. loss: 73.182014\n",
      "Total training time: 189.23 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 407.13, NNZs: 8192, Bias: -19.533598, T: 8280000, Avg. loss: 83.630399\n",
      "Total training time: 189.34 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 403.83, NNZs: 8192, Bias: -8.580659, T: 8235000, Avg. loss: 71.449048\n",
      "Total training time: 189.37 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 452.11, NNZs: 8192, Bias: -54.403807, T: 8505000, Avg. loss: 56.438747\n",
      "Total training time: 189.49 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 495.88, NNZs: 8192, Bias: -50.789608, T: 8505000, Avg. loss: 54.698570\n",
      "Total training time: 189.60 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 414.05, NNZs: 8192, Bias: -59.120415, T: 8415000, Avg. loss: 66.761670\n",
      "Total training time: 189.71 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 393.74, NNZs: 8192, Bias: 49.476551, T: 8505000, Avg. loss: 44.281657\n",
      "Total training time: 189.96 seconds.\n",
      "Norm: 428.98, NNZs: 8192, Bias: -5.430897, T: 8640000, Avg. loss: 36.081611\n",
      "Total training time: 189.91 seconds.\n",
      "-- Epoch 190\n",
      "-- Epoch 193\n",
      "Norm: 425.04, NNZs: 8192, Bias: -61.725562, T: 8370000, Avg. loss: 73.410182\n",
      "Total training time: 189.98 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 406.28, NNZs: 8192, Bias: -19.534800, T: 8325000, Avg. loss: 83.666682\n",
      "Total training time: 190.10 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 402.91, NNZs: 8192, Bias: -8.585507, T: 8280000, Avg. loss: 70.653571\n",
      "Total training time: 190.13 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 451.03, NNZs: 8192, Bias: -54.409676, T: 8550000, Avg. loss: 55.024358\n",
      "Total training time: 190.22 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 494.46, NNZs: 8192, Bias: -50.791953, T: 8550000, Avg. loss: 53.546982\n",
      "Total training time: 190.33 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 413.01, NNZs: 8192, Bias: -59.125155, T: 8460000, Avg. loss: 66.438699\n",
      "Total training time: 190.46 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 427.91, NNZs: 8192, Bias: -5.433200, T: 8685000, Avg. loss: 35.775160\n",
      "Total training time: 190.66 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 392.78, NNZs: 8192, Bias: 49.480072, T: 8550000, Avg. loss: 44.191669\n",
      "Total training time: 190.71 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 424.10, NNZs: 8192, Bias: -61.726755, T: 8415000, Avg. loss: 72.922573\n",
      "Total training time: 190.74 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 405.45, NNZs: 8192, Bias: -19.533597, T: 8370000, Avg. loss: 82.536281\n",
      "Total training time: 190.88 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 401.95, NNZs: 8192, Bias: -8.586712, T: 8325000, Avg. loss: 71.564998\n",
      "Total training time: 190.91 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 449.95, NNZs: 8192, Bias: -54.412010, T: 8595000, Avg. loss: 56.240442\n",
      "Total training time: 190.97 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 493.13, NNZs: 8192, Bias: -50.791957, T: 8595000, Avg. loss: 54.121822\n",
      "Total training time: 191.08 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 412.03, NNZs: 8192, Bias: -59.132224, T: 8505000, Avg. loss: 66.430766\n",
      "Total training time: 191.23 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 426.91, NNZs: 8192, Bias: -5.435487, T: 8730000, Avg. loss: 35.940240\n",
      "Total training time: 191.41 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 391.75, NNZs: 8192, Bias: 49.480080, T: 8595000, Avg. loss: 43.612786\n",
      "Total training time: 191.49 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 423.05, NNZs: 8192, Bias: -61.730310, T: 8460000, Avg. loss: 71.748656\n",
      "Total training time: 191.52 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 404.63, NNZs: 8192, Bias: -19.534782, T: 8415000, Avg. loss: 81.272234\n",
      "Total training time: 191.68 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 401.06, NNZs: 8192, Bias: -8.586704, T: 8370000, Avg. loss: 70.510379\n",
      "Total training time: 191.72 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 448.93, NNZs: 8192, Bias: -54.416652, T: 8640000, Avg. loss: 55.754199\n",
      "Total training time: 191.75 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 491.80, NNZs: 8192, Bias: -50.794287, T: 8640000, Avg. loss: 52.872460\n",
      "Total training time: 191.86 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 411.07, NNZs: 8192, Bias: -59.138078, T: 8550000, Avg. loss: 64.619694\n",
      "Total training time: 192.04 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 425.82, NNZs: 8192, Bias: -5.435484, T: 8775000, Avg. loss: 35.371784\n",
      "Total training time: 192.17 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 390.82, NNZs: 8192, Bias: 49.484727, T: 8640000, Avg. loss: 42.426347\n",
      "Total training time: 192.28 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 422.26, NNZs: 8192, Bias: -61.731491, T: 8505000, Avg. loss: 70.887794\n",
      "Total training time: 192.32 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 403.75, NNZs: 8192, Bias: -19.535964, T: 8460000, Avg. loss: 82.079301\n",
      "Total training time: 192.49 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 400.14, NNZs: 8192, Bias: -8.585517, T: 8415000, Avg. loss: 69.784534\n",
      "Total training time: 192.54 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 447.95, NNZs: 8192, Bias: -54.421265, T: 8685000, Avg. loss: 54.464244\n",
      "Total training time: 192.53 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 490.58, NNZs: 8192, Bias: -50.797754, T: 8685000, Avg. loss: 53.637498\n",
      "Total training time: 192.62 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 410.12, NNZs: 8192, Bias: -59.143910, T: 8595000, Avg. loss: 65.282980\n",
      "Total training time: 192.79 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 424.83, NNZs: 8192, Bias: -5.432076, T: 8820000, Avg. loss: 34.552482\n",
      "Total training time: 192.90 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 389.93, NNZs: 8192, Bias: 49.485885, T: 8685000, Avg. loss: 43.032181\n",
      "Total training time: 193.04 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 421.37, NNZs: 8192, Bias: -61.737361, T: 8550000, Avg. loss: 71.366074\n",
      "Total training time: 193.07 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 402.74, NNZs: 8192, Bias: -19.533609, T: 8505000, Avg. loss: 80.799228\n",
      "Total training time: 193.27 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 446.87, NNZs: 8192, Bias: -54.423563, T: 8730000, Avg. loss: 55.890260\n",
      "Total training time: 193.30 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 399.18, NNZs: 8192, Bias: -8.585528, T: 8460000, Avg. loss: 68.373074\n",
      "Total training time: 193.34 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 489.23, NNZs: 8192, Bias: -50.802349, T: 8730000, Avg. loss: 52.619440\n",
      "Total training time: 193.37 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 409.18, NNZs: 8192, Bias: -59.148546, T: 8640000, Avg. loss: 65.510241\n",
      "Total training time: 193.56 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 423.78, NNZs: 8192, Bias: -5.434342, T: 8865000, Avg. loss: 35.162393\n",
      "Total training time: 193.64 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 388.90, NNZs: 8192, Bias: 49.485881, T: 8730000, Avg. loss: 42.499320\n",
      "Total training time: 193.80 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 420.47, NNZs: 8192, Bias: -61.739693, T: 8595000, Avg. loss: 71.316046\n",
      "Total training time: 193.81 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 401.88, NNZs: 8192, Bias: -19.533603, T: 8550000, Avg. loss: 80.444908\n",
      "Total training time: 194.04 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 445.86, NNZs: 8192, Bias: -54.428142, T: 8775000, Avg. loss: 54.525853\n",
      "Total training time: 194.06 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 398.29, NNZs: 8192, Bias: -8.589067, T: 8505000, Avg. loss: 67.848930\n",
      "Total training time: 194.13 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 488.10, NNZs: 8192, Bias: -50.802349, T: 8775000, Avg. loss: 53.093534\n",
      "Total training time: 194.14 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 408.27, NNZs: 8192, Bias: -59.151994, T: 8685000, Avg. loss: 64.713164\n",
      "Total training time: 194.37 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 422.70, NNZs: 8192, Bias: -5.435465, T: 8910000, Avg. loss: 35.636279\n",
      "Total training time: 194.41 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 387.99, NNZs: 8192, Bias: 49.491599, T: 8775000, Avg. loss: 42.842705\n",
      "Total training time: 194.60 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 419.61, NNZs: 8192, Bias: -61.743165, T: 8640000, Avg. loss: 71.606531\n",
      "Total training time: 194.62 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 444.86, NNZs: 8192, Bias: -54.434965, T: 8820000, Avg. loss: 52.814083\n",
      "Total training time: 194.83 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 400.89, NNZs: 8192, Bias: -19.535934, T: 8595000, Avg. loss: 80.514582\n",
      "Total training time: 194.86 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 486.84, NNZs: 8192, Bias: -50.805763, T: 8820000, Avg. loss: 52.764907\n",
      "Total training time: 194.92 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 397.37, NNZs: 8192, Bias: -8.591412, T: 8550000, Avg. loss: 67.989086\n",
      "Total training time: 194.95 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 407.33, NNZs: 8192, Bias: -59.156584, T: 8730000, Avg. loss: 64.873831\n",
      "Total training time: 195.14 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 421.69, NNZs: 8192, Bias: -5.437706, T: 8955000, Avg. loss: 34.705581\n",
      "Total training time: 195.16 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 387.01, NNZs: 8192, Bias: 49.495012, T: 8820000, Avg. loss: 42.523659\n",
      "Total training time: 195.38 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 418.79, NNZs: 8192, Bias: -61.747777, T: 8685000, Avg. loss: 72.041594\n",
      "Total training time: 195.43 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 443.89, NNZs: 8192, Bias: -54.439488, T: 8865000, Avg. loss: 54.009724\n",
      "Total training time: 195.62 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 399.97, NNZs: 8192, Bias: -19.535936, T: 8640000, Avg. loss: 79.489471\n",
      "Total training time: 195.68 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 485.51, NNZs: 8192, Bias: -50.808024, T: 8865000, Avg. loss: 51.862863\n",
      "Total training time: 195.71 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 396.54, NNZs: 8192, Bias: -8.587920, T: 8595000, Avg. loss: 67.817877\n",
      "Total training time: 195.76 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 406.36, NNZs: 8192, Bias: -59.158872, T: 8775000, Avg. loss: 63.615159\n",
      "Total training time: 195.95 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 420.70, NNZs: 8192, Bias: -5.435475, T: 9000000, Avg. loss: 34.761980\n",
      "Total training time: 195.94 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 386.05, NNZs: 8192, Bias: 49.495017, T: 8865000, Avg. loss: 42.248595\n",
      "Total training time: 196.18 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 417.79, NNZs: 8192, Bias: -61.752364, T: 8730000, Avg. loss: 70.424613\n",
      "Total training time: 196.25 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 442.95, NNZs: 8192, Bias: -54.442859, T: 8910000, Avg. loss: 52.258874\n",
      "Total training time: 196.43 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 399.05, NNZs: 8192, Bias: -19.538247, T: 8685000, Avg. loss: 79.072034\n",
      "Total training time: 196.50 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 484.29, NNZs: 8192, Bias: -50.811405, T: 8910000, Avg. loss: 51.712199\n",
      "Total training time: 196.53 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 395.81, NNZs: 8192, Bias: -8.590249, T: 8640000, Avg. loss: 67.875158\n",
      "Total training time: 196.59 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 405.49, NNZs: 8192, Bias: -59.166823, T: 8820000, Avg. loss: 64.084963\n",
      "Total training time: 196.75 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 419.75, NNZs: 8192, Bias: -5.437691, T: 9045000, Avg. loss: 33.655534\n",
      "Total training time: 196.72 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 385.15, NNZs: 8192, Bias: 49.499515, T: 8910000, Avg. loss: 42.519488\n",
      "Total training time: 196.97 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 416.89, NNZs: 8192, Bias: -61.752359, T: 8775000, Avg. loss: 69.737127\n",
      "Total training time: 197.06 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 441.95, NNZs: 8192, Bias: -54.443981, T: 8955000, Avg. loss: 52.807353\n",
      "Total training time: 197.22 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 398.24, NNZs: 8192, Bias: -19.540548, T: 8730000, Avg. loss: 79.649066\n",
      "Total training time: 197.30 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 483.01, NNZs: 8192, Bias: -50.814764, T: 8955000, Avg. loss: 50.573006\n",
      "Total training time: 197.31 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 394.91, NNZs: 8192, Bias: -8.590252, T: 8685000, Avg. loss: 66.233409\n",
      "Total training time: 197.39 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 418.71, NNZs: 8192, Bias: -5.435487, T: 9090000, Avg. loss: 33.652003\n",
      "Total training time: 197.49 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 404.55, NNZs: 8192, Bias: -59.166822, T: 8865000, Avg. loss: 63.403124\n",
      "Total training time: 197.54 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 384.20, NNZs: 8192, Bias: 49.500637, T: 8955000, Avg. loss: 42.417508\n",
      "Total training time: 197.76 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 416.05, NNZs: 8192, Bias: -61.754631, T: 8820000, Avg. loss: 69.213788\n",
      "Total training time: 197.85 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 440.98, NNZs: 8192, Bias: -54.449555, T: 9000000, Avg. loss: 53.390022\n",
      "Total training time: 197.98 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 481.70, NNZs: 8192, Bias: -50.819211, T: 9000000, Avg. loss: 50.831898\n",
      "Total training time: 198.07 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 397.33, NNZs: 8192, Bias: -19.540547, T: 8775000, Avg. loss: 78.193937\n",
      "Total training time: 198.09 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 394.05, NNZs: 8192, Bias: -8.592552, T: 8730000, Avg. loss: 66.931574\n",
      "Total training time: 198.18 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 417.76, NNZs: 8192, Bias: -5.433293, T: 9135000, Avg. loss: 33.579932\n",
      "Total training time: 198.24 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 403.52, NNZs: 8192, Bias: -59.172449, T: 8910000, Avg. loss: 63.979647\n",
      "Total training time: 198.33 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 383.28, NNZs: 8192, Bias: 49.501748, T: 9000000, Avg. loss: 41.308556\n",
      "Total training time: 198.54 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 415.22, NNZs: 8192, Bias: -61.760287, T: 8865000, Avg. loss: 69.450790\n",
      "Total training time: 198.64 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 439.94, NNZs: 8192, Bias: -54.453987, T: 9045000, Avg. loss: 52.800869\n",
      "Total training time: 198.77 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 480.43, NNZs: 8192, Bias: -50.819210, T: 9045000, Avg. loss: 49.789024\n",
      "Total training time: 198.86 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 396.46, NNZs: 8192, Bias: -19.541687, T: 8820000, Avg. loss: 77.952630\n",
      "Total training time: 198.91 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 393.36, NNZs: 8192, Bias: -8.593690, T: 8775000, Avg. loss: 65.821245\n",
      "Total training time: 198.98 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 416.75, NNZs: 8192, Bias: -5.434388, T: 9180000, Avg. loss: 33.462197\n",
      "Total training time: 199.01 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 402.57, NNZs: 8192, Bias: -59.176923, T: 8955000, Avg. loss: 61.295752\n",
      "Total training time: 199.12 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 382.41, NNZs: 8192, Bias: 49.501748, T: 9045000, Avg. loss: 40.710757\n",
      "Total training time: 199.31 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 414.37, NNZs: 8192, Bias: -61.761409, T: 8910000, Avg. loss: 69.250584\n",
      "Total training time: 199.40 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 438.93, NNZs: 8192, Bias: -54.459505, T: 9090000, Avg. loss: 52.545960\n",
      "Total training time: 199.51 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 479.18, NNZs: 8192, Bias: -50.823619, T: 9090000, Avg. loss: 51.274669\n",
      "Total training time: 199.59 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 395.67, NNZs: 8192, Bias: -19.542818, T: 8865000, Avg. loss: 77.150130\n",
      "Total training time: 199.66 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 392.43, NNZs: 8192, Bias: -8.592549, T: 8820000, Avg. loss: 66.900608\n",
      "Total training time: 199.74 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 415.70, NNZs: 8192, Bias: -5.433303, T: 9225000, Avg. loss: 33.254582\n",
      "Total training time: 199.74 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 401.72, NNZs: 8192, Bias: -59.181384, T: 9000000, Avg. loss: 62.045269\n",
      "Total training time: 199.87 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 381.54, NNZs: 8192, Bias: 49.506167, T: 9090000, Avg. loss: 40.818940\n",
      "Total training time: 200.06 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 413.52, NNZs: 8192, Bias: -61.762525, T: 8955000, Avg. loss: 68.142207\n",
      "Total training time: 200.16 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 438.01, NNZs: 8192, Bias: -54.462795, T: 9135000, Avg. loss: 52.301876\n",
      "Total training time: 200.24 seconds.\n",
      "Convergence after 203 epochs took 200.24 seconds\n",
      "Norm: 477.92, NNZs: 8192, Bias: -50.824716, T: 9135000, Avg. loss: 48.896786\n",
      "Total training time: 200.33 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 394.80, NNZs: 8192, Bias: -19.543942, T: 8910000, Avg. loss: 75.792433\n",
      "Total training time: 200.41 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 414.72, NNZs: 8192, Bias: -5.434384, T: 9270000, Avg. loss: 33.650484\n",
      "Total training time: 200.44 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 391.59, NNZs: 8192, Bias: -8.591412, T: 8865000, Avg. loss: 65.565419\n",
      "Total training time: 200.47 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 400.91, NNZs: 8192, Bias: -59.186927, T: 9045000, Avg. loss: 62.421610\n",
      "Total training time: 200.57 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 380.61, NNZs: 8192, Bias: 49.506175, T: 9135000, Avg. loss: 41.238799\n",
      "Total training time: 200.72 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 412.61, NNZs: 8192, Bias: -61.764753, T: 9000000, Avg. loss: 68.102821\n",
      "Total training time: 200.83 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 476.73, NNZs: 8192, Bias: -50.825806, T: 9180000, Avg. loss: 49.379864\n",
      "Total training time: 200.95 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 413.75, NNZs: 8192, Bias: -5.439764, T: 9315000, Avg. loss: 32.971061\n",
      "Total training time: 201.06 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 394.02, NNZs: 8192, Bias: -19.546187, T: 8955000, Avg. loss: 75.973188\n",
      "Total training time: 201.07 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 390.68, NNZs: 8192, Bias: -8.592537, T: 8910000, Avg. loss: 65.976398\n",
      "Total training time: 201.13 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 400.00, NNZs: 8192, Bias: -59.191341, T: 9090000, Avg. loss: 61.784056\n",
      "Total training time: 201.21 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 379.80, NNZs: 8192, Bias: 49.509452, T: 9180000, Avg. loss: 40.227087\n",
      "Total training time: 201.36 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 411.84, NNZs: 8192, Bias: -61.766967, T: 9045000, Avg. loss: 67.667523\n",
      "Total training time: 201.47 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 475.58, NNZs: 8192, Bias: -50.827982, T: 9225000, Avg. loss: 48.712199\n",
      "Total training time: 201.56 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 412.82, NNZs: 8192, Bias: -5.439761, T: 9360000, Avg. loss: 32.482995\n",
      "Total training time: 201.65 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 393.34, NNZs: 8192, Bias: -19.545070, T: 9000000, Avg. loss: 75.695732\n",
      "Total training time: 201.69 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 389.88, NNZs: 8192, Bias: -8.593655, T: 8955000, Avg. loss: 64.838751\n",
      "Total training time: 201.77 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 399.20, NNZs: 8192, Bias: -59.199026, T: 9135000, Avg. loss: 60.989199\n",
      "Total training time: 201.84 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 379.00, NNZs: 8192, Bias: 49.511623, T: 9225000, Avg. loss: 40.456139\n",
      "Total training time: 201.98 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 411.05, NNZs: 8192, Bias: -61.768072, T: 9090000, Avg. loss: 67.495111\n",
      "Total training time: 202.13 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 474.44, NNZs: 8192, Bias: -50.834469, T: 9270000, Avg. loss: 49.608456\n",
      "Total training time: 202.20 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 411.88, NNZs: 8192, Bias: -5.438696, T: 9405000, Avg. loss: 32.675026\n",
      "Total training time: 202.29 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 392.54, NNZs: 8192, Bias: -19.547279, T: 9045000, Avg. loss: 76.215275\n",
      "Total training time: 202.36 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 389.07, NNZs: 8192, Bias: -8.594760, T: 9000000, Avg. loss: 64.769219\n",
      "Total training time: 202.45 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 398.29, NNZs: 8192, Bias: -59.201214, T: 9180000, Avg. loss: 61.179723\n",
      "Total training time: 202.50 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 378.10, NNZs: 8192, Bias: 49.511624, T: 9270000, Avg. loss: 39.776940\n",
      "Total training time: 202.63 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 410.31, NNZs: 8192, Bias: -61.773556, T: 9135000, Avg. loss: 66.552332\n",
      "Total training time: 202.76 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 473.12, NNZs: 8192, Bias: -50.830163, T: 9315000, Avg. loss: 49.145740\n",
      "Total training time: 202.82 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 410.97, NNZs: 8192, Bias: -5.439754, T: 9450000, Avg. loss: 33.480417\n",
      "Total training time: 202.90 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 391.77, NNZs: 8192, Bias: -19.549479, T: 9090000, Avg. loss: 75.321350\n",
      "Total training time: 203.02 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 388.29, NNZs: 8192, Bias: -8.595871, T: 9045000, Avg. loss: 63.572943\n",
      "Total training time: 203.11 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 397.38, NNZs: 8192, Bias: -59.208824, T: 9225000, Avg. loss: 60.841088\n",
      "Total training time: 203.15 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 377.19, NNZs: 8192, Bias: 49.512701, T: 9315000, Avg. loss: 39.425063\n",
      "Total training time: 203.28 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 409.48, NNZs: 8192, Bias: -61.776832, T: 9180000, Avg. loss: 67.539825\n",
      "Total training time: 203.41 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 471.88, NNZs: 8192, Bias: -50.835522, T: 9360000, Avg. loss: 49.281826\n",
      "Total training time: 203.46 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 409.99, NNZs: 8192, Bias: -5.440812, T: 9495000, Avg. loss: 32.622515\n",
      "Total training time: 203.55 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 391.00, NNZs: 8192, Bias: -19.545088, T: 9135000, Avg. loss: 74.081481\n",
      "Total training time: 203.68 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 396.57, NNZs: 8192, Bias: -59.212065, T: 9270000, Avg. loss: 60.655161\n",
      "Total training time: 203.78 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 387.54, NNZs: 8192, Bias: -8.594772, T: 9090000, Avg. loss: 63.938099\n",
      "Total training time: 203.78 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 376.43, NNZs: 8192, Bias: 49.513771, T: 9360000, Avg. loss: 38.990814\n",
      "Total training time: 203.93 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 408.76, NNZs: 8192, Bias: -61.777914, T: 9225000, Avg. loss: 65.202657\n",
      "Total training time: 204.05 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 470.72, NNZs: 8192, Bias: -50.836585, T: 9405000, Avg. loss: 48.590164\n",
      "Total training time: 204.09 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 408.99, NNZs: 8192, Bias: -5.437660, T: 9540000, Avg. loss: 32.790003\n",
      "Total training time: 204.18 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 390.28, NNZs: 8192, Bias: -19.551638, T: 9180000, Avg. loss: 74.194439\n",
      "Total training time: 204.36 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 395.76, NNZs: 8192, Bias: -59.219596, T: 9315000, Avg. loss: 58.531537\n",
      "Total training time: 204.46 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 386.68, NNZs: 8192, Bias: -8.598067, T: 9135000, Avg. loss: 63.991497\n",
      "Total training time: 204.48 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 375.69, NNZs: 8192, Bias: 49.515898, T: 9405000, Avg. loss: 39.092414\n",
      "Total training time: 204.63 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 407.95, NNZs: 8192, Bias: -61.782250, T: 9270000, Avg. loss: 65.336895\n",
      "Total training time: 204.74 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 469.65, NNZs: 8192, Bias: -50.840832, T: 9450000, Avg. loss: 47.856250\n",
      "Total training time: 204.76 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 408.01, NNZs: 8192, Bias: -5.439754, T: 9585000, Avg. loss: 31.992532\n",
      "Total training time: 204.84 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 389.49, NNZs: 8192, Bias: -19.549456, T: 9225000, Avg. loss: 73.697662\n",
      "Total training time: 205.04 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 394.98, NNZs: 8192, Bias: -59.223879, T: 9360000, Avg. loss: 58.413594\n",
      "Total training time: 205.12 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 385.92, NNZs: 8192, Bias: -8.602436, T: 9180000, Avg. loss: 62.408291\n",
      "Total training time: 205.17 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 374.75, NNZs: 8192, Bias: 49.519085, T: 9450000, Avg. loss: 39.599330\n",
      "Total training time: 205.30 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 407.04, NNZs: 8192, Bias: -61.786560, T: 9315000, Avg. loss: 65.352404\n",
      "Total training time: 205.42 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 468.48, NNZs: 8192, Bias: -50.843998, T: 9495000, Avg. loss: 48.694816\n",
      "Total training time: 205.42 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 407.04, NNZs: 8192, Bias: -5.439751, T: 9630000, Avg. loss: 32.795430\n",
      "Total training time: 205.49 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 388.71, NNZs: 8192, Bias: -19.553783, T: 9270000, Avg. loss: 74.406476\n",
      "Total training time: 205.72 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 394.16, NNZs: 8192, Bias: -59.226009, T: 9405000, Avg. loss: 59.285142\n",
      "Total training time: 205.80 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 385.19, NNZs: 8192, Bias: -8.601348, T: 9225000, Avg. loss: 63.111110\n",
      "Total training time: 205.86 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 373.90, NNZs: 8192, Bias: 49.524358, T: 9495000, Avg. loss: 38.939556\n",
      "Total training time: 205.97 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 406.23, NNZs: 8192, Bias: -61.788696, T: 9360000, Avg. loss: 65.067270\n",
      "Total training time: 206.08 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 467.25, NNZs: 8192, Bias: -50.846098, T: 9540000, Avg. loss: 48.383266\n",
      "Total training time: 206.07 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 406.14, NNZs: 8192, Bias: -5.436644, T: 9675000, Avg. loss: 31.338799\n",
      "Total training time: 206.14 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 387.89, NNZs: 8192, Bias: -19.554866, T: 9315000, Avg. loss: 73.300094\n",
      "Total training time: 206.38 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 393.30, NNZs: 8192, Bias: -59.231314, T: 9450000, Avg. loss: 58.517924\n",
      "Total training time: 206.46 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 384.35, NNZs: 8192, Bias: -8.602428, T: 9270000, Avg. loss: 62.977202\n",
      "Total training time: 206.55 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 373.16, NNZs: 8192, Bias: 49.524356, T: 9540000, Avg. loss: 38.193484\n",
      "Total training time: 206.62 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 405.45, NNZs: 8192, Bias: -61.789765, T: 9405000, Avg. loss: 65.371923\n",
      "Total training time: 206.72 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 466.15, NNZs: 8192, Bias: -50.849238, T: 9585000, Avg. loss: 47.682130\n",
      "Total training time: 206.71 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 405.20, NNZs: 8192, Bias: -5.438705, T: 9720000, Avg. loss: 32.913968\n",
      "Total training time: 206.77 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 387.14, NNZs: 8192, Bias: -19.554865, T: 9360000, Avg. loss: 73.143467\n",
      "Total training time: 207.03 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 392.58, NNZs: 8192, Bias: -59.232368, T: 9495000, Avg. loss: 58.032628\n",
      "Total training time: 207.11 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 383.59, NNZs: 8192, Bias: -8.603509, T: 9315000, Avg. loss: 62.579560\n",
      "Total training time: 207.21 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 372.30, NNZs: 8192, Bias: 49.525405, T: 9585000, Avg. loss: 38.763362\n",
      "Total training time: 207.27 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 404.76, NNZs: 8192, Bias: -61.792948, T: 9450000, Avg. loss: 63.604827\n",
      "Total training time: 207.37 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 464.98, NNZs: 8192, Bias: -50.853398, T: 9630000, Avg. loss: 47.319118\n",
      "Total training time: 207.35 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 404.23, NNZs: 8192, Bias: -5.439729, T: 9765000, Avg. loss: 31.787926\n",
      "Total training time: 207.42 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 386.29, NNZs: 8192, Bias: -19.558059, T: 9405000, Avg. loss: 73.017063\n",
      "Total training time: 207.71 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 391.81, NNZs: 8192, Bias: -59.238675, T: 9540000, Avg. loss: 59.058156\n",
      "Total training time: 207.79 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 371.48, NNZs: 8192, Bias: 49.528530, T: 9630000, Avg. loss: 38.236058\n",
      "Total training time: 207.92 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 382.83, NNZs: 8192, Bias: -8.606721, T: 9360000, Avg. loss: 61.737678\n",
      "Total training time: 207.89 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 463.83, NNZs: 8192, Bias: -50.856504, T: 9675000, Avg. loss: 47.076574\n",
      "Total training time: 208.00 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 403.92, NNZs: 8192, Bias: -61.792942, T: 9495000, Avg. loss: 63.926251\n",
      "Total training time: 208.04 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 403.44, NNZs: 8192, Bias: -5.440745, T: 9810000, Avg. loss: 31.494984\n",
      "Total training time: 208.06 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 385.59, NNZs: 8192, Bias: -19.559119, T: 9450000, Avg. loss: 72.522086\n",
      "Total training time: 208.39 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 391.08, NNZs: 8192, Bias: -59.242864, T: 9585000, Avg. loss: 57.982296\n",
      "Total training time: 208.46 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 370.60, NNZs: 8192, Bias: 49.529565, T: 9675000, Avg. loss: 37.645909\n",
      "Total training time: 208.57 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 382.07, NNZs: 8192, Bias: -8.606717, T: 9405000, Avg. loss: 61.145560\n",
      "Total training time: 208.54 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 462.66, NNZs: 8192, Bias: -50.858565, T: 9720000, Avg. loss: 47.125227\n",
      "Total training time: 208.65 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 403.18, NNZs: 8192, Bias: -61.793994, T: 9540000, Avg. loss: 63.354401\n",
      "Total training time: 208.69 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 402.52, NNZs: 8192, Bias: -5.436676, T: 9855000, Avg. loss: 30.616517\n",
      "Total training time: 208.68 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 384.71, NNZs: 8192, Bias: -19.559116, T: 9495000, Avg. loss: 72.040597\n",
      "Total training time: 209.02 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 390.25, NNZs: 8192, Bias: -59.247031, T: 9630000, Avg. loss: 58.152297\n",
      "Total training time: 209.09 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 369.81, NNZs: 8192, Bias: 49.531628, T: 9720000, Avg. loss: 38.033151\n",
      "Total training time: 209.21 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 381.44, NNZs: 8192, Bias: -8.605660, T: 9450000, Avg. loss: 60.633841\n",
      "Total training time: 209.20 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 461.55, NNZs: 8192, Bias: -50.861642, T: 9765000, Avg. loss: 46.504846\n",
      "Total training time: 209.27 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 401.55, NNZs: 8192, Bias: -5.437692, T: 9900000, Avg. loss: 30.923555\n",
      "Total training time: 209.29 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 402.35, NNZs: 8192, Bias: -61.792953, T: 9585000, Avg. loss: 64.078199\n",
      "Total training time: 209.34 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 384.03, NNZs: 8192, Bias: -19.557018, T: 9540000, Avg. loss: 71.373182\n",
      "Total training time: 209.65 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 389.39, NNZs: 8192, Bias: -59.249101, T: 9675000, Avg. loss: 57.098982\n",
      "Total training time: 209.72 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 369.12, NNZs: 8192, Bias: 49.534701, T: 9765000, Avg. loss: 37.734370\n",
      "Total training time: 209.83 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 380.66, NNZs: 8192, Bias: -8.607766, T: 9495000, Avg. loss: 60.470754\n",
      "Total training time: 209.84 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 460.45, NNZs: 8192, Bias: -50.861640, T: 9810000, Avg. loss: 46.755864\n",
      "Total training time: 209.87 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 400.65, NNZs: 8192, Bias: -5.440719, T: 9945000, Avg. loss: 30.931475\n",
      "Total training time: 209.89 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 401.57, NNZs: 8192, Bias: -61.797121, T: 9630000, Avg. loss: 62.787061\n",
      "Total training time: 209.96 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 388.65, NNZs: 8192, Bias: -59.253229, T: 9720000, Avg. loss: 57.531567\n",
      "Total training time: 210.35 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 383.32, NNZs: 8192, Bias: -19.559116, T: 9585000, Avg. loss: 71.304503\n",
      "Total training time: 210.31 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 368.29, NNZs: 8192, Bias: 49.535721, T: 9810000, Avg. loss: 36.628554\n",
      "Total training time: 210.47 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 379.94, NNZs: 8192, Bias: -8.605663, T: 9540000, Avg. loss: 60.988029\n",
      "Total training time: 210.51 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 399.77, NNZs: 8192, Bias: -5.438709, T: 9990000, Avg. loss: 30.471489\n",
      "Total training time: 210.51 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 459.43, NNZs: 8192, Bias: -50.864692, T: 9855000, Avg. loss: 45.317313\n",
      "Total training time: 210.51 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 400.92, NNZs: 8192, Bias: -61.799189, T: 9675000, Avg. loss: 64.055317\n",
      "Total training time: 210.63 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 387.80, NNZs: 8192, Bias: -59.257334, T: 9765000, Avg. loss: 58.076753\n",
      "Total training time: 210.98 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 382.50, NNZs: 8192, Bias: -19.561198, T: 9630000, Avg. loss: 71.098179\n",
      "Total training time: 210.96 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 367.54, NNZs: 8192, Bias: 49.537752, T: 9855000, Avg. loss: 37.026753\n",
      "Total training time: 211.13 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 398.95, NNZs: 8192, Bias: -5.438706, T: 10035000, Avg. loss: 30.893615\n",
      "Total training time: 211.14 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 458.36, NNZs: 8192, Bias: -50.864695, T: 9900000, Avg. loss: 45.424075\n",
      "Total training time: 211.15 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 379.15, NNZs: 8192, Bias: -8.608809, T: 9585000, Avg. loss: 60.233813\n",
      "Total training time: 211.17 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 400.16, NNZs: 8192, Bias: -61.800221, T: 9720000, Avg. loss: 61.982705\n",
      "Total training time: 211.28 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 387.04, NNZs: 8192, Bias: -59.260402, T: 9810000, Avg. loss: 57.118140\n",
      "Total training time: 211.65 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 381.78, NNZs: 8192, Bias: -19.563264, T: 9675000, Avg. loss: 69.277470\n",
      "Total training time: 211.63 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 366.68, NNZs: 8192, Bias: 49.538763, T: 9900000, Avg. loss: 36.794363\n",
      "Total training time: 211.77 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 398.11, NNZs: 8192, Bias: -5.440693, T: 10080000, Avg. loss: 30.428875\n",
      "Total training time: 211.78 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 457.24, NNZs: 8192, Bias: -50.866714, T: 9945000, Avg. loss: 46.064382\n",
      "Total training time: 211.79 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 378.48, NNZs: 8192, Bias: -8.610889, T: 9630000, Avg. loss: 59.574438\n",
      "Total training time: 211.84 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 399.34, NNZs: 8192, Bias: -61.800223, T: 9765000, Avg. loss: 62.525661\n",
      "Total training time: 211.94 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 386.24, NNZs: 8192, Bias: -59.266503, T: 9855000, Avg. loss: 56.057414\n",
      "Total training time: 212.31 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 381.07, NNZs: 8192, Bias: -19.564294, T: 9720000, Avg. loss: 69.678852\n",
      "Total training time: 212.31 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 365.90, NNZs: 8192, Bias: 49.540778, T: 9945000, Avg. loss: 36.772807\n",
      "Total training time: 212.43 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 397.24, NNZs: 8192, Bias: -5.441684, T: 10125000, Avg. loss: 29.616434\n",
      "Total training time: 212.41 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 456.12, NNZs: 8192, Bias: -50.870726, T: 9990000, Avg. loss: 45.375983\n",
      "Total training time: 212.44 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 377.79, NNZs: 8192, Bias: -8.609849, T: 9675000, Avg. loss: 60.373874\n",
      "Total training time: 212.50 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 398.60, NNZs: 8192, Bias: -61.802269, T: 9810000, Avg. loss: 61.620947\n",
      "Total training time: 212.60 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 385.47, NNZs: 8192, Bias: -59.270547, T: 9900000, Avg. loss: 56.121302\n",
      "Total training time: 212.97 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 380.35, NNZs: 8192, Bias: -19.564293, T: 9765000, Avg. loss: 69.476943\n",
      "Total training time: 212.98 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 396.35, NNZs: 8192, Bias: -5.443655, T: 10170000, Avg. loss: 30.122537\n",
      "Total training time: 213.06 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 365.12, NNZs: 8192, Bias: 49.541783, T: 9990000, Avg. loss: 36.896184\n",
      "Total training time: 213.10 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 455.15, NNZs: 8192, Bias: -50.869730, T: 10035000, Avg. loss: 44.219263\n",
      "Total training time: 213.11 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 377.08, NNZs: 8192, Bias: -8.610882, T: 9720000, Avg. loss: 58.556150\n",
      "Total training time: 213.21 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 397.83, NNZs: 8192, Bias: -61.803290, T: 9855000, Avg. loss: 61.502842\n",
      "Total training time: 213.31 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 384.65, NNZs: 8192, Bias: -59.272562, T: 9945000, Avg. loss: 56.710956\n",
      "Total training time: 213.67 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 379.67, NNZs: 8192, Bias: -19.564293, T: 9810000, Avg. loss: 69.394579\n",
      "Total training time: 213.69 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 395.45, NNZs: 8192, Bias: -5.443660, T: 10215000, Avg. loss: 29.162682\n",
      "Total training time: 213.71 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 364.37, NNZs: 8192, Bias: 49.541784, T: 10035000, Avg. loss: 36.354453\n",
      "Total training time: 213.78 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 454.17, NNZs: 8192, Bias: -50.871725, T: 10080000, Avg. loss: 45.069961\n",
      "Total training time: 213.77 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 376.44, NNZs: 8192, Bias: -8.609862, T: 9765000, Avg. loss: 58.734443\n",
      "Total training time: 213.88 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 397.05, NNZs: 8192, Bias: -61.805316, T: 9900000, Avg. loss: 60.704039\n",
      "Total training time: 213.95 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 383.86, NNZs: 8192, Bias: -59.276575, T: 9990000, Avg. loss: 54.893564\n",
      "Total training time: 214.30 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 394.67, NNZs: 8192, Bias: -5.443661, T: 10260000, Avg. loss: 29.692724\n",
      "Total training time: 214.33 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 378.94, NNZs: 8192, Bias: -19.564294, T: 9855000, Avg. loss: 69.769062\n",
      "Total training time: 214.34 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 363.60, NNZs: 8192, Bias: 49.545761, T: 10080000, Avg. loss: 37.033478\n",
      "Total training time: 214.42 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 453.19, NNZs: 8192, Bias: -50.874696, T: 10125000, Avg. loss: 44.001479\n",
      "Total training time: 214.40 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 375.66, NNZs: 8192, Bias: -8.609858, T: 9810000, Avg. loss: 57.787718\n",
      "Total training time: 214.56 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 396.23, NNZs: 8192, Bias: -61.806321, T: 9945000, Avg. loss: 60.585166\n",
      "Total training time: 214.57 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 383.11, NNZs: 8192, Bias: -59.276582, T: 10035000, Avg. loss: 55.596445\n",
      "Total training time: 214.92 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 393.84, NNZs: 8192, Bias: -5.442688, T: 10305000, Avg. loss: 28.784630\n",
      "Total training time: 214.92 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 378.18, NNZs: 8192, Bias: -19.565308, T: 9900000, Avg. loss: 68.757117\n",
      "Total training time: 214.96 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 452.18, NNZs: 8192, Bias: -50.876670, T: 10170000, Avg. loss: 44.122856\n",
      "Total training time: 215.00 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 362.82, NNZs: 8192, Bias: 49.547740, T: 10125000, Avg. loss: 36.433079\n",
      "Total training time: 215.05 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 395.67, NNZs: 8192, Bias: -61.807325, T: 9990000, Avg. loss: 59.002789\n",
      "Total training time: 215.20 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 374.94, NNZs: 8192, Bias: -8.610880, T: 9855000, Avg. loss: 57.961813\n",
      "Total training time: 215.20 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 382.37, NNZs: 8192, Bias: -59.282548, T: 10080000, Avg. loss: 55.257793\n",
      "Total training time: 215.55 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 393.13, NNZs: 8192, Bias: -5.443659, T: 10350000, Avg. loss: 29.329514\n",
      "Total training time: 215.54 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 377.51, NNZs: 8192, Bias: -19.562294, T: 9945000, Avg. loss: 68.570838\n",
      "Total training time: 215.60 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 451.16, NNZs: 8192, Bias: -50.879610, T: 10215000, Avg. loss: 44.021821\n",
      "Total training time: 215.62 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 362.07, NNZs: 8192, Bias: 49.546752, T: 10170000, Avg. loss: 36.463685\n",
      "Total training time: 215.70 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 394.91, NNZs: 8192, Bias: -61.813323, T: 10035000, Avg. loss: 59.235226\n",
      "Total training time: 215.82 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 374.23, NNZs: 8192, Bias: -8.610886, T: 9900000, Avg. loss: 58.674437\n",
      "Total training time: 215.87 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 392.28, NNZs: 8192, Bias: -5.444619, T: 10395000, Avg. loss: 29.328919\n",
      "Total training time: 216.14 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 381.54, NNZs: 8192, Bias: -59.288488, T: 10125000, Avg. loss: 55.323775\n",
      "Total training time: 216.19 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 376.80, NNZs: 8192, Bias: -19.564305, T: 9990000, Avg. loss: 67.296479\n",
      "Total training time: 216.24 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 450.09, NNZs: 8192, Bias: -50.882539, T: 10260000, Avg. loss: 43.919382\n",
      "Total training time: 216.23 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 361.35, NNZs: 8192, Bias: 49.551658, T: 10215000, Avg. loss: 35.791365\n",
      "Total training time: 216.33 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 394.21, NNZs: 8192, Bias: -61.813321, T: 10080000, Avg. loss: 59.507717\n",
      "Total training time: 216.45 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 373.53, NNZs: 8192, Bias: -8.612902, T: 9945000, Avg. loss: 57.245074\n",
      "Total training time: 216.52 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 391.49, NNZs: 8192, Bias: -5.446537, T: 10440000, Avg. loss: 28.943337\n",
      "Total training time: 216.74 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 380.83, NNZs: 8192, Bias: -59.293406, T: 10170000, Avg. loss: 53.718461\n",
      "Total training time: 216.81 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 449.18, NNZs: 8192, Bias: -50.882535, T: 10305000, Avg. loss: 43.336542\n",
      "Total training time: 216.83 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 376.18, NNZs: 8192, Bias: -19.566300, T: 10035000, Avg. loss: 67.619159\n",
      "Total training time: 216.87 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 360.61, NNZs: 8192, Bias: 49.555561, T: 10260000, Avg. loss: 35.866977\n",
      "Total training time: 216.96 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 393.55, NNZs: 8192, Bias: -61.816292, T: 10125000, Avg. loss: 58.584722\n",
      "Total training time: 217.08 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 372.99, NNZs: 8192, Bias: -8.612907, T: 9990000, Avg. loss: 57.792890\n",
      "Total training time: 217.18 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 390.69, NNZs: 8192, Bias: -5.445580, T: 10485000, Avg. loss: 28.621489\n",
      "Total training time: 217.36 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 380.07, NNZs: 8192, Bias: -59.296347, T: 10215000, Avg. loss: 53.772203\n",
      "Total training time: 217.44 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 448.17, NNZs: 8192, Bias: -50.886406, T: 10350000, Avg. loss: 43.399977\n",
      "Total training time: 217.45 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 375.44, NNZs: 8192, Bias: -19.566302, T: 10080000, Avg. loss: 68.007992\n",
      "Total training time: 217.51 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 359.89, NNZs: 8192, Bias: 49.556534, T: 10305000, Avg. loss: 35.619740\n",
      "Total training time: 217.60 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 392.83, NNZs: 8192, Bias: -61.818263, T: 10170000, Avg. loss: 59.535685\n",
      "Total training time: 217.71 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 372.29, NNZs: 8192, Bias: -8.611914, T: 10035000, Avg. loss: 56.672210\n",
      "Total training time: 217.82 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 389.92, NNZs: 8192, Bias: -5.445583, T: 10530000, Avg. loss: 28.341609\n",
      "Total training time: 217.97 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 379.29, NNZs: 8192, Bias: -59.298302, T: 10260000, Avg. loss: 53.586873\n",
      "Total training time: 218.08 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 447.21, NNZs: 8192, Bias: -50.890253, T: 10395000, Avg. loss: 43.141025\n",
      "Total training time: 218.07 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 374.69, NNZs: 8192, Bias: -19.568280, T: 10125000, Avg. loss: 67.398814\n",
      "Total training time: 218.16 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 359.10, NNZs: 8192, Bias: 49.558471, T: 10350000, Avg. loss: 35.312994\n",
      "Total training time: 218.24 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 392.08, NNZs: 8192, Bias: -61.820222, T: 10215000, Avg. loss: 59.278454\n",
      "Total training time: 218.35 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 371.55, NNZs: 8192, Bias: -8.613910, T: 10080000, Avg. loss: 56.585668\n",
      "Total training time: 218.46 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 389.09, NNZs: 8192, Bias: -5.441795, T: 10575000, Avg. loss: 28.755597\n",
      "Total training time: 218.58 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 378.55, NNZs: 8192, Bias: -59.304133, T: 10305000, Avg. loss: 53.405432\n",
      "Total training time: 218.71 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 446.31, NNZs: 8192, Bias: -50.890246, T: 10440000, Avg. loss: 42.704661\n",
      "Total training time: 218.68 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 374.13, NNZs: 8192, Bias: -19.568274, T: 10170000, Avg. loss: 65.954244\n",
      "Total training time: 218.79 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 358.36, NNZs: 8192, Bias: 49.560395, T: 10395000, Avg. loss: 35.371903\n",
      "Total training time: 218.86 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 391.34, NNZs: 8192, Bias: -61.819245, T: 10260000, Avg. loss: 57.903338\n",
      "Total training time: 218.98 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 370.81, NNZs: 8192, Bias: -8.615892, T: 10125000, Avg. loss: 56.502447\n",
      "Total training time: 219.11 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 388.32, NNZs: 8192, Bias: -5.445571, T: 10620000, Avg. loss: 28.521613\n",
      "Total training time: 219.20 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 445.36, NNZs: 8192, Bias: -50.890247, T: 10485000, Avg. loss: 43.083111\n",
      "Total training time: 219.31 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 377.81, NNZs: 8192, Bias: -59.307035, T: 10350000, Avg. loss: 53.546527\n",
      "Total training time: 219.36 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 373.38, NNZs: 8192, Bias: -19.569261, T: 10215000, Avg. loss: 66.355012\n",
      "Total training time: 219.44 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 357.63, NNZs: 8192, Bias: 49.561354, T: 10440000, Avg. loss: 35.141353\n",
      "Total training time: 219.49 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 390.65, NNZs: 8192, Bias: -61.822165, T: 10305000, Avg. loss: 58.676335\n",
      "Total training time: 219.62 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 370.12, NNZs: 8192, Bias: -8.615892, T: 10170000, Avg. loss: 56.071265\n",
      "Total training time: 219.73 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 387.48, NNZs: 8192, Bias: -5.446508, T: 10665000, Avg. loss: 28.456442\n",
      "Total training time: 219.80 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 444.38, NNZs: 8192, Bias: -50.892153, T: 10530000, Avg. loss: 42.258112\n",
      "Total training time: 219.93 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 377.15, NNZs: 8192, Bias: -59.313782, T: 10395000, Avg. loss: 52.743633\n",
      "Total training time: 220.00 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 356.95, NNZs: 8192, Bias: 49.562310, T: 10485000, Avg. loss: 34.085136\n",
      "Total training time: 220.12 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 372.68, NNZs: 8192, Bias: -19.571214, T: 10260000, Avg. loss: 64.866220\n",
      "Total training time: 220.09 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 389.98, NNZs: 8192, Bias: -61.822164, T: 10350000, Avg. loss: 57.801552\n",
      "Total training time: 220.27 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 369.47, NNZs: 8192, Bias: -8.616874, T: 10215000, Avg. loss: 55.991287\n",
      "Total training time: 220.39 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 386.63, NNZs: 8192, Bias: -5.446507, T: 10710000, Avg. loss: 28.061992\n",
      "Total training time: 220.44 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 443.46, NNZs: 8192, Bias: -50.895946, T: 10575000, Avg. loss: 42.269912\n",
      "Total training time: 220.56 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 376.58, NNZs: 8192, Bias: -59.317614, T: 10440000, Avg. loss: 52.192444\n",
      "Total training time: 220.63 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 356.26, NNZs: 8192, Bias: 49.564210, T: 10530000, Avg. loss: 34.845668\n",
      "Total training time: 220.74 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 371.99, NNZs: 8192, Bias: -19.572190, T: 10305000, Avg. loss: 65.343684\n",
      "Total training time: 220.75 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 389.29, NNZs: 8192, Bias: -61.825059, T: 10395000, Avg. loss: 58.223442\n",
      "Total training time: 220.91 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 368.81, NNZs: 8192, Bias: -8.614924, T: 10260000, Avg. loss: 55.731278\n",
      "Total training time: 221.03 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 385.80, NNZs: 8192, Bias: -5.447440, T: 10755000, Avg. loss: 27.864481\n",
      "Total training time: 221.04 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 442.52, NNZs: 8192, Bias: -50.896892, T: 10620000, Avg. loss: 41.552136\n",
      "Total training time: 221.20 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 375.80, NNZs: 8192, Bias: -59.318571, T: 10485000, Avg. loss: 51.822750\n",
      "Total training time: 221.27 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 355.56, NNZs: 8192, Bias: 49.565158, T: 10575000, Avg. loss: 34.596044\n",
      "Total training time: 221.37 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 371.33, NNZs: 8192, Bias: -19.574127, T: 10350000, Avg. loss: 65.094701\n",
      "Total training time: 221.40 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 388.64, NNZs: 8192, Bias: -61.827936, T: 10440000, Avg. loss: 57.641468\n",
      "Total training time: 221.56 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 384.92, NNZs: 8192, Bias: -5.448363, T: 10800000, Avg. loss: 27.424208\n",
      "Total training time: 221.66 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 368.17, NNZs: 8192, Bias: -8.615899, T: 10305000, Avg. loss: 56.354720\n",
      "Total training time: 221.68 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 441.55, NNZs: 8192, Bias: -50.897831, T: 10665000, Avg. loss: 41.919370\n",
      "Total training time: 221.81 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 375.09, NNZs: 8192, Bias: -59.323330, T: 10530000, Avg. loss: 51.573885\n",
      "Total training time: 221.88 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 354.92, NNZs: 8192, Bias: 49.567050, T: 10620000, Avg. loss: 33.697748\n",
      "Total training time: 221.99 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 370.61, NNZs: 8192, Bias: -19.574131, T: 10395000, Avg. loss: 65.337393\n",
      "Total training time: 222.07 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 387.98, NNZs: 8192, Bias: -61.830805, T: 10485000, Avg. loss: 57.677864\n",
      "Total training time: 222.24 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 384.18, NNZs: 8192, Bias: -5.449282, T: 10845000, Avg. loss: 27.292741\n",
      "Total training time: 222.34 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 367.52, NNZs: 8192, Bias: -8.615895, T: 10350000, Avg. loss: 55.030584\n",
      "Total training time: 222.38 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 440.60, NNZs: 8192, Bias: -50.901571, T: 10710000, Avg. loss: 40.987679\n",
      "Total training time: 222.50 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 374.47, NNZs: 8192, Bias: -59.326174, T: 10575000, Avg. loss: 52.211408\n",
      "Total training time: 222.55 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 354.17, NNZs: 8192, Bias: 49.570811, T: 10665000, Avg. loss: 33.642713\n",
      "Total training time: 222.65 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 369.98, NNZs: 8192, Bias: -19.576053, T: 10440000, Avg. loss: 63.612263\n",
      "Total training time: 222.72 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 387.31, NNZs: 8192, Bias: -61.831752, T: 10530000, Avg. loss: 56.644145\n",
      "Total training time: 222.86 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 383.37, NNZs: 8192, Bias: -5.450201, T: 10890000, Avg. loss: 27.432350\n",
      "Total training time: 222.93 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 366.82, NNZs: 8192, Bias: -8.618793, T: 10395000, Avg. loss: 55.258365\n",
      "Total training time: 223.01 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 439.72, NNZs: 8192, Bias: -50.900642, T: 10755000, Avg. loss: 42.638956\n",
      "Total training time: 223.10 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 373.78, NNZs: 8192, Bias: -59.331837, T: 10620000, Avg. loss: 52.324034\n",
      "Total training time: 223.18 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 353.51, NNZs: 8192, Bias: 49.574549, T: 10710000, Avg. loss: 33.564881\n",
      "Total training time: 223.27 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 369.31, NNZs: 8192, Bias: -19.576054, T: 10485000, Avg. loss: 65.112796\n",
      "Total training time: 223.38 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 386.67, NNZs: 8192, Bias: -61.833647, T: 10575000, Avg. loss: 56.459438\n",
      "Total training time: 223.54 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 382.53, NNZs: 8192, Bias: -5.451113, T: 10935000, Avg. loss: 27.079307\n",
      "Total training time: 223.58 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 366.15, NNZs: 8192, Bias: -8.622633, T: 10440000, Avg. loss: 54.453790\n",
      "Total training time: 223.69 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 438.76, NNZs: 8192, Bias: -50.903427, T: 10800000, Avg. loss: 40.330509\n",
      "Total training time: 223.75 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 373.12, NNZs: 8192, Bias: -59.334657, T: 10665000, Avg. loss: 51.125119\n",
      "Total training time: 223.84 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 352.86, NNZs: 8192, Bias: 49.576410, T: 10755000, Avg. loss: 33.732313\n",
      "Total training time: 223.92 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 368.71, NNZs: 8192, Bias: -19.576057, T: 10530000, Avg. loss: 64.292938\n",
      "Total training time: 224.05 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 385.95, NNZs: 8192, Bias: -61.836473, T: 10620000, Avg. loss: 56.578353\n",
      "Total training time: 224.18 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 381.80, NNZs: 8192, Bias: -5.451111, T: 10980000, Avg. loss: 27.381599\n",
      "Total training time: 224.20 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 365.60, NNZs: 8192, Bias: -8.622630, T: 10485000, Avg. loss: 54.686891\n",
      "Total training time: 224.36 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 437.86, NNZs: 8192, Bias: -50.905278, T: 10845000, Avg. loss: 40.845885\n",
      "Total training time: 224.40 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 372.48, NNZs: 8192, Bias: -59.339335, T: 10710000, Avg. loss: 51.084841\n",
      "Total training time: 224.50 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 352.20, NNZs: 8192, Bias: 49.577336, T: 10800000, Avg. loss: 32.740478\n",
      "Total training time: 224.58 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 368.05, NNZs: 8192, Bias: -19.576055, T: 10575000, Avg. loss: 64.576838\n",
      "Total training time: 224.72 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 385.22, NNZs: 8192, Bias: -61.840234, T: 10665000, Avg. loss: 56.183168\n",
      "Total training time: 224.82 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 380.99, NNZs: 8192, Bias: -5.452020, T: 11025000, Avg. loss: 26.935515\n",
      "Total training time: 224.82 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 365.00, NNZs: 8192, Bias: -8.622631, T: 10530000, Avg. loss: 53.764483\n",
      "Total training time: 225.03 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 436.83, NNZs: 8192, Bias: -50.907122, T: 10890000, Avg. loss: 40.689690\n",
      "Total training time: 225.02 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 371.75, NNZs: 8192, Bias: -59.343987, T: 10755000, Avg. loss: 50.141891\n",
      "Total training time: 225.13 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 351.56, NNZs: 8192, Bias: 49.574569, T: 10845000, Avg. loss: 33.364266\n",
      "Total training time: 225.21 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 367.43, NNZs: 8192, Bias: -19.579825, T: 10620000, Avg. loss: 64.088255\n",
      "Total training time: 225.38 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 380.26, NNZs: 8192, Bias: -5.453831, T: 11070000, Avg. loss: 26.893738\n",
      "Total training time: 225.43 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 384.54, NNZs: 8192, Bias: -61.841168, T: 10710000, Avg. loss: 56.780661\n",
      "Total training time: 225.46 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 435.82, NNZs: 8192, Bias: -50.910790, T: 10935000, Avg. loss: 40.762932\n",
      "Total training time: 225.64 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 364.30, NNZs: 8192, Bias: -8.624529, T: 10575000, Avg. loss: 53.629561\n",
      "Total training time: 225.68 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 371.00, NNZs: 8192, Bias: -59.345839, T: 10800000, Avg. loss: 51.370922\n",
      "Total training time: 225.77 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 350.91, NNZs: 8192, Bias: 49.577333, T: 10890000, Avg. loss: 33.324137\n",
      "Total training time: 225.86 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 366.78, NNZs: 8192, Bias: -19.578880, T: 10665000, Avg. loss: 62.500080\n",
      "Total training time: 226.02 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 379.51, NNZs: 8192, Bias: -5.453830, T: 11115000, Avg. loss: 26.364125\n",
      "Total training time: 226.04 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 383.92, NNZs: 8192, Bias: -61.843969, T: 10755000, Avg. loss: 55.479901\n",
      "Total training time: 226.08 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 434.86, NNZs: 8192, Bias: -50.911699, T: 10980000, Avg. loss: 40.069356\n",
      "Total training time: 226.24 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 363.65, NNZs: 8192, Bias: -8.625472, T: 10620000, Avg. loss: 53.976560\n",
      "Total training time: 226.34 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 370.34, NNZs: 8192, Bias: -59.349536, T: 10845000, Avg. loss: 50.214228\n",
      "Total training time: 226.39 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 350.20, NNZs: 8192, Bias: 49.580996, T: 10935000, Avg. loss: 32.834847\n",
      "Total training time: 226.48 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 378.72, NNZs: 8192, Bias: -5.453828, T: 11160000, Avg. loss: 27.251357\n",
      "Total training time: 226.66 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 366.17, NNZs: 8192, Bias: -19.578881, T: 10710000, Avg. loss: 63.368962\n",
      "Total training time: 226.67 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 383.22, NNZs: 8192, Bias: -61.845829, T: 10800000, Avg. loss: 55.169315\n",
      "Total training time: 226.72 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 433.89, NNZs: 8192, Bias: -50.911694, T: 11025000, Avg. loss: 40.392017\n",
      "Total training time: 226.86 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 369.61, NNZs: 8192, Bias: -59.353217, T: 10890000, Avg. loss: 50.497537\n",
      "Total training time: 227.03 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 363.00, NNZs: 8192, Bias: -8.620777, T: 10665000, Avg. loss: 53.251948\n",
      "Total training time: 227.01 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 349.53, NNZs: 8192, Bias: 49.580994, T: 10980000, Avg. loss: 33.109630\n",
      "Total training time: 227.13 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 377.95, NNZs: 8192, Bias: -5.452938, T: 11205000, Avg. loss: 26.305551\n",
      "Total training time: 227.27 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 365.52, NNZs: 8192, Bias: -19.577950, T: 10755000, Avg. loss: 62.690547\n",
      "Total training time: 227.31 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 382.52, NNZs: 8192, Bias: -61.849528, T: 10845000, Avg. loss: 55.992173\n",
      "Total training time: 227.35 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 433.02, NNZs: 8192, Bias: -50.914410, T: 11070000, Avg. loss: 40.005500\n",
      "Total training time: 227.48 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 368.95, NNZs: 8192, Bias: -59.358713, T: 10935000, Avg. loss: 49.862838\n",
      "Total training time: 227.65 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 362.35, NNZs: 8192, Bias: -8.622649, T: 10710000, Avg. loss: 52.949331\n",
      "Total training time: 227.66 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 348.92, NNZs: 8192, Bias: 49.584627, T: 11025000, Avg. loss: 32.615679\n",
      "Total training time: 227.77 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 377.22, NNZs: 8192, Bias: -5.452051, T: 11250000, Avg. loss: 26.657348\n",
      "Total training time: 227.87 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 364.90, NNZs: 8192, Bias: -19.578881, T: 10800000, Avg. loss: 62.248187\n",
      "Total training time: 227.94 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 381.83, NNZs: 8192, Bias: -61.853207, T: 10890000, Avg. loss: 54.063456\n",
      "Total training time: 227.98 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 432.09, NNZs: 8192, Bias: -50.916215, T: 11115000, Avg. loss: 39.770659\n",
      "Total training time: 228.09 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 368.31, NNZs: 8192, Bias: -59.364184, T: 10980000, Avg. loss: 49.244676\n",
      "Total training time: 228.28 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 361.79, NNZs: 8192, Bias: -8.622645, T: 10755000, Avg. loss: 52.551614\n",
      "Total training time: 228.32 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 348.28, NNZs: 8192, Bias: 49.584622, T: 11070000, Avg. loss: 32.957404\n",
      "Total training time: 228.41 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 376.47, NNZs: 8192, Bias: -5.453827, T: 11295000, Avg. loss: 26.213536\n",
      "Total training time: 228.49 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 364.27, NNZs: 8192, Bias: -19.580728, T: 10845000, Avg. loss: 61.456530\n",
      "Total training time: 228.57 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 381.20, NNZs: 8192, Bias: -61.851373, T: 10935000, Avg. loss: 55.030420\n",
      "Total training time: 228.61 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 431.19, NNZs: 8192, Bias: -50.916217, T: 11160000, Avg. loss: 39.400244\n",
      "Total training time: 228.72 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 367.68, NNZs: 8192, Bias: -59.366909, T: 11025000, Avg. loss: 50.019333\n",
      "Total training time: 228.92 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 361.13, NNZs: 8192, Bias: -8.622643, T: 10800000, Avg. loss: 52.590187\n",
      "Total training time: 228.96 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 347.63, NNZs: 8192, Bias: 49.584619, T: 11115000, Avg. loss: 32.875960\n",
      "Total training time: 229.02 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 375.77, NNZs: 8192, Bias: -5.453826, T: 11340000, Avg. loss: 26.137137\n",
      "Total training time: 229.11 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 363.68, NNZs: 8192, Bias: -19.580726, T: 10890000, Avg. loss: 61.177471\n",
      "Total training time: 229.23 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 380.53, NNZs: 8192, Bias: -61.853199, T: 10980000, Avg. loss: 54.155826\n",
      "Total training time: 229.25 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 430.29, NNZs: 8192, Bias: -50.921581, T: 11205000, Avg. loss: 39.980893\n",
      "Total training time: 229.37 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 367.10, NNZs: 8192, Bias: -59.368722, T: 11070000, Avg. loss: 49.489450\n",
      "Total training time: 229.54 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 360.49, NNZs: 8192, Bias: -8.621724, T: 10845000, Avg. loss: 52.733470\n",
      "Total training time: 229.59 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 346.99, NNZs: 8192, Bias: 49.585519, T: 11160000, Avg. loss: 32.618291\n",
      "Total training time: 229.63 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 375.01, NNZs: 8192, Bias: -5.452947, T: 11385000, Avg. loss: 25.880561\n",
      "Total training time: 229.72 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 379.92, NNZs: 8192, Bias: -61.856835, T: 11025000, Avg. loss: 53.114460\n",
      "Total training time: 229.88 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 363.09, NNZs: 8192, Bias: -19.580730, T: 10935000, Avg. loss: 62.076603\n",
      "Total training time: 229.86 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 429.42, NNZs: 8192, Bias: -50.921578, T: 11250000, Avg. loss: 39.062118\n",
      "Total training time: 229.98 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 366.47, NNZs: 8192, Bias: -59.372333, T: 11115000, Avg. loss: 49.257376\n",
      "Total training time: 230.19 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 346.35, NNZs: 8192, Bias: 49.587310, T: 11205000, Avg. loss: 32.637265\n",
      "Total training time: 230.27 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 359.85, NNZs: 8192, Bias: -8.623563, T: 10890000, Avg. loss: 52.296218\n",
      "Total training time: 230.26 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 374.29, NNZs: 8192, Bias: -5.455571, T: 11430000, Avg. loss: 26.419862\n",
      "Total training time: 230.35 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 379.30, NNZs: 8192, Bias: -61.857740, T: 11070000, Avg. loss: 54.147801\n",
      "Total training time: 230.55 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 362.56, NNZs: 8192, Bias: -19.585294, T: 10980000, Avg. loss: 61.304357\n",
      "Total training time: 230.54 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 428.53, NNZs: 8192, Bias: -50.922461, T: 11295000, Avg. loss: 39.363872\n",
      "Total training time: 230.62 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 365.78, NNZs: 8192, Bias: -59.376822, T: 11160000, Avg. loss: 48.912016\n",
      "Total training time: 230.85 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 345.65, NNZs: 8192, Bias: 49.589091, T: 11250000, Avg. loss: 32.113738\n",
      "Total training time: 230.95 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 359.25, NNZs: 8192, Bias: -8.623565, T: 10935000, Avg. loss: 51.894621\n",
      "Total training time: 230.95 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 373.55, NNZs: 8192, Bias: -5.453822, T: 11475000, Avg. loss: 26.125268\n",
      "Total training time: 231.02 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 378.69, NNZs: 8192, Bias: -61.860446, T: 11115000, Avg. loss: 52.879610\n",
      "Total training time: 231.24 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 361.91, NNZs: 8192, Bias: -19.584382, T: 11025000, Avg. loss: 60.958226\n",
      "Total training time: 231.25 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 427.74, NNZs: 8192, Bias: -50.923347, T: 11340000, Avg. loss: 39.106422\n",
      "Total training time: 231.30 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 365.12, NNZs: 8192, Bias: -59.378609, T: 11205000, Avg. loss: 48.453981\n",
      "Total training time: 231.52 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 345.01, NNZs: 8192, Bias: 49.591748, T: 11295000, Avg. loss: 32.651069\n",
      "Total training time: 231.61 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 358.57, NNZs: 8192, Bias: -8.627205, T: 10980000, Avg. loss: 52.124285\n",
      "Total training time: 231.62 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 372.84, NNZs: 8192, Bias: -5.457298, T: 11520000, Avg. loss: 25.773626\n",
      "Total training time: 231.65 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 378.04, NNZs: 8192, Bias: -61.862241, T: 11160000, Avg. loss: 53.533892\n",
      "Total training time: 231.88 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 361.34, NNZs: 8192, Bias: -19.585291, T: 11070000, Avg. loss: 61.366895\n",
      "Total training time: 231.89 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 426.87, NNZs: 8192, Bias: -50.927748, T: 11385000, Avg. loss: 39.429433\n",
      "Total training time: 231.92 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 364.49, NNZs: 8192, Bias: -59.381281, T: 11250000, Avg. loss: 48.132386\n",
      "Total training time: 232.15 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 344.37, NNZs: 8192, Bias: 49.590861, T: 11340000, Avg. loss: 31.852295\n",
      "Total training time: 232.23 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 358.01, NNZs: 8192, Bias: -8.625384, T: 11025000, Avg. loss: 51.297838\n",
      "Total training time: 232.29 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 372.10, NNZs: 8192, Bias: -5.455565, T: 11565000, Avg. loss: 25.243151\n",
      "Total training time: 232.28 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 377.45, NNZs: 8192, Bias: -61.864029, T: 11205000, Avg. loss: 53.435502\n",
      "Total training time: 232.52 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 360.71, NNZs: 8192, Bias: -19.588895, T: 11115000, Avg. loss: 61.335620\n",
      "Total training time: 232.54 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 426.00, NNZs: 8192, Bias: -50.928619, T: 11430000, Avg. loss: 38.446123\n",
      "Total training time: 232.55 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 363.89, NNZs: 8192, Bias: -59.382173, T: 11295000, Avg. loss: 48.222963\n",
      "Total training time: 232.77 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 343.73, NNZs: 8192, Bias: 49.592622, T: 11385000, Avg. loss: 31.992270\n",
      "Total training time: 232.86 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 371.33, NNZs: 8192, Bias: -5.456430, T: 11610000, Avg. loss: 25.361925\n",
      "Total training time: 232.87 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 357.39, NNZs: 8192, Bias: -8.626289, T: 11070000, Avg. loss: 51.479374\n",
      "Total training time: 232.92 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 376.91, NNZs: 8192, Bias: -61.867588, T: 11250000, Avg. loss: 51.892439\n",
      "Total training time: 233.15 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 360.15, NNZs: 8192, Bias: -19.590689, T: 11160000, Avg. loss: 60.964157\n",
      "Total training time: 233.17 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 425.13, NNZs: 8192, Bias: -50.928614, T: 11475000, Avg. loss: 37.845790\n",
      "Total training time: 233.17 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 363.26, NNZs: 8192, Bias: -59.386594, T: 11340000, Avg. loss: 47.910925\n",
      "Total training time: 233.41 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 343.15, NNZs: 8192, Bias: 49.595254, T: 11430000, Avg. loss: 30.960030\n",
      "Total training time: 233.49 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 370.65, NNZs: 8192, Bias: -5.456432, T: 11655000, Avg. loss: 25.061931\n",
      "Total training time: 233.50 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 356.84, NNZs: 8192, Bias: -8.625386, T: 11115000, Avg. loss: 51.273708\n",
      "Total training time: 233.58 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 376.28, NNZs: 8192, Bias: -61.866699, T: 11295000, Avg. loss: 52.301138\n",
      "Total training time: 233.82 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 424.22, NNZs: 8192, Bias: -50.930352, T: 11520000, Avg. loss: 37.772290\n",
      "Total training time: 233.83 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 359.54, NNZs: 8192, Bias: -19.590694, T: 11205000, Avg. loss: 60.858817\n",
      "Total training time: 233.85 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 362.59, NNZs: 8192, Bias: -59.388354, T: 11385000, Avg. loss: 48.313536\n",
      "Total training time: 234.07 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 342.53, NNZs: 8192, Bias: 49.597003, T: 11475000, Avg. loss: 30.574121\n",
      "Total training time: 234.16 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 369.98, NNZs: 8192, Bias: -5.457288, T: 11700000, Avg. loss: 25.098311\n",
      "Total training time: 234.14 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 356.26, NNZs: 8192, Bias: -8.628974, T: 11160000, Avg. loss: 51.927683\n",
      "Total training time: 234.26 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 375.67, NNZs: 8192, Bias: -61.868466, T: 11340000, Avg. loss: 52.146261\n",
      "Total training time: 234.48 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 423.41, NNZs: 8192, Bias: -50.932948, T: 11565000, Avg. loss: 37.923296\n",
      "Total training time: 234.47 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 358.94, NNZs: 8192, Bias: -19.594258, T: 11250000, Avg. loss: 59.911782\n",
      "Total training time: 234.51 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 361.97, NNZs: 8192, Bias: -59.391860, T: 11430000, Avg. loss: 47.692797\n",
      "Total training time: 234.72 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 341.90, NNZs: 8192, Bias: 49.597872, T: 11520000, Avg. loss: 31.010024\n",
      "Total training time: 234.79 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 369.32, NNZs: 8192, Bias: -5.455580, T: 11745000, Avg. loss: 25.349514\n",
      "Total training time: 234.76 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 355.65, NNZs: 8192, Bias: -8.628972, T: 11205000, Avg. loss: 51.058356\n",
      "Total training time: 234.91 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 422.65, NNZs: 8192, Bias: -50.935536, T: 11610000, Avg. loss: 37.240462\n",
      "Total training time: 235.09 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 375.06, NNZs: 8192, Bias: -61.870225, T: 11385000, Avg. loss: 51.845295\n",
      "Total training time: 235.12 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 358.40, NNZs: 8192, Bias: -19.593368, T: 11295000, Avg. loss: 59.347927\n",
      "Total training time: 235.15 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 361.34, NNZs: 8192, Bias: -59.395357, T: 11475000, Avg. loss: 46.594386\n",
      "Total training time: 235.37 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 368.62, NNZs: 8192, Bias: -5.456432, T: 11790000, Avg. loss: 25.292052\n",
      "Total training time: 235.39 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 341.26, NNZs: 8192, Bias: 49.599600, T: 11565000, Avg. loss: 30.831457\n",
      "Total training time: 235.44 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 355.03, NNZs: 8192, Bias: -8.628090, T: 11250000, Avg. loss: 49.946368\n",
      "Total training time: 235.59 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 421.84, NNZs: 8192, Bias: -50.938110, T: 11655000, Avg. loss: 36.480934\n",
      "Total training time: 235.73 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 374.44, NNZs: 8192, Bias: -61.871977, T: 11430000, Avg. loss: 52.478520\n",
      "Total training time: 235.78 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 357.81, NNZs: 8192, Bias: -19.591601, T: 11340000, Avg. loss: 59.608217\n",
      "Total training time: 235.82 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 360.67, NNZs: 8192, Bias: -59.398837, T: 11520000, Avg. loss: 47.334951\n",
      "Total training time: 236.00 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 367.96, NNZs: 8192, Bias: -5.456434, T: 11835000, Avg. loss: 24.665466\n",
      "Total training time: 235.99 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 340.65, NNZs: 8192, Bias: 49.602188, T: 11610000, Avg. loss: 30.371313\n",
      "Total training time: 236.09 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 354.35, NNZs: 8192, Bias: -8.628094, T: 11295000, Avg. loss: 50.559699\n",
      "Total training time: 236.26 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 420.94, NNZs: 8192, Bias: -50.938961, T: 11700000, Avg. loss: 37.660262\n",
      "Total training time: 236.35 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 373.90, NNZs: 8192, Bias: -61.872849, T: 11475000, Avg. loss: 51.820455\n",
      "Total training time: 236.42 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 357.28, NNZs: 8192, Bias: -19.592480, T: 11385000, Avg. loss: 58.629423\n",
      "Total training time: 236.45 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 360.08, NNZs: 8192, Bias: -59.403168, T: 11565000, Avg. loss: 46.709653\n",
      "Total training time: 236.63 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 367.26, NNZs: 8192, Bias: -5.454747, T: 11880000, Avg. loss: 24.766321\n",
      "Total training time: 236.61 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 340.04, NNZs: 8192, Bias: 49.605626, T: 11655000, Avg. loss: 30.809535\n",
      "Total training time: 236.74 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 353.74, NNZs: 8192, Bias: -8.633397, T: 11340000, Avg. loss: 50.141834\n",
      "Total training time: 236.91 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 420.07, NNZs: 8192, Bias: -50.938961, T: 11745000, Avg. loss: 36.968692\n",
      "Total training time: 236.97 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 373.34, NNZs: 8192, Bias: -61.873717, T: 11520000, Avg. loss: 51.081167\n",
      "Total training time: 237.05 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 356.69, NNZs: 8192, Bias: -19.593354, T: 11430000, Avg. loss: 58.754539\n",
      "Total training time: 237.10 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 366.52, NNZs: 8192, Bias: -5.454749, T: 11925000, Avg. loss: 24.245155\n",
      "Total training time: 237.23 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 359.51, NNZs: 8192, Bias: -59.407483, T: 11610000, Avg. loss: 47.408108\n",
      "Total training time: 237.27 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 339.44, NNZs: 8192, Bias: 49.606477, T: 11700000, Avg. loss: 30.416135\n",
      "Total training time: 237.38 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 353.18, NNZs: 8192, Bias: -8.630757, T: 11385000, Avg. loss: 49.719475\n",
      "Total training time: 237.57 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 419.26, NNZs: 8192, Bias: -50.940665, T: 11790000, Avg. loss: 36.725359\n",
      "Total training time: 237.58 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 372.75, NNZs: 8192, Bias: -61.875451, T: 11565000, Avg. loss: 50.976657\n",
      "Total training time: 237.69 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 356.10, NNZs: 8192, Bias: -19.594224, T: 11475000, Avg. loss: 57.633183\n",
      "Total training time: 237.74 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 365.81, NNZs: 8192, Bias: -5.453913, T: 11970000, Avg. loss: 24.967699\n",
      "Total training time: 237.85 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 358.87, NNZs: 8192, Bias: -59.410064, T: 11655000, Avg. loss: 46.131407\n",
      "Total training time: 237.90 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 338.85, NNZs: 8192, Bias: 49.606473, T: 11745000, Avg. loss: 30.776477\n",
      "Total training time: 238.02 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 418.42, NNZs: 8192, Bias: -50.942359, T: 11835000, Avg. loss: 36.710726\n",
      "Total training time: 238.19 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 352.59, NNZs: 8192, Bias: -8.630758, T: 11430000, Avg. loss: 50.018773\n",
      "Total training time: 238.24 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 372.17, NNZs: 8192, Bias: -61.878041, T: 11610000, Avg. loss: 50.884439\n",
      "Total training time: 238.33 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 355.58, NNZs: 8192, Bias: -19.595090, T: 11520000, Avg. loss: 57.372306\n",
      "Total training time: 238.38 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 365.14, NNZs: 8192, Bias: -5.455578, T: 12015000, Avg. loss: 24.989621\n",
      "Total training time: 238.44 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 358.29, NNZs: 8192, Bias: -59.412637, T: 11700000, Avg. loss: 46.078302\n",
      "Total training time: 238.51 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 338.25, NNZs: 8192, Bias: 49.608172, T: 11790000, Avg. loss: 30.325757\n",
      "Total training time: 238.65 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 417.63, NNZs: 8192, Bias: -50.945729, T: 11880000, Avg. loss: 36.808021\n",
      "Total training time: 238.78 seconds.\n",
      "Convergence after 264 epochs took 238.78 seconds\n",
      "Norm: 351.98, NNZs: 8192, Bias: -8.630762, T: 11475000, Avg. loss: 49.493779\n",
      "Total training time: 238.87 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 371.63, NNZs: 8192, Bias: -61.878039, T: 11655000, Avg. loss: 51.232484\n",
      "Total training time: 238.93 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 355.00, NNZs: 8192, Bias: -19.595090, T: 11565000, Avg. loss: 57.499120\n",
      "Total training time: 238.98 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 364.49, NNZs: 8192, Bias: -5.455576, T: 12060000, Avg. loss: 24.483705\n",
      "Total training time: 239.00 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 357.69, NNZs: 8192, Bias: -59.416903, T: 11745000, Avg. loss: 46.590270\n",
      "Total training time: 239.09 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 337.66, NNZs: 8192, Bias: 49.611558, T: 11835000, Avg. loss: 29.932137\n",
      "Total training time: 239.23 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 351.40, NNZs: 8192, Bias: -8.629890, T: 11520000, Avg. loss: 48.317746\n",
      "Total training time: 239.43 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 370.94, NNZs: 8192, Bias: -61.878048, T: 11700000, Avg. loss: 50.287308\n",
      "Total training time: 239.48 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 363.83, NNZs: 8192, Bias: -5.454749, T: 12105000, Avg. loss: 24.435781\n",
      "Total training time: 239.51 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 354.51, NNZs: 8192, Bias: -19.596814, T: 11610000, Avg. loss: 58.011797\n",
      "Total training time: 239.53 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 357.12, NNZs: 8192, Bias: -59.419450, T: 11790000, Avg. loss: 46.595162\n",
      "Total training time: 239.63 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 337.02, NNZs: 8192, Bias: 49.611563, T: 11880000, Avg. loss: 30.062717\n",
      "Total training time: 239.75 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 350.79, NNZs: 8192, Bias: -8.632487, T: 11565000, Avg. loss: 48.779102\n",
      "Total training time: 239.94 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 370.42, NNZs: 8192, Bias: -61.884870, T: 11745000, Avg. loss: 50.169220\n",
      "Total training time: 240.00 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 363.14, NNZs: 8192, Bias: -5.455574, T: 12150000, Avg. loss: 24.587501\n",
      "Total training time: 240.03 seconds.\n",
      "Convergence after 270 epochs took 240.03 seconds\n",
      "Norm: 353.97, NNZs: 8192, Bias: -19.597670, T: 11655000, Avg. loss: 56.648228\n",
      "Total training time: 240.05 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 356.46, NNZs: 8192, Bias: -59.422839, T: 11835000, Avg. loss: 46.443676\n",
      "Total training time: 240.16 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 336.43, NNZs: 8192, Bias: 49.615764, T: 11925000, Avg. loss: 29.709169\n",
      "Total training time: 240.24 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 350.30, NNZs: 8192, Bias: -8.632485, T: 11610000, Avg. loss: 48.562281\n",
      "Total training time: 240.44 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 369.82, NNZs: 8192, Bias: -61.884866, T: 11790000, Avg. loss: 49.310893\n",
      "Total training time: 240.48 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 353.36, NNZs: 8192, Bias: -19.598523, T: 11700000, Avg. loss: 56.266148\n",
      "Total training time: 240.52 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 355.84, NNZs: 8192, Bias: -59.425370, T: 11880000, Avg. loss: 46.637894\n",
      "Total training time: 240.63 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 335.82, NNZs: 8192, Bias: 49.617432, T: 11970000, Avg. loss: 29.830639\n",
      "Total training time: 240.69 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 349.76, NNZs: 8192, Bias: -8.632486, T: 11655000, Avg. loss: 48.363421\n",
      "Total training time: 240.89 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 369.27, NNZs: 8192, Bias: -61.887405, T: 11835000, Avg. loss: 49.506573\n",
      "Total training time: 240.92 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 352.81, NNZs: 8192, Bias: -19.599372, T: 11745000, Avg. loss: 56.363517\n",
      "Total training time: 240.97 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 355.22, NNZs: 8192, Bias: -59.429568, T: 11925000, Avg. loss: 45.264004\n",
      "Total training time: 241.09 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 335.25, NNZs: 8192, Bias: 49.617436, T: 12015000, Avg. loss: 29.963198\n",
      "Total training time: 241.14 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 349.16, NNZs: 8192, Bias: -8.631629, T: 11700000, Avg. loss: 47.657327\n",
      "Total training time: 241.37 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 368.66, NNZs: 8192, Bias: -61.889092, T: 11880000, Avg. loss: 49.312177\n",
      "Total training time: 241.40 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 352.27, NNZs: 8192, Bias: -19.597671, T: 11790000, Avg. loss: 56.001706\n",
      "Total training time: 241.45 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 354.66, NNZs: 8192, Bias: -59.433747, T: 11970000, Avg. loss: 44.635351\n",
      "Total training time: 241.56 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 334.68, NNZs: 8192, Bias: 49.618271, T: 12060000, Avg. loss: 29.715641\n",
      "Total training time: 241.59 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 348.54, NNZs: 8192, Bias: -8.632484, T: 11745000, Avg. loss: 47.800750\n",
      "Total training time: 241.83 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 368.05, NNZs: 8192, Bias: -61.889934, T: 11925000, Avg. loss: 49.696689\n",
      "Total training time: 241.85 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 351.70, NNZs: 8192, Bias: -19.602747, T: 11835000, Avg. loss: 56.622870\n",
      "Total training time: 241.91 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 354.04, NNZs: 8192, Bias: -59.435413, T: 12015000, Avg. loss: 44.862702\n",
      "Total training time: 242.00 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 334.04, NNZs: 8192, Bias: 49.618276, T: 12105000, Avg. loss: 29.975182\n",
      "Total training time: 242.03 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 347.96, NNZs: 8192, Bias: -8.630790, T: 11790000, Avg. loss: 47.763701\n",
      "Total training time: 242.29 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 367.38, NNZs: 8192, Bias: -61.891607, T: 11970000, Avg. loss: 49.107567\n",
      "Total training time: 242.31 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 351.12, NNZs: 8192, Bias: -19.601898, T: 11880000, Avg. loss: 56.673625\n",
      "Total training time: 242.37 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 353.43, NNZs: 8192, Bias: -59.436243, T: 12060000, Avg. loss: 44.919275\n",
      "Total training time: 242.46 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 333.49, NNZs: 8192, Bias: 49.620753, T: 12150000, Avg. loss: 29.463468\n",
      "Total training time: 242.48 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 347.41, NNZs: 8192, Bias: -8.632488, T: 11835000, Avg. loss: 47.509689\n",
      "Total training time: 242.75 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 366.82, NNZs: 8192, Bias: -61.892438, T: 12015000, Avg. loss: 49.210433\n",
      "Total training time: 242.76 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 350.53, NNZs: 8192, Bias: -19.601057, T: 11925000, Avg. loss: 55.362116\n",
      "Total training time: 242.82 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 332.87, NNZs: 8192, Bias: 49.621578, T: 12195000, Avg. loss: 28.880464\n",
      "Total training time: 242.90 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 352.84, NNZs: 8192, Bias: -59.438724, T: 12105000, Avg. loss: 45.458037\n",
      "Total training time: 242.91 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 346.89, NNZs: 8192, Bias: -8.633334, T: 11880000, Avg. loss: 47.779787\n",
      "Total training time: 243.19 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 366.28, NNZs: 8192, Bias: -61.894933, T: 12060000, Avg. loss: 48.481811\n",
      "Total training time: 243.20 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 350.05, NNZs: 8192, Bias: -19.602730, T: 11970000, Avg. loss: 56.366443\n",
      "Total training time: 243.28 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 332.28, NNZs: 8192, Bias: 49.624036, T: 12240000, Avg. loss: 29.133129\n",
      "Total training time: 243.35 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 352.31, NNZs: 8192, Bias: -59.441197, T: 12150000, Avg. loss: 44.988276\n",
      "Total training time: 243.37 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 365.62, NNZs: 8192, Bias: -61.895762, T: 12105000, Avg. loss: 49.057287\n",
      "Total training time: 243.65 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 346.33, NNZs: 8192, Bias: -8.635013, T: 11925000, Avg. loss: 47.139398\n",
      "Total training time: 243.65 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 349.54, NNZs: 8192, Bias: -19.600233, T: 12015000, Avg. loss: 55.303704\n",
      "Total training time: 243.74 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 331.72, NNZs: 8192, Bias: 49.625666, T: 12285000, Avg. loss: 29.240295\n",
      "Total training time: 243.79 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 351.74, NNZs: 8192, Bias: -59.445305, T: 12195000, Avg. loss: 43.954962\n",
      "Total training time: 243.80 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 345.81, NNZs: 8192, Bias: -8.634181, T: 11970000, Avg. loss: 46.901988\n",
      "Total training time: 244.09 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 365.10, NNZs: 8192, Bias: -61.899059, T: 12150000, Avg. loss: 47.724413\n",
      "Total training time: 244.10 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 331.16, NNZs: 8192, Bias: 49.624852, T: 12330000, Avg. loss: 28.863948\n",
      "Total training time: 244.21 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 348.94, NNZs: 8192, Bias: -19.601064, T: 12060000, Avg. loss: 55.731966\n",
      "Total training time: 244.18 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 351.17, NNZs: 8192, Bias: -59.446126, T: 12240000, Avg. loss: 44.383520\n",
      "Total training time: 244.25 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 345.26, NNZs: 8192, Bias: -8.635848, T: 12015000, Avg. loss: 46.910731\n",
      "Total training time: 244.55 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 364.61, NNZs: 8192, Bias: -61.899058, T: 12195000, Avg. loss: 47.730889\n",
      "Total training time: 244.56 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 330.64, NNZs: 8192, Bias: 49.627282, T: 12375000, Avg. loss: 28.421683\n",
      "Total training time: 244.66 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 348.44, NNZs: 8192, Bias: -19.602718, T: 12105000, Avg. loss: 54.237160\n",
      "Total training time: 244.64 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 350.63, NNZs: 8192, Bias: -59.449390, T: 12285000, Avg. loss: 43.940758\n",
      "Total training time: 244.71 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 364.15, NNZs: 8192, Bias: -61.902331, T: 12240000, Avg. loss: 47.732231\n",
      "Total training time: 245.01 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 344.72, NNZs: 8192, Bias: -8.636678, T: 12060000, Avg. loss: 46.966288\n",
      "Total training time: 245.00 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 330.06, NNZs: 8192, Bias: 49.630506, T: 12420000, Avg. loss: 29.290081\n",
      "Total training time: 245.09 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 347.95, NNZs: 8192, Bias: -19.601894, T: 12150000, Avg. loss: 54.894334\n",
      "Total training time: 245.09 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 350.04, NNZs: 8192, Bias: -59.451013, T: 12330000, Avg. loss: 44.276015\n",
      "Total training time: 245.15 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 363.58, NNZs: 8192, Bias: -61.905593, T: 12285000, Avg. loss: 48.529364\n",
      "Total training time: 245.45 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 344.16, NNZs: 8192, Bias: -8.636673, T: 12105000, Avg. loss: 46.537549\n",
      "Total training time: 245.46 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 329.50, NNZs: 8192, Bias: 49.632113, T: 12465000, Avg. loss: 28.297806\n",
      "Total training time: 245.54 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 347.41, NNZs: 8192, Bias: -19.603539, T: 12195000, Avg. loss: 54.390942\n",
      "Total training time: 245.55 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 349.52, NNZs: 8192, Bias: -59.454252, T: 12375000, Avg. loss: 43.774679\n",
      "Total training time: 245.60 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 362.99, NNZs: 8192, Bias: -61.907218, T: 12330000, Avg. loss: 47.970676\n",
      "Total training time: 245.90 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 343.59, NNZs: 8192, Bias: -8.640792, T: 12150000, Avg. loss: 46.323579\n",
      "Total training time: 245.92 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 328.96, NNZs: 8192, Bias: 49.632113, T: 12510000, Avg. loss: 28.354785\n",
      "Total training time: 245.98 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 346.90, NNZs: 8192, Bias: -19.603538, T: 12240000, Avg. loss: 53.996861\n",
      "Total training time: 245.99 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 348.91, NNZs: 8192, Bias: -59.460702, T: 12420000, Avg. loss: 42.619086\n",
      "Total training time: 246.05 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 362.45, NNZs: 8192, Bias: -61.907219, T: 12375000, Avg. loss: 47.358887\n",
      "Total training time: 246.35 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 343.09, NNZs: 8192, Bias: -8.639968, T: 12195000, Avg. loss: 46.344018\n",
      "Total training time: 246.38 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 328.44, NNZs: 8192, Bias: 49.633709, T: 12555000, Avg. loss: 28.435824\n",
      "Total training time: 246.42 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 346.35, NNZs: 8192, Bias: -19.605170, T: 12285000, Avg. loss: 54.572753\n",
      "Total training time: 246.44 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 348.31, NNZs: 8192, Bias: -59.463114, T: 12465000, Avg. loss: 43.071004\n",
      "Total training time: 246.50 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 361.91, NNZs: 8192, Bias: -61.908026, T: 12420000, Avg. loss: 46.776652\n",
      "Total training time: 246.80 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 327.91, NNZs: 8192, Bias: 49.635298, T: 12600000, Avg. loss: 28.244792\n",
      "Total training time: 246.86 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 342.55, NNZs: 8192, Bias: -8.639972, T: 12240000, Avg. loss: 46.329366\n",
      "Total training time: 246.84 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 345.76, NNZs: 8192, Bias: -19.605984, T: 12330000, Avg. loss: 54.156361\n",
      "Total training time: 246.89 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 347.74, NNZs: 8192, Bias: -59.466315, T: 12510000, Avg. loss: 42.557061\n",
      "Total training time: 246.93 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 361.44, NNZs: 8192, Bias: -61.910436, T: 12465000, Avg. loss: 47.445601\n",
      "Total training time: 247.24 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 327.32, NNZs: 8192, Bias: 49.636094, T: 12645000, Avg. loss: 28.114792\n",
      "Total training time: 247.30 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 341.98, NNZs: 8192, Bias: -8.642420, T: 12285000, Avg. loss: 45.871525\n",
      "Total training time: 247.29 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 347.17, NNZs: 8192, Bias: -59.469505, T: 12555000, Avg. loss: 43.183544\n",
      "Total training time: 247.36 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 345.25, NNZs: 8192, Bias: -19.605983, T: 12375000, Avg. loss: 53.373826\n",
      "Total training time: 247.34 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 360.88, NNZs: 8192, Bias: -61.908036, T: 12510000, Avg. loss: 46.311581\n",
      "Total training time: 247.69 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 326.79, NNZs: 8192, Bias: 49.636883, T: 12690000, Avg. loss: 27.637234\n",
      "Total training time: 247.75 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 341.41, NNZs: 8192, Bias: -8.644043, T: 12330000, Avg. loss: 44.874649\n",
      "Total training time: 247.75 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 346.63, NNZs: 8192, Bias: -59.471092, T: 12600000, Avg. loss: 43.548931\n",
      "Total training time: 247.81 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 344.78, NNZs: 8192, Bias: -19.607599, T: 12420000, Avg. loss: 52.836129\n",
      "Total training time: 247.79 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 360.41, NNZs: 8192, Bias: -61.912026, T: 12555000, Avg. loss: 46.193836\n",
      "Total training time: 248.13 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 326.27, NNZs: 8192, Bias: 49.637668, T: 12735000, Avg. loss: 27.612879\n",
      "Total training time: 248.19 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 340.87, NNZs: 8192, Bias: -8.641615, T: 12375000, Avg. loss: 45.365630\n",
      "Total training time: 248.20 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 346.10, NNZs: 8192, Bias: -59.474261, T: 12645000, Avg. loss: 42.534678\n",
      "Total training time: 248.25 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 344.24, NNZs: 8192, Bias: -19.607604, T: 12465000, Avg. loss: 53.086098\n",
      "Total training time: 248.23 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 359.88, NNZs: 8192, Bias: -61.913616, T: 12600000, Avg. loss: 46.146679\n",
      "Total training time: 248.58 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 325.70, NNZs: 8192, Bias: 49.640801, T: 12780000, Avg. loss: 28.248357\n",
      "Total training time: 248.63 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 345.58, NNZs: 8192, Bias: -59.478207, T: 12690000, Avg. loss: 42.196743\n",
      "Total training time: 248.70 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 340.34, NNZs: 8192, Bias: -8.644839, T: 12420000, Avg. loss: 44.539197\n",
      "Total training time: 248.68 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 343.76, NNZs: 8192, Bias: -19.608407, T: 12510000, Avg. loss: 53.059393\n",
      "Total training time: 248.70 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 359.35, NNZs: 8192, Bias: -61.915203, T: 12645000, Avg. loss: 46.373993\n",
      "Total training time: 249.07 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 325.15, NNZs: 8192, Bias: 49.640797, T: 12825000, Avg. loss: 27.680643\n",
      "Total training time: 249.12 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 344.99, NNZs: 8192, Bias: -59.478992, T: 12735000, Avg. loss: 42.962559\n",
      "Total training time: 249.19 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 339.75, NNZs: 8192, Bias: -8.644031, T: 12465000, Avg. loss: 45.178774\n",
      "Total training time: 249.18 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 343.27, NNZs: 8192, Bias: -19.610804, T: 12555000, Avg. loss: 53.314334\n",
      "Total training time: 249.19 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 358.81, NNZs: 8192, Bias: -61.917571, T: 12690000, Avg. loss: 45.889497\n",
      "Total training time: 249.56 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 324.61, NNZs: 8192, Bias: 49.640793, T: 12870000, Avg. loss: 27.482790\n",
      "Total training time: 249.60 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 344.47, NNZs: 8192, Bias: -59.481345, T: 12780000, Avg. loss: 41.091188\n",
      "Total training time: 249.66 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 339.25, NNZs: 8192, Bias: -8.643228, T: 12510000, Avg. loss: 44.846920\n",
      "Total training time: 249.67 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 342.80, NNZs: 8192, Bias: -19.612391, T: 12600000, Avg. loss: 52.457564\n",
      "Total training time: 249.67 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 358.31, NNZs: 8192, Bias: -61.919142, T: 12735000, Avg. loss: 45.919470\n",
      "Total training time: 250.01 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 324.03, NNZs: 8192, Bias: 49.640794, T: 12915000, Avg. loss: 27.785391\n",
      "Total training time: 250.05 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 343.98, NNZs: 8192, Bias: -59.484470, T: 12825000, Avg. loss: 42.189810\n",
      "Total training time: 250.11 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 338.74, NNZs: 8192, Bias: -8.646417, T: 12555000, Avg. loss: 44.309986\n",
      "Total training time: 250.13 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 342.31, NNZs: 8192, Bias: -19.611599, T: 12645000, Avg. loss: 51.743663\n",
      "Total training time: 250.13 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 357.72, NNZs: 8192, Bias: -61.922278, T: 12780000, Avg. loss: 45.460031\n",
      "Total training time: 250.50 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 323.53, NNZs: 8192, Bias: 49.643113, T: 12960000, Avg. loss: 27.265818\n",
      "Total training time: 250.55 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 343.42, NNZs: 8192, Bias: -59.487583, T: 12870000, Avg. loss: 42.634413\n",
      "Total training time: 250.63 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 338.23, NNZs: 8192, Bias: -8.648007, T: 12600000, Avg. loss: 44.748939\n",
      "Total training time: 250.66 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 341.79, NNZs: 8192, Bias: -19.612384, T: 12690000, Avg. loss: 51.918237\n",
      "Total training time: 250.67 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 357.20, NNZs: 8192, Bias: -61.923061, T: 12825000, Avg. loss: 45.636085\n",
      "Total training time: 251.05 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 323.03, NNZs: 8192, Bias: 49.645423, T: 13005000, Avg. loss: 26.979401\n",
      "Total training time: 251.09 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 342.94, NNZs: 8192, Bias: -59.491460, T: 12915000, Avg. loss: 42.379540\n",
      "Total training time: 251.19 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 337.70, NNZs: 8192, Bias: -8.648003, T: 12645000, Avg. loss: 43.618598\n",
      "Total training time: 251.23 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 341.27, NNZs: 8192, Bias: -19.612381, T: 12735000, Avg. loss: 51.502258\n",
      "Total training time: 251.22 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 322.53, NNZs: 8192, Bias: 49.646191, T: 13050000, Avg. loss: 27.159850\n",
      "Total training time: 251.59 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 356.67, NNZs: 8192, Bias: -61.923844, T: 12870000, Avg. loss: 45.392960\n",
      "Total training time: 251.58 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 342.45, NNZs: 8192, Bias: -59.493778, T: 12960000, Avg. loss: 41.672553\n",
      "Total training time: 251.71 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 337.13, NNZs: 8192, Bias: -8.648003, T: 12690000, Avg. loss: 44.065774\n",
      "Total training time: 251.74 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 340.72, NNZs: 8192, Bias: -19.611596, T: 12780000, Avg. loss: 52.096556\n",
      "Total training time: 251.75 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 322.04, NNZs: 8192, Bias: 49.648482, T: 13095000, Avg. loss: 26.779946\n",
      "Total training time: 252.11 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 356.13, NNZs: 8192, Bias: -61.926170, T: 12915000, Avg. loss: 45.536674\n",
      "Total training time: 252.11 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 341.89, NNZs: 8192, Bias: -59.496089, T: 13005000, Avg. loss: 41.121038\n",
      "Total training time: 252.24 seconds.\n",
      "Convergence after 289 epochs took 252.24 seconds\n",
      "Norm: 336.65, NNZs: 8192, Bias: -8.647218, T: 12735000, Avg. loss: 43.569751\n",
      "Total training time: 252.27 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 340.26, NNZs: 8192, Bias: -19.611598, T: 12825000, Avg. loss: 51.928317\n",
      "Total training time: 252.27 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 321.52, NNZs: 8192, Bias: 49.648483, T: 13140000, Avg. loss: 26.435937\n",
      "Total training time: 252.56 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 355.65, NNZs: 8192, Bias: -61.927714, T: 12960000, Avg. loss: 45.147408\n",
      "Total training time: 252.58 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 336.21, NNZs: 8192, Bias: -8.648790, T: 12780000, Avg. loss: 43.975300\n",
      "Total training time: 252.72 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 339.77, NNZs: 8192, Bias: -19.613935, T: 12870000, Avg. loss: 51.053290\n",
      "Total training time: 252.73 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 321.04, NNZs: 8192, Bias: 49.651519, T: 13185000, Avg. loss: 26.709378\n",
      "Total training time: 252.99 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 355.12, NNZs: 8192, Bias: -61.927712, T: 13005000, Avg. loss: 44.532333\n",
      "Total training time: 253.04 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 335.72, NNZs: 8192, Bias: -8.648011, T: 12825000, Avg. loss: 43.979135\n",
      "Total training time: 253.18 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 339.29, NNZs: 8192, Bias: -19.613162, T: 12915000, Avg. loss: 51.088747\n",
      "Total training time: 253.20 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 320.50, NNZs: 8192, Bias: 49.650763, T: 13230000, Avg. loss: 27.020403\n",
      "Total training time: 253.44 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 354.59, NNZs: 8192, Bias: -61.930015, T: 13050000, Avg. loss: 44.523902\n",
      "Total training time: 253.51 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 335.30, NNZs: 8192, Bias: -8.651122, T: 12870000, Avg. loss: 43.485441\n",
      "Total training time: 253.63 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 338.77, NNZs: 8192, Bias: -19.614707, T: 12960000, Avg. loss: 50.361039\n",
      "Total training time: 253.67 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 319.95, NNZs: 8192, Bias: 49.652273, T: 13275000, Avg. loss: 26.586413\n",
      "Total training time: 253.88 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 354.02, NNZs: 8192, Bias: -61.930014, T: 13095000, Avg. loss: 44.536575\n",
      "Total training time: 253.97 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 334.79, NNZs: 8192, Bias: -8.651126, T: 12915000, Avg. loss: 43.262332\n",
      "Total training time: 254.14 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 338.26, NNZs: 8192, Bias: -19.612396, T: 13005000, Avg. loss: 49.902373\n",
      "Total training time: 254.16 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 319.43, NNZs: 8192, Bias: 49.654524, T: 13320000, Avg. loss: 26.196012\n",
      "Total training time: 254.36 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 353.56, NNZs: 8192, Bias: -61.931538, T: 13140000, Avg. loss: 44.155696\n",
      "Total training time: 254.47 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 334.30, NNZs: 8192, Bias: -8.652675, T: 12960000, Avg. loss: 42.505973\n",
      "Total training time: 254.61 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 337.78, NNZs: 8192, Bias: -19.613934, T: 13050000, Avg. loss: 50.598150\n",
      "Total training time: 254.63 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 318.95, NNZs: 8192, Bias: 49.654527, T: 13365000, Avg. loss: 26.391311\n",
      "Total training time: 254.81 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 353.03, NNZs: 8192, Bias: -61.932297, T: 13185000, Avg. loss: 44.616947\n",
      "Total training time: 254.93 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 333.79, NNZs: 8192, Bias: -8.652680, T: 13005000, Avg. loss: 42.874704\n",
      "Total training time: 255.06 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 337.28, NNZs: 8192, Bias: -19.613933, T: 13095000, Avg. loss: 50.107035\n",
      "Total training time: 255.09 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 318.49, NNZs: 8192, Bias: 49.655274, T: 13410000, Avg. loss: 26.647787\n",
      "Total training time: 255.27 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 352.60, NNZs: 8192, Bias: -61.936838, T: 13230000, Avg. loss: 43.081680\n",
      "Total training time: 255.39 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 333.32, NNZs: 8192, Bias: -8.651915, T: 13050000, Avg. loss: 42.654786\n",
      "Total training time: 255.52 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 336.82, NNZs: 8192, Bias: -19.616220, T: 13140000, Avg. loss: 49.832543\n",
      "Total training time: 255.57 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 318.00, NNZs: 8192, Bias: 49.656017, T: 13455000, Avg. loss: 25.793978\n",
      "Total training time: 255.70 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 352.13, NNZs: 8192, Bias: -61.936083, T: 13275000, Avg. loss: 43.464925\n",
      "Total training time: 255.85 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 332.84, NNZs: 8192, Bias: -8.653446, T: 13095000, Avg. loss: 42.564450\n",
      "Total training time: 255.96 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 336.35, NNZs: 8192, Bias: -19.618499, T: 13185000, Avg. loss: 50.098455\n",
      "Total training time: 256.00 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 317.56, NNZs: 8192, Bias: 49.657501, T: 13500000, Avg. loss: 25.881351\n",
      "Total training time: 256.11 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 351.61, NNZs: 8192, Bias: -61.936082, T: 13320000, Avg. loss: 43.479737\n",
      "Total training time: 256.26 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 332.38, NNZs: 8192, Bias: -8.655733, T: 13140000, Avg. loss: 42.320807\n",
      "Total training time: 256.37 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 335.91, NNZs: 8192, Bias: -19.616981, T: 13230000, Avg. loss: 49.449405\n",
      "Total training time: 256.42 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 317.06, NNZs: 8192, Bias: 49.658980, T: 13545000, Avg. loss: 25.752587\n",
      "Total training time: 256.52 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 351.09, NNZs: 8192, Bias: -61.938326, T: 13365000, Avg. loss: 43.716889\n",
      "Total training time: 256.67 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 331.94, NNZs: 8192, Bias: -8.657250, T: 13185000, Avg. loss: 42.101007\n",
      "Total training time: 256.77 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 335.43, NNZs: 8192, Bias: -19.615471, T: 13275000, Avg. loss: 49.149839\n",
      "Total training time: 256.82 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 316.63, NNZs: 8192, Bias: 49.659715, T: 13590000, Avg. loss: 25.599776\n",
      "Total training time: 256.90 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 350.64, NNZs: 8192, Bias: -61.938324, T: 13410000, Avg. loss: 43.061897\n",
      "Total training time: 257.07 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 331.45, NNZs: 8192, Bias: -8.656495, T: 13230000, Avg. loss: 42.395261\n",
      "Total training time: 257.18 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 334.99, NNZs: 8192, Bias: -19.615475, T: 13320000, Avg. loss: 49.608643\n",
      "Total training time: 257.24 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 316.17, NNZs: 8192, Bias: 49.658982, T: 13635000, Avg. loss: 25.515570\n",
      "Total training time: 257.31 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 350.17, NNZs: 8192, Bias: -61.940558, T: 13455000, Avg. loss: 42.475531\n",
      "Total training time: 257.48 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 330.97, NNZs: 8192, Bias: -8.658005, T: 13275000, Avg. loss: 41.660751\n",
      "Total training time: 257.59 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 334.54, NNZs: 8192, Bias: -19.616972, T: 13365000, Avg. loss: 49.226821\n",
      "Total training time: 257.65 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 315.69, NNZs: 8192, Bias: 49.660449, T: 13680000, Avg. loss: 25.886221\n",
      "Total training time: 257.71 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 349.67, NNZs: 8192, Bias: -61.942784, T: 13500000, Avg. loss: 42.663215\n",
      "Total training time: 257.88 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 330.51, NNZs: 8192, Bias: -8.660259, T: 13320000, Avg. loss: 41.359120\n",
      "Total training time: 257.99 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 315.20, NNZs: 8192, Bias: 49.662639, T: 13725000, Avg. loss: 25.931365\n",
      "Total training time: 258.10 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 334.03, NNZs: 8192, Bias: -19.618466, T: 13410000, Avg. loss: 48.446397\n",
      "Total training time: 258.06 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 349.20, NNZs: 8192, Bias: -61.945002, T: 13545000, Avg. loss: 41.681662\n",
      "Total training time: 258.29 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 330.03, NNZs: 8192, Bias: -8.659512, T: 13365000, Avg. loss: 41.605506\n",
      "Total training time: 258.42 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 314.69, NNZs: 8192, Bias: 49.664822, T: 13770000, Avg. loss: 25.772627\n",
      "Total training time: 258.50 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 333.60, NNZs: 8192, Bias: -19.617720, T: 13455000, Avg. loss: 49.303666\n",
      "Total training time: 258.49 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 348.72, NNZs: 8192, Bias: -61.946477, T: 13590000, Avg. loss: 42.678109\n",
      "Total training time: 258.69 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 329.61, NNZs: 8192, Bias: -8.661006, T: 13410000, Avg. loss: 40.801719\n",
      "Total training time: 258.83 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 314.21, NNZs: 8192, Bias: 49.666272, T: 13815000, Avg. loss: 25.411280\n",
      "Total training time: 258.90 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 333.15, NNZs: 8192, Bias: -19.619946, T: 13500000, Avg. loss: 48.369020\n",
      "Total training time: 258.89 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 348.27, NNZs: 8192, Bias: -61.946480, T: 13635000, Avg. loss: 41.703806\n",
      "Total training time: 259.09 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 329.10, NNZs: 8192, Bias: -8.661003, T: 13455000, Avg. loss: 41.041473\n",
      "Total training time: 259.24 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 313.73, NNZs: 8192, Bias: 49.667716, T: 13860000, Avg. loss: 25.238703\n",
      "Total training time: 259.29 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 332.69, NNZs: 8192, Bias: -19.619206, T: 13545000, Avg. loss: 48.569961\n",
      "Total training time: 259.31 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 347.75, NNZs: 8192, Bias: -61.948679, T: 13680000, Avg. loss: 41.600054\n",
      "Total training time: 259.54 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 328.66, NNZs: 8192, Bias: -8.661002, T: 13500000, Avg. loss: 40.960113\n",
      "Total training time: 259.69 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 313.28, NNZs: 8192, Bias: 49.669157, T: 13905000, Avg. loss: 24.969091\n",
      "Total training time: 259.73 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 332.25, NNZs: 8192, Bias: -19.620681, T: 13590000, Avg. loss: 48.172570\n",
      "Total training time: 259.75 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 347.31, NNZs: 8192, Bias: -61.947950, T: 13725000, Avg. loss: 41.583018\n",
      "Total training time: 259.94 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 312.79, NNZs: 8192, Bias: 49.669159, T: 13950000, Avg. loss: 25.691551\n",
      "Total training time: 260.14 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 328.19, NNZs: 8192, Bias: -8.662481, T: 13545000, Avg. loss: 41.000013\n",
      "Total training time: 260.11 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 331.78, NNZs: 8192, Bias: -19.622152, T: 13635000, Avg. loss: 48.131018\n",
      "Total training time: 260.18 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 346.88, NNZs: 8192, Bias: -61.950133, T: 13770000, Avg. loss: 41.876188\n",
      "Total training time: 260.37 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 312.32, NNZs: 8192, Bias: 49.670591, T: 13995000, Avg. loss: 24.731655\n",
      "Total training time: 260.54 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 327.69, NNZs: 8192, Bias: -8.662479, T: 13590000, Avg. loss: 40.593972\n",
      "Total training time: 260.53 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 331.37, NNZs: 8192, Bias: -19.621420, T: 13680000, Avg. loss: 47.888581\n",
      "Total training time: 260.60 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 346.39, NNZs: 8192, Bias: -61.951583, T: 13815000, Avg. loss: 41.611048\n",
      "Total training time: 260.78 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 311.88, NNZs: 8192, Bias: 49.672733, T: 14040000, Avg. loss: 24.401572\n",
      "Total training time: 260.94 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 327.27, NNZs: 8192, Bias: -8.663949, T: 13635000, Avg. loss: 40.289493\n",
      "Total training time: 260.94 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 330.88, NNZs: 8192, Bias: -19.622149, T: 13725000, Avg. loss: 48.291120\n",
      "Total training time: 261.01 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 345.97, NNZs: 8192, Bias: -61.953752, T: 13860000, Avg. loss: 41.424813\n",
      "Total training time: 261.18 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 311.40, NNZs: 8192, Bias: 49.673447, T: 14085000, Avg. loss: 24.248276\n",
      "Total training time: 261.33 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 326.79, NNZs: 8192, Bias: -8.661021, T: 13680000, Avg. loss: 40.556375\n",
      "Total training time: 261.34 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 330.49, NNZs: 8192, Bias: -19.621423, T: 13770000, Avg. loss: 47.782440\n",
      "Total training time: 261.41 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 345.49, NNZs: 8192, Bias: -61.955192, T: 13905000, Avg. loss: 41.063170\n",
      "Total training time: 261.58 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 310.94, NNZs: 8192, Bias: 49.672742, T: 14130000, Avg. loss: 24.786435\n",
      "Total training time: 261.72 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 326.35, NNZs: 8192, Bias: -8.660292, T: 13725000, Avg. loss: 40.680696\n",
      "Total training time: 261.76 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 330.03, NNZs: 8192, Bias: -19.620702, T: 13815000, Avg. loss: 47.890641\n",
      "Total training time: 261.81 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 345.04, NNZs: 8192, Bias: -61.956626, T: 13950000, Avg. loss: 41.182083\n",
      "Total training time: 261.97 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 310.48, NNZs: 8192, Bias: 49.676272, T: 14175000, Avg. loss: 24.434242\n",
      "Total training time: 262.10 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 325.89, NNZs: 8192, Bias: -8.659567, T: 13770000, Avg. loss: 40.368092\n",
      "Total training time: 262.15 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 329.65, NNZs: 8192, Bias: -19.620703, T: 13860000, Avg. loss: 47.338339\n",
      "Total training time: 262.22 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 344.60, NNZs: 8192, Bias: -61.957342, T: 13995000, Avg. loss: 40.892692\n",
      "Total training time: 262.37 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 310.03, NNZs: 8192, Bias: 49.678379, T: 14220000, Avg. loss: 24.174049\n",
      "Total training time: 262.49 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 325.45, NNZs: 8192, Bias: -8.660293, T: 13815000, Avg. loss: 40.322490\n",
      "Total training time: 262.56 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 329.26, NNZs: 8192, Bias: -19.622145, T: 13905000, Avg. loss: 47.013946\n",
      "Total training time: 262.63 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 344.11, NNZs: 8192, Bias: -61.958057, T: 14040000, Avg. loss: 40.855672\n",
      "Total training time: 262.78 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 309.56, NNZs: 8192, Bias: 49.676971, T: 14265000, Avg. loss: 24.968870\n",
      "Total training time: 262.88 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 325.00, NNZs: 8192, Bias: -8.662463, T: 13860000, Avg. loss: 40.187907\n",
      "Total training time: 262.96 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 328.78, NNZs: 8192, Bias: -19.623584, T: 13950000, Avg. loss: 47.332542\n",
      "Total training time: 263.03 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 343.58, NNZs: 8192, Bias: -61.958770, T: 14085000, Avg. loss: 41.110974\n",
      "Total training time: 263.20 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 309.11, NNZs: 8192, Bias: 49.679072, T: 14310000, Avg. loss: 24.516549\n",
      "Total training time: 263.27 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 324.57, NNZs: 8192, Bias: -8.663187, T: 13905000, Avg. loss: 39.361420\n",
      "Total training time: 263.36 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 328.31, NNZs: 8192, Bias: -19.625014, T: 13995000, Avg. loss: 46.941904\n",
      "Total training time: 263.45 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 343.08, NNZs: 8192, Bias: -61.960191, T: 14130000, Avg. loss: 40.317670\n",
      "Total training time: 263.61 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 308.65, NNZs: 8192, Bias: 49.680467, T: 14355000, Avg. loss: 23.791018\n",
      "Total training time: 263.67 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 324.10, NNZs: 8192, Bias: -8.662472, T: 13950000, Avg. loss: 40.014805\n",
      "Total training time: 263.78 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 327.88, NNZs: 8192, Bias: -19.625016, T: 14040000, Avg. loss: 46.707167\n",
      "Total training time: 263.87 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 342.63, NNZs: 8192, Bias: -61.961603, T: 14175000, Avg. loss: 40.301589\n",
      "Total training time: 264.02 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 308.22, NNZs: 8192, Bias: 49.682552, T: 14400000, Avg. loss: 24.158611\n",
      "Total training time: 264.06 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 323.68, NNZs: 8192, Bias: -8.664621, T: 13995000, Avg. loss: 39.153661\n",
      "Total training time: 264.19 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 327.45, NNZs: 8192, Bias: -19.627147, T: 14085000, Avg. loss: 46.656828\n",
      "Total training time: 264.29 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 307.77, NNZs: 8192, Bias: 49.684630, T: 14445000, Avg. loss: 23.602091\n",
      "Total training time: 264.46 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 342.18, NNZs: 8192, Bias: -61.963010, T: 14220000, Avg. loss: 40.283254\n",
      "Total training time: 264.44 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 323.24, NNZs: 8192, Bias: -8.664620, T: 14040000, Avg. loss: 39.701374\n",
      "Total training time: 264.60 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 327.00, NNZs: 8192, Bias: -19.627147, T: 14130000, Avg. loss: 45.932761\n",
      "Total training time: 264.71 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 307.30, NNZs: 8192, Bias: 49.682555, T: 14490000, Avg. loss: 23.722908\n",
      "Total training time: 264.86 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 341.74, NNZs: 8192, Bias: -61.963010, T: 14265000, Avg. loss: 40.873062\n",
      "Total training time: 264.85 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 322.78, NNZs: 8192, Bias: -8.664621, T: 14085000, Avg. loss: 38.865638\n",
      "Total training time: 265.02 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 326.58, NNZs: 8192, Bias: -19.627148, T: 14175000, Avg. loss: 46.506453\n",
      "Total training time: 265.14 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 306.84, NNZs: 8192, Bias: 49.683244, T: 14535000, Avg. loss: 23.887404\n",
      "Total training time: 265.25 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 341.27, NNZs: 8192, Bias: -61.964411, T: 14310000, Avg. loss: 40.533279\n",
      "Total training time: 265.27 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 322.39, NNZs: 8192, Bias: -8.665329, T: 14130000, Avg. loss: 38.545108\n",
      "Total training time: 265.43 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 326.16, NNZs: 8192, Bias: -19.627851, T: 14220000, Avg. loss: 46.741172\n",
      "Total training time: 265.56 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 306.39, NNZs: 8192, Bias: 49.682558, T: 14580000, Avg. loss: 23.951572\n",
      "Total training time: 265.66 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 340.84, NNZs: 8192, Bias: -61.964413, T: 14355000, Avg. loss: 39.833132\n",
      "Total training time: 265.69 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 322.01, NNZs: 8192, Bias: -8.666740, T: 14175000, Avg. loss: 39.182495\n",
      "Total training time: 265.86 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 325.70, NNZs: 8192, Bias: -19.627150, T: 14265000, Avg. loss: 46.045559\n",
      "Total training time: 265.99 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 305.95, NNZs: 8192, Bias: 49.683926, T: 14625000, Avg. loss: 23.535586\n",
      "Total training time: 266.07 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 340.41, NNZs: 8192, Bias: -61.966500, T: 14400000, Avg. loss: 39.091332\n",
      "Total training time: 266.12 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 321.61, NNZs: 8192, Bias: -8.668147, T: 14220000, Avg. loss: 38.783089\n",
      "Total training time: 266.27 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 325.27, NNZs: 8192, Bias: -19.627155, T: 14310000, Avg. loss: 45.326962\n",
      "Total training time: 266.40 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 305.47, NNZs: 8192, Bias: 49.684609, T: 14670000, Avg. loss: 23.506207\n",
      "Total training time: 266.46 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 339.99, NNZs: 8192, Bias: -61.967195, T: 14445000, Avg. loss: 39.383753\n",
      "Total training time: 266.52 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 321.20, NNZs: 8192, Bias: -8.667442, T: 14265000, Avg. loss: 38.870799\n",
      "Total training time: 266.68 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 305.02, NNZs: 8192, Bias: 49.685287, T: 14715000, Avg. loss: 23.510678\n",
      "Total training time: 266.85 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 324.87, NNZs: 8192, Bias: -19.627855, T: 14355000, Avg. loss: 45.209958\n",
      "Total training time: 266.81 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 339.55, NNZs: 8192, Bias: -61.968576, T: 14490000, Avg. loss: 39.331668\n",
      "Total training time: 266.93 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 320.78, NNZs: 8192, Bias: -8.667438, T: 14310000, Avg. loss: 38.732458\n",
      "Total training time: 267.09 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 304.56, NNZs: 8192, Bias: 49.685966, T: 14760000, Avg. loss: 23.472305\n",
      "Total training time: 267.25 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 324.40, NNZs: 8192, Bias: -19.629249, T: 14400000, Avg. loss: 44.815646\n",
      "Total training time: 267.23 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 339.08, NNZs: 8192, Bias: -61.969264, T: 14535000, Avg. loss: 39.502520\n",
      "Total training time: 267.33 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 320.36, NNZs: 8192, Bias: -8.666740, T: 14355000, Avg. loss: 38.272824\n",
      "Total training time: 267.49 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 304.13, NNZs: 8192, Bias: 49.687318, T: 14805000, Avg. loss: 23.195667\n",
      "Total training time: 267.63 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 323.96, NNZs: 8192, Bias: -19.628555, T: 14445000, Avg. loss: 45.307468\n",
      "Total training time: 267.62 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 338.63, NNZs: 8192, Bias: -61.971326, T: 14580000, Avg. loss: 39.573679\n",
      "Total training time: 267.73 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 319.96, NNZs: 8192, Bias: -8.670215, T: 14400000, Avg. loss: 37.924850\n",
      "Total training time: 267.89 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 303.70, NNZs: 8192, Bias: 49.689339, T: 14850000, Avg. loss: 22.797760\n",
      "Total training time: 268.02 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 323.55, NNZs: 8192, Bias: -19.629247, T: 14490000, Avg. loss: 44.729326\n",
      "Total training time: 268.04 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 338.19, NNZs: 8192, Bias: -61.971326, T: 14625000, Avg. loss: 39.118737\n",
      "Total training time: 268.14 seconds.\n",
      "Convergence after 325 epochs took 268.14 seconds\n",
      "Norm: 319.55, NNZs: 8192, Bias: -8.670213, T: 14445000, Avg. loss: 37.840878\n",
      "Total training time: 268.29 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 303.28, NNZs: 8192, Bias: 49.688668, T: 14895000, Avg. loss: 23.180294\n",
      "Total training time: 268.41 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 323.13, NNZs: 8192, Bias: -19.629937, T: 14535000, Avg. loss: 44.652059\n",
      "Total training time: 268.43 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 319.12, NNZs: 8192, Bias: -8.671595, T: 14490000, Avg. loss: 37.871493\n",
      "Total training time: 268.67 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 302.83, NNZs: 8192, Bias: 49.689339, T: 14940000, Avg. loss: 22.818808\n",
      "Total training time: 268.77 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 322.73, NNZs: 8192, Bias: -19.628564, T: 14580000, Avg. loss: 44.922767\n",
      "Total training time: 268.81 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 318.72, NNZs: 8192, Bias: -8.671596, T: 14535000, Avg. loss: 37.759845\n",
      "Total training time: 269.04 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 302.40, NNZs: 8192, Bias: 49.691344, T: 14985000, Avg. loss: 22.969831\n",
      "Total training time: 269.14 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 322.33, NNZs: 8192, Bias: -19.629933, T: 14625000, Avg. loss: 44.514001\n",
      "Total training time: 269.19 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 318.28, NNZs: 8192, Bias: -8.672972, T: 14580000, Avg. loss: 37.816972\n",
      "Total training time: 269.42 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 301.95, NNZs: 8192, Bias: 49.694672, T: 15030000, Avg. loss: 22.894609\n",
      "Total training time: 269.50 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 321.86, NNZs: 8192, Bias: -19.631981, T: 14670000, Avg. loss: 44.239026\n",
      "Total training time: 269.57 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 317.87, NNZs: 8192, Bias: -8.672973, T: 14625000, Avg. loss: 38.042853\n",
      "Total training time: 269.79 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 301.54, NNZs: 8192, Bias: 49.694005, T: 15075000, Avg. loss: 22.531534\n",
      "Total training time: 269.86 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 321.44, NNZs: 8192, Bias: -19.630619, T: 14715000, Avg. loss: 44.062117\n",
      "Total training time: 269.94 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 317.45, NNZs: 8192, Bias: -8.672970, T: 14670000, Avg. loss: 37.214199\n",
      "Total training time: 270.17 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 301.15, NNZs: 8192, Bias: 49.695331, T: 15120000, Avg. loss: 22.687152\n",
      "Total training time: 270.23 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 321.05, NNZs: 8192, Bias: -19.630621, T: 14760000, Avg. loss: 43.433268\n",
      "Total training time: 270.33 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 317.04, NNZs: 8192, Bias: -8.673649, T: 14715000, Avg. loss: 36.688377\n",
      "Total training time: 270.54 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 300.75, NNZs: 8192, Bias: 49.696653, T: 15165000, Avg. loss: 21.900487\n",
      "Total training time: 270.59 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 320.64, NNZs: 8192, Bias: -19.630621, T: 14805000, Avg. loss: 43.757350\n",
      "Total training time: 270.71 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 300.33, NNZs: 8192, Bias: 49.696655, T: 15210000, Avg. loss: 22.611608\n",
      "Total training time: 270.95 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 316.60, NNZs: 8192, Bias: -8.672292, T: 14760000, Avg. loss: 36.631071\n",
      "Total training time: 270.92 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 320.31, NNZs: 8192, Bias: -19.631967, T: 14850000, Avg. loss: 43.751525\n",
      "Total training time: 271.10 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 299.95, NNZs: 8192, Bias: 49.696655, T: 15255000, Avg. loss: 22.350453\n",
      "Total training time: 271.32 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 316.22, NNZs: 8192, Bias: -8.674998, T: 14805000, Avg. loss: 36.937300\n",
      "Total training time: 271.31 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 319.93, NNZs: 8192, Bias: -19.631294, T: 14895000, Avg. loss: 43.223219\n",
      "Total training time: 271.49 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 299.50, NNZs: 8192, Bias: 49.697310, T: 15300000, Avg. loss: 22.509955\n",
      "Total training time: 271.68 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 315.83, NNZs: 8192, Bias: -8.675669, T: 14850000, Avg. loss: 36.732180\n",
      "Total training time: 271.68 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 319.49, NNZs: 8192, Bias: -19.631292, T: 14940000, Avg. loss: 43.847148\n",
      "Total training time: 271.88 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 299.10, NNZs: 8192, Bias: 49.699918, T: 15345000, Avg. loss: 22.423355\n",
      "Total training time: 272.04 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 315.43, NNZs: 8192, Bias: -8.674999, T: 14895000, Avg. loss: 37.040616\n",
      "Total training time: 272.07 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 319.12, NNZs: 8192, Bias: -19.629955, T: 14985000, Avg. loss: 43.740208\n",
      "Total training time: 272.26 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 298.68, NNZs: 8192, Bias: 49.699917, T: 15390000, Avg. loss: 22.289862\n",
      "Total training time: 272.42 seconds.\n",
      "Convergence after 342 epochs took 272.42 seconds\n",
      "Norm: 315.02, NNZs: 8192, Bias: -8.673661, T: 14940000, Avg. loss: 36.771109\n",
      "Total training time: 272.45 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 318.71, NNZs: 8192, Bias: -19.630621, T: 15030000, Avg. loss: 43.592309\n",
      "Total training time: 272.64 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 314.62, NNZs: 8192, Bias: -8.675669, T: 14985000, Avg. loss: 36.656444\n",
      "Total training time: 272.81 seconds.\n",
      "Convergence after 333 epochs took 272.81 seconds\n",
      "Norm: 318.34, NNZs: 8192, Bias: -19.629957, T: 15075000, Avg. loss: 42.954842\n",
      "Total training time: 273.00 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 317.98, NNZs: 8192, Bias: -19.630620, T: 15120000, Avg. loss: 42.643113\n",
      "Total training time: 273.36 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 317.58, NNZs: 8192, Bias: -19.633258, T: 15165000, Avg. loss: 42.144959\n",
      "Total training time: 273.71 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 317.20, NNZs: 8192, Bias: -19.631281, T: 15210000, Avg. loss: 42.593878\n",
      "Total training time: 274.06 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 316.80, NNZs: 8192, Bias: -19.633251, T: 15255000, Avg. loss: 42.679561\n",
      "Total training time: 274.42 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 316.39, NNZs: 8192, Bias: -19.635212, T: 15300000, Avg. loss: 42.348751\n",
      "Total training time: 274.77 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 315.99, NNZs: 8192, Bias: -19.636516, T: 15345000, Avg. loss: 41.722641\n",
      "Total training time: 275.12 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 315.58, NNZs: 8192, Bias: -19.637815, T: 15390000, Avg. loss: 41.550309\n",
      "Total training time: 275.48 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 315.24, NNZs: 8192, Bias: -19.635870, T: 15435000, Avg. loss: 42.342814\n",
      "Total training time: 275.84 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 314.85, NNZs: 8192, Bias: -19.639103, T: 15480000, Avg. loss: 41.864788\n",
      "Total training time: 276.19 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 314.47, NNZs: 8192, Bias: -19.639100, T: 15525000, Avg. loss: 41.763864\n",
      "Total training time: 276.55 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 314.07, NNZs: 8192, Bias: -19.638455, T: 15570000, Avg. loss: 41.553503\n",
      "Total training time: 276.91 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 313.67, NNZs: 8192, Bias: -19.639096, T: 15615000, Avg. loss: 41.490367\n",
      "Total training time: 277.27 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 313.28, NNZs: 8192, Bias: -19.639096, T: 15660000, Avg. loss: 41.925218\n",
      "Total training time: 277.63 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 312.93, NNZs: 8192, Bias: -19.639733, T: 15705000, Avg. loss: 40.946036\n",
      "Total training time: 277.98 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 312.59, NNZs: 8192, Bias: -19.639096, T: 15750000, Avg. loss: 41.423016\n",
      "Total training time: 278.35 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 312.22, NNZs: 8192, Bias: -19.639728, T: 15795000, Avg. loss: 40.432147\n",
      "Total training time: 278.71 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 311.86, NNZs: 8192, Bias: -19.639726, T: 15840000, Avg. loss: 40.886236\n",
      "Total training time: 279.08 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 311.45, NNZs: 8192, Bias: -19.640357, T: 15885000, Avg. loss: 41.132444\n",
      "Total training time: 279.44 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 311.09, NNZs: 8192, Bias: -19.640358, T: 15930000, Avg. loss: 40.552036\n",
      "Total training time: 279.79 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 310.71, NNZs: 8192, Bias: -19.641610, T: 15975000, Avg. loss: 40.574310\n",
      "Total training time: 280.15 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 310.33, NNZs: 8192, Bias: -19.639735, T: 16020000, Avg. loss: 39.631424\n",
      "Total training time: 280.50 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 309.96, NNZs: 8192, Bias: -19.640357, T: 16065000, Avg. loss: 39.970266\n",
      "Total training time: 280.85 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 309.65, NNZs: 8192, Bias: -19.639737, T: 16110000, Avg. loss: 39.947031\n",
      "Total training time: 281.21 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 309.30, NNZs: 8192, Bias: -19.640978, T: 16155000, Avg. loss: 40.174064\n",
      "Total training time: 281.57 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 308.95, NNZs: 8192, Bias: -19.643448, T: 16200000, Avg. loss: 39.991824\n",
      "Total training time: 281.92 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 308.55, NNZs: 8192, Bias: -19.644062, T: 16245000, Avg. loss: 39.294276\n",
      "Total training time: 282.28 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 308.16, NNZs: 8192, Bias: -19.644063, T: 16290000, Avg. loss: 39.604351\n",
      "Total training time: 282.63 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 307.87, NNZs: 8192, Bias: -19.644677, T: 16335000, Avg. loss: 38.881755\n",
      "Total training time: 283.00 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 307.52, NNZs: 8192, Bias: -19.643456, T: 16380000, Avg. loss: 39.715810\n",
      "Total training time: 283.36 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 307.19, NNZs: 8192, Bias: -19.645287, T: 16425000, Avg. loss: 38.356892\n",
      "Total training time: 283.71 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 306.85, NNZs: 8192, Bias: -19.647719, T: 16470000, Avg. loss: 38.870797\n",
      "Total training time: 284.07 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 306.52, NNZs: 8192, Bias: -19.647111, T: 16515000, Avg. loss: 39.227387\n",
      "Total training time: 284.43 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 306.13, NNZs: 8192, Bias: -19.646505, T: 16560000, Avg. loss: 39.044283\n",
      "Total training time: 284.78 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 305.79, NNZs: 8192, Bias: -19.647108, T: 16605000, Avg. loss: 38.760873\n",
      "Total training time: 285.14 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 305.45, NNZs: 8192, Bias: -19.648309, T: 16650000, Avg. loss: 38.226319\n",
      "Total training time: 285.50 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 305.15, NNZs: 8192, Bias: -19.646509, T: 16695000, Avg. loss: 37.901926\n",
      "Total training time: 285.86 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 304.82, NNZs: 8192, Bias: -19.647706, T: 16740000, Avg. loss: 38.140397\n",
      "Total training time: 286.22 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 304.49, NNZs: 8192, Bias: -19.647708, T: 16785000, Avg. loss: 38.591330\n",
      "Total training time: 286.58 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 304.19, NNZs: 8192, Bias: -19.648897, T: 16830000, Avg. loss: 37.811933\n",
      "Total training time: 286.93 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 303.87, NNZs: 8192, Bias: -19.649491, T: 16875000, Avg. loss: 37.563917\n",
      "Total training time: 287.29 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 303.55, NNZs: 8192, Bias: -19.650673, T: 16920000, Avg. loss: 37.580994\n",
      "Total training time: 287.65 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 303.20, NNZs: 8192, Bias: -19.651853, T: 16965000, Avg. loss: 37.840351\n",
      "Total training time: 288.00 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 302.87, NNZs: 8192, Bias: -19.650677, T: 17010000, Avg. loss: 37.544078\n",
      "Total training time: 288.36 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 302.52, NNZs: 8192, Bias: -19.650679, T: 17055000, Avg. loss: 37.212054\n",
      "Total training time: 288.72 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 302.23, NNZs: 8192, Bias: -19.650095, T: 17100000, Avg. loss: 37.396406\n",
      "Total training time: 289.08 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 301.87, NNZs: 8192, Bias: -19.650680, T: 17145000, Avg. loss: 37.301654\n",
      "Total training time: 289.43 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 301.56, NNZs: 8192, Bias: -19.652428, T: 17190000, Avg. loss: 37.284321\n",
      "Total training time: 289.78 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 301.23, NNZs: 8192, Bias: -19.651848, T: 17235000, Avg. loss: 37.492284\n",
      "Total training time: 290.15 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 300.93, NNZs: 8192, Bias: -19.653007, T: 17280000, Avg. loss: 37.212927\n",
      "Total training time: 290.50 seconds.\n",
      "Convergence after 384 epochs took 290.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 9.8947, val.acc = 0.4898\n",
      "Rep: 1, te.acc = 0.4770\n",
      "\n",
      "All reps test.acc:\n",
      "[0.477]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_1_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=1, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 10\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 10 # proportion of the reconstruction loss\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.1539, time: 18.9362\n",
      "reconstruction loss = 0.0487, similarity loss: 0.6669\n",
      "Epoch 1, loss = 0.9427, time: 18.0150\n",
      "reconstruction loss = 0.0378, similarity loss: 0.5644\n",
      "Epoch 2, loss = 0.9457, time: 17.8622\n",
      "reconstruction loss = 0.0338, similarity loss: 0.6080\n",
      "Epoch 3, loss = 0.8722, time: 17.9865\n",
      "reconstruction loss = 0.0265, similarity loss: 0.6070\n",
      "Epoch 4, loss = 0.7479, time: 17.8067\n",
      "reconstruction loss = 0.0225, similarity loss: 0.5229\n",
      "Epoch 5, loss = 0.7894, time: 17.4768\n",
      "reconstruction loss = 0.0208, similarity loss: 0.5815\n",
      "Epoch 6, loss = 0.7236, time: 17.6113\n",
      "reconstruction loss = 0.0200, similarity loss: 0.5237\n",
      "Epoch 7, loss = 0.6997, time: 17.6339\n",
      "reconstruction loss = 0.0181, similarity loss: 0.5186\n",
      "Epoch 8, loss = 0.6479, time: 17.5429\n",
      "reconstruction loss = 0.0190, similarity loss: 0.4577\n",
      "Epoch 9, loss = 0.6519, time: 17.7144\n",
      "reconstruction loss = 0.0191, similarity loss: 0.4607\n",
      "Epoch 10, loss = 0.5998, time: 17.5885\n",
      "reconstruction loss = 0.0182, similarity loss: 0.4179\n",
      "Epoch 11, loss = 0.6089, time: 17.5853\n",
      "reconstruction loss = 0.0171, similarity loss: 0.4376\n",
      "Epoch 12, loss = 0.5580, time: 17.5195\n",
      "reconstruction loss = 0.0195, similarity loss: 0.3627\n",
      "Epoch 13, loss = 0.5424, time: 17.5359\n",
      "reconstruction loss = 0.0173, similarity loss: 0.3693\n",
      "Epoch 14, loss = 0.5711, time: 17.3884\n",
      "reconstruction loss = 0.0174, similarity loss: 0.3972\n",
      "Epoch 15, loss = 0.5595, time: 17.6327\n",
      "reconstruction loss = 0.0182, similarity loss: 0.3774\n",
      "Epoch 16, loss = 0.4928, time: 17.6772\n",
      "reconstruction loss = 0.0158, similarity loss: 0.3345\n",
      "Epoch 17, loss = 0.4774, time: 17.6402\n",
      "reconstruction loss = 0.0147, similarity loss: 0.3306\n",
      "Epoch 18, loss = 0.4927, time: 17.9749\n",
      "reconstruction loss = 0.0151, similarity loss: 0.3420\n",
      "Epoch 19, loss = 0.4860, time: 18.1761\n",
      "reconstruction loss = 0.0156, similarity loss: 0.3302\n",
      "Epoch 20, loss = 0.4775, time: 18.0058\n",
      "reconstruction loss = 0.0151, similarity loss: 0.3266\n",
      "Epoch 21, loss = 0.4621, time: 18.5348\n",
      "reconstruction loss = 0.0155, similarity loss: 0.3074\n",
      "Epoch 22, loss = 0.4444, time: 18.1550\n",
      "reconstruction loss = 0.0155, similarity loss: 0.2899\n",
      "Epoch 23, loss = 0.4523, time: 18.2196\n",
      "reconstruction loss = 0.0146, similarity loss: 0.3060\n",
      "Epoch 24, loss = 0.4478, time: 18.3357\n",
      "reconstruction loss = 0.0144, similarity loss: 0.3041\n",
      "Epoch 25, loss = 0.4569, time: 18.2756\n",
      "reconstruction loss = 0.0161, similarity loss: 0.2957\n",
      "Epoch 26, loss = 0.4392, time: 17.9444\n",
      "reconstruction loss = 0.0141, similarity loss: 0.2987\n",
      "Epoch 27, loss = 0.4346, time: 18.2770\n",
      "reconstruction loss = 0.0140, similarity loss: 0.2946\n",
      "Epoch 28, loss = 0.4097, time: 18.0373\n",
      "reconstruction loss = 0.0156, similarity loss: 0.2535\n",
      "Epoch 29, loss = 0.4680, time: 18.1223\n",
      "reconstruction loss = 0.0156, similarity loss: 0.3117\n",
      "Epoch 30, loss = 0.4463, time: 18.2378\n",
      "reconstruction loss = 0.0144, similarity loss: 0.3023\n",
      "Epoch 31, loss = 0.4275, time: 18.2428\n",
      "reconstruction loss = 0.0140, similarity loss: 0.2877\n",
      "Epoch 32, loss = 0.4467, time: 18.0643\n",
      "reconstruction loss = 0.0161, similarity loss: 0.2852\n",
      "Epoch 33, loss = 0.4279, time: 18.1216\n",
      "reconstruction loss = 0.0155, similarity loss: 0.2726\n",
      "Epoch 34, loss = 0.4363, time: 18.0102\n",
      "reconstruction loss = 0.0137, similarity loss: 0.2994\n",
      "Epoch 35, loss = 0.4371, time: 18.2154\n",
      "reconstruction loss = 0.0130, similarity loss: 0.3072\n",
      "Epoch 36, loss = 0.4053, time: 17.9561\n",
      "reconstruction loss = 0.0126, similarity loss: 0.2793\n",
      "Epoch 37, loss = 0.4433, time: 17.6656\n",
      "reconstruction loss = 0.0129, similarity loss: 0.3141\n",
      "Epoch 38, loss = 0.3951, time: 17.7335\n",
      "reconstruction loss = 0.0131, similarity loss: 0.2637\n",
      "Epoch 39, loss = 0.4038, time: 17.6601\n",
      "reconstruction loss = 0.0129, similarity loss: 0.2751\n",
      "Epoch 40, loss = 0.4044, time: 17.5132\n",
      "reconstruction loss = 0.0127, similarity loss: 0.2776\n",
      "Epoch 41, loss = 0.4306, time: 17.8697\n",
      "reconstruction loss = 0.0121, similarity loss: 0.3097\n",
      "Epoch 42, loss = 0.3963, time: 17.8199\n",
      "reconstruction loss = 0.0129, similarity loss: 0.2675\n",
      "Epoch 43, loss = 0.3781, time: 17.2109\n",
      "reconstruction loss = 0.0116, similarity loss: 0.2616\n",
      "Epoch 44, loss = 0.3527, time: 17.1563\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2448\n",
      "Epoch 45, loss = 0.3730, time: 17.1386\n",
      "reconstruction loss = 0.0114, similarity loss: 0.2588\n",
      "Epoch 46, loss = 0.3740, time: 17.2189\n",
      "reconstruction loss = 0.0118, similarity loss: 0.2561\n",
      "Epoch 47, loss = 0.4061, time: 17.1801\n",
      "reconstruction loss = 0.0132, similarity loss: 0.2739\n",
      "Epoch 48, loss = 0.3776, time: 17.0894\n",
      "reconstruction loss = 0.0133, similarity loss: 0.2449\n",
      "Epoch 49, loss = 0.3855, time: 17.2057\n",
      "reconstruction loss = 0.0114, similarity loss: 0.2719\n",
      "Epoch 50, loss = 0.3629, time: 17.0869\n",
      "reconstruction loss = 0.0119, similarity loss: 0.2434\n",
      "Epoch 51, loss = 0.3649, time: 17.1553\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2520\n",
      "Epoch 52, loss = 0.3526, time: 17.2209\n",
      "reconstruction loss = 0.0114, similarity loss: 0.2390\n",
      "Epoch 53, loss = 0.4111, time: 17.0560\n",
      "reconstruction loss = 0.0109, similarity loss: 0.3019\n",
      "Epoch 54, loss = 0.4041, time: 17.0579\n",
      "reconstruction loss = 0.0116, similarity loss: 0.2884\n",
      "Epoch 55, loss = 0.3854, time: 17.1799\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2776\n",
      "Epoch 56, loss = 0.3704, time: 17.0499\n",
      "reconstruction loss = 0.0110, similarity loss: 0.2605\n",
      "Epoch 57, loss = 0.3987, time: 17.1458\n",
      "reconstruction loss = 0.0119, similarity loss: 0.2798\n",
      "Epoch 58, loss = 0.3516, time: 17.0593\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2409\n",
      "Epoch 59, loss = 0.3690, time: 17.1069\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2561\n",
      "Epoch 60, loss = 0.4041, time: 17.1159\n",
      "reconstruction loss = 0.0125, similarity loss: 0.2794\n",
      "Epoch 61, loss = 0.3732, time: 17.1123\n",
      "reconstruction loss = 0.0118, similarity loss: 0.2550\n",
      "Epoch 62, loss = 0.4027, time: 17.1369\n",
      "reconstruction loss = 0.0112, similarity loss: 0.2905\n",
      "Epoch 63, loss = 0.3873, time: 17.1788\n",
      "reconstruction loss = 0.0103, similarity loss: 0.2842\n",
      "Epoch 64, loss = 0.4048, time: 17.1062\n",
      "reconstruction loss = 0.0107, similarity loss: 0.2976\n",
      "Epoch 65, loss = 0.3484, time: 17.3484\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2439\n",
      "Epoch 66, loss = 0.3602, time: 17.7036\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2615\n",
      "Epoch 67, loss = 0.3773, time: 17.7373\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2680\n",
      "Epoch 68, loss = 0.3585, time: 17.1304\n",
      "reconstruction loss = 0.0100, similarity loss: 0.2588\n",
      "Epoch 69, loss = 0.3434, time: 17.1050\n",
      "reconstruction loss = 0.0103, similarity loss: 0.2403\n",
      "Epoch 70, loss = 0.3964, time: 17.1439\n",
      "reconstruction loss = 0.0105, similarity loss: 0.2917\n",
      "Epoch 71, loss = 0.3491, time: 17.0248\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2414\n",
      "Epoch 72, loss = 0.4077, time: 17.1601\n",
      "reconstruction loss = 0.0126, similarity loss: 0.2814\n",
      "Epoch 73, loss = 0.3817, time: 17.2084\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2708\n",
      "Epoch 74, loss = 0.3479, time: 17.1929\n",
      "reconstruction loss = 0.0107, similarity loss: 0.2413\n",
      "Epoch 75, loss = 0.3308, time: 17.6769\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2322\n",
      "Epoch 76, loss = 0.3572, time: 17.6120\n",
      "reconstruction loss = 0.0106, similarity loss: 0.2512\n",
      "Epoch 77, loss = 0.3948, time: 17.5054\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2856\n",
      "Epoch 78, loss = 0.3214, time: 17.0197\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2177\n",
      "Epoch 79, loss = 0.3412, time: 17.0991\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2461\n",
      "Epoch 80, loss = 0.3344, time: 17.3119\n",
      "reconstruction loss = 0.0093, similarity loss: 0.2410\n",
      "Epoch 81, loss = 0.3444, time: 17.1999\n",
      "reconstruction loss = 0.0102, similarity loss: 0.2426\n",
      "Epoch 82, loss = 0.3185, time: 17.2352\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2353\n",
      "Epoch 83, loss = 0.3426, time: 17.1629\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2452\n",
      "Epoch 84, loss = 0.3410, time: 17.1971\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2424\n",
      "Epoch 85, loss = 0.3779, time: 17.5930\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2826\n",
      "Epoch 86, loss = 0.3509, time: 17.5068\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2536\n",
      "Epoch 87, loss = 0.3651, time: 17.9636\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2760\n",
      "Epoch 88, loss = 0.3899, time: 17.3776\n",
      "reconstruction loss = 0.0121, similarity loss: 0.2690\n",
      "Epoch 89, loss = 0.3776, time: 17.3899\n",
      "reconstruction loss = 0.0112, similarity loss: 0.2660\n",
      "Epoch 90, loss = 0.3546, time: 17.7390\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2634\n",
      "Epoch 91, loss = 0.3919, time: 17.7909\n",
      "reconstruction loss = 0.0107, similarity loss: 0.2844\n",
      "Epoch 92, loss = 0.3557, time: 17.2949\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2612\n",
      "Epoch 93, loss = 0.3533, time: 17.8316\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2494\n",
      "Epoch 94, loss = 0.3484, time: 17.5923\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2502\n",
      "Epoch 95, loss = 0.3007, time: 17.3019\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2153\n",
      "Epoch 96, loss = 0.3668, time: 17.3806\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2540\n",
      "Epoch 97, loss = 0.3428, time: 17.2531\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2515\n",
      "Epoch 98, loss = 0.3678, time: 17.3800\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2701\n",
      "Epoch 99, loss = 0.2974, time: 17.2932\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2142\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "loss = 4.8185, val.acc = 0.5362\n",
      "Rep: 1, te.acc = 0.5326\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5326]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_10_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=0, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 20\n",
      "clfnonlinear: None\n",
      "headnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 20\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars.loadnet = torch.load(\n",
    "    \"save/CONV6/AE/net_rep_1_hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_1e-05_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 2.2564, val.loss = 2.1069, val.acc = 0.2604\n",
      "Epoch 1, loss = 2.0203, val.loss = 1.9396, val.acc = 0.3652\n",
      "Epoch 2, loss = 1.8792, val.loss = 1.8245, val.acc = 0.4154\n",
      "Epoch 3, loss = 1.7793, val.loss = 1.7409, val.acc = 0.4470\n",
      "Epoch 4, loss = 1.7045, val.loss = 1.6767, val.acc = 0.4652\n",
      "Epoch 5, loss = 1.6458, val.loss = 1.6253, val.acc = 0.4816\n",
      "Epoch 6, loss = 1.5977, val.loss = 1.5826, val.acc = 0.4976\n",
      "Epoch 7, loss = 1.5573, val.loss = 1.5464, val.acc = 0.5078\n",
      "Epoch 8, loss = 1.5225, val.loss = 1.5151, val.acc = 0.5178\n",
      "Epoch 9, loss = 1.4920, val.loss = 1.4875, val.acc = 0.5236\n",
      "Epoch 10, loss = 1.4649, val.loss = 1.4630, val.acc = 0.5292\n",
      "Epoch 11, loss = 1.4407, val.loss = 1.4411, val.acc = 0.5366\n",
      "Epoch 12, loss = 1.4187, val.loss = 1.4212, val.acc = 0.5438\n",
      "Epoch 13, loss = 1.3987, val.loss = 1.4031, val.acc = 0.5508\n",
      "Epoch 14, loss = 1.3803, val.loss = 1.3865, val.acc = 0.5544\n",
      "Epoch 15, loss = 1.3634, val.loss = 1.3713, val.acc = 0.5572\n",
      "Epoch 16, loss = 1.3478, val.loss = 1.3572, val.acc = 0.5612\n",
      "Epoch 17, loss = 1.3332, val.loss = 1.3442, val.acc = 0.5634\n",
      "Epoch 18, loss = 1.3196, val.loss = 1.3320, val.acc = 0.5660\n",
      "Epoch 19, loss = 1.3069, val.loss = 1.3207, val.acc = 0.5694\n",
      "Epoch 20, loss = 1.2949, val.loss = 1.3101, val.acc = 0.5736\n",
      "Epoch 21, loss = 1.2837, val.loss = 1.3002, val.acc = 0.5776\n",
      "Epoch 22, loss = 1.2730, val.loss = 1.2908, val.acc = 0.5806\n",
      "Epoch 23, loss = 1.2629, val.loss = 1.2820, val.acc = 0.5834\n",
      "Epoch 24, loss = 1.2534, val.loss = 1.2737, val.acc = 0.5850\n",
      "Epoch 25, loss = 1.2443, val.loss = 1.2658, val.acc = 0.5866\n",
      "Epoch 26, loss = 1.2356, val.loss = 1.2583, val.acc = 0.5886\n",
      "Epoch 27, loss = 1.2274, val.loss = 1.2512, val.acc = 0.5904\n",
      "Epoch 28, loss = 1.2195, val.loss = 1.2445, val.acc = 0.5918\n",
      "Epoch 29, loss = 1.2119, val.loss = 1.2381, val.acc = 0.5942\n",
      "Epoch 30, loss = 1.2047, val.loss = 1.2319, val.acc = 0.5968\n",
      "Epoch 31, loss = 1.1977, val.loss = 1.2261, val.acc = 0.5990\n",
      "Epoch 32, loss = 1.1910, val.loss = 1.2205, val.acc = 0.6008\n",
      "Epoch 33, loss = 1.1845, val.loss = 1.2151, val.acc = 0.6012\n",
      "Epoch 34, loss = 1.1783, val.loss = 1.2100, val.acc = 0.6024\n",
      "Epoch 35, loss = 1.1723, val.loss = 1.2051, val.acc = 0.6042\n",
      "Epoch 36, loss = 1.1665, val.loss = 1.2003, val.acc = 0.6054\n",
      "Epoch 37, loss = 1.1609, val.loss = 1.1958, val.acc = 0.6064\n",
      "Epoch 38, loss = 1.1555, val.loss = 1.1914, val.acc = 0.6064\n",
      "Epoch 39, loss = 1.1502, val.loss = 1.1872, val.acc = 0.6090\n",
      "Epoch 40, loss = 1.1451, val.loss = 1.1831, val.acc = 0.6094\n",
      "Epoch 41, loss = 1.1401, val.loss = 1.1792, val.acc = 0.6106\n",
      "Epoch 42, loss = 1.1353, val.loss = 1.1754, val.acc = 0.6110\n",
      "Epoch 43, loss = 1.1306, val.loss = 1.1717, val.acc = 0.6126\n",
      "Epoch 44, loss = 1.1261, val.loss = 1.1681, val.acc = 0.6142\n",
      "Epoch 45, loss = 1.1216, val.loss = 1.1647, val.acc = 0.6154\n",
      "Epoch 46, loss = 1.1173, val.loss = 1.1614, val.acc = 0.6176\n",
      "Epoch 47, loss = 1.1131, val.loss = 1.1582, val.acc = 0.6184\n",
      "Epoch 48, loss = 1.1090, val.loss = 1.1550, val.acc = 0.6198\n",
      "Epoch 49, loss = 1.1050, val.loss = 1.1520, val.acc = 0.6226\n",
      "Epoch 50, loss = 1.1010, val.loss = 1.1491, val.acc = 0.6224\n",
      "Epoch 51, loss = 1.0972, val.loss = 1.1462, val.acc = 0.6238\n",
      "Epoch 52, loss = 1.0935, val.loss = 1.1434, val.acc = 0.6250\n",
      "Epoch 53, loss = 1.0898, val.loss = 1.1407, val.acc = 0.6256\n",
      "Epoch 54, loss = 1.0862, val.loss = 1.1381, val.acc = 0.6268\n",
      "Epoch 55, loss = 1.0827, val.loss = 1.1355, val.acc = 0.6278\n",
      "Epoch 56, loss = 1.0792, val.loss = 1.1330, val.acc = 0.6278\n",
      "Epoch 57, loss = 1.0759, val.loss = 1.1306, val.acc = 0.6294\n",
      "Epoch 58, loss = 1.0725, val.loss = 1.1282, val.acc = 0.6300\n",
      "Epoch 59, loss = 1.0693, val.loss = 1.1259, val.acc = 0.6306\n",
      "Epoch 60, loss = 1.0661, val.loss = 1.1237, val.acc = 0.6314\n",
      "Epoch 61, loss = 1.0630, val.loss = 1.1215, val.acc = 0.6318\n",
      "Epoch 62, loss = 1.0599, val.loss = 1.1193, val.acc = 0.6328\n",
      "Epoch 63, loss = 1.0569, val.loss = 1.1173, val.acc = 0.6324\n",
      "Epoch 64, loss = 1.0539, val.loss = 1.1152, val.acc = 0.6330\n",
      "Epoch 65, loss = 1.0510, val.loss = 1.1132, val.acc = 0.6338\n",
      "Epoch 66, loss = 1.0481, val.loss = 1.1112, val.acc = 0.6348\n",
      "Epoch 67, loss = 1.0453, val.loss = 1.1093, val.acc = 0.6344\n",
      "Epoch 68, loss = 1.0425, val.loss = 1.1075, val.acc = 0.6348\n",
      "Epoch 69, loss = 1.0398, val.loss = 1.1056, val.acc = 0.6356\n",
      "Epoch 70, loss = 1.0371, val.loss = 1.1038, val.acc = 0.6358\n",
      "Epoch 71, loss = 1.0344, val.loss = 1.1021, val.acc = 0.6362\n",
      "Epoch 72, loss = 1.0318, val.loss = 1.1003, val.acc = 0.6366\n",
      "Epoch 73, loss = 1.0293, val.loss = 1.0987, val.acc = 0.6374\n",
      "Epoch 74, loss = 1.0267, val.loss = 1.0970, val.acc = 0.6378\n",
      "Epoch 75, loss = 1.0242, val.loss = 1.0954, val.acc = 0.6386\n",
      "Epoch 76, loss = 1.0217, val.loss = 1.0938, val.acc = 0.6398\n",
      "Epoch 77, loss = 1.0193, val.loss = 1.0922, val.acc = 0.6406\n",
      "Epoch 78, loss = 1.0169, val.loss = 1.0907, val.acc = 0.6410\n",
      "Epoch 79, loss = 1.0145, val.loss = 1.0892, val.acc = 0.6414\n",
      "Epoch 80, loss = 1.0122, val.loss = 1.0877, val.acc = 0.6418\n",
      "Epoch 81, loss = 1.0099, val.loss = 1.0863, val.acc = 0.6422\n",
      "Epoch 82, loss = 1.0076, val.loss = 1.0849, val.acc = 0.6432\n",
      "Epoch 83, loss = 1.0054, val.loss = 1.0835, val.acc = 0.6434\n",
      "Epoch 84, loss = 1.0031, val.loss = 1.0821, val.acc = 0.6436\n",
      "Epoch 85, loss = 1.0009, val.loss = 1.0808, val.acc = 0.6444\n",
      "Epoch 86, loss = 0.9988, val.loss = 1.0794, val.acc = 0.6456\n",
      "Epoch 87, loss = 0.9966, val.loss = 1.0781, val.acc = 0.6458\n",
      "Epoch 88, loss = 0.9945, val.loss = 1.0769, val.acc = 0.6454\n",
      "Epoch 89, loss = 0.9924, val.loss = 1.0756, val.acc = 0.6446\n",
      "Epoch 90, loss = 0.9903, val.loss = 1.0744, val.acc = 0.6454\n",
      "Epoch 91, loss = 0.9883, val.loss = 1.0732, val.acc = 0.6464\n",
      "Epoch 92, loss = 0.9863, val.loss = 1.0719, val.acc = 0.6462\n",
      "Epoch 93, loss = 0.9843, val.loss = 1.0708, val.acc = 0.6462\n",
      "Epoch 94, loss = 0.9823, val.loss = 1.0696, val.acc = 0.6464\n",
      "Epoch 95, loss = 0.9803, val.loss = 1.0685, val.acc = 0.6472\n",
      "Epoch 96, loss = 0.9784, val.loss = 1.0674, val.acc = 0.6482\n",
      "Epoch 97, loss = 0.9764, val.loss = 1.0662, val.acc = 0.6486\n",
      "Epoch 98, loss = 0.9745, val.loss = 1.0651, val.acc = 0.6492\n",
      "Epoch 99, loss = 0.9727, val.loss = 1.0641, val.acc = 0.6500\n",
      "Rep: 1, te.acc = 0.6353\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6353]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 1e-5\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_1e-5')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_5e-05_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.9215, val.loss = 1.6681, val.acc = 0.4614\n",
      "Epoch 1, loss = 1.5646, val.loss = 1.4883, val.acc = 0.5202\n",
      "Epoch 2, loss = 1.4291, val.loss = 1.3919, val.acc = 0.5448\n",
      "Epoch 3, loss = 1.3463, val.loss = 1.3287, val.acc = 0.5632\n",
      "Epoch 4, loss = 1.2881, val.loss = 1.2834, val.acc = 0.5764\n",
      "Epoch 5, loss = 1.2440, val.loss = 1.2489, val.acc = 0.5894\n",
      "Epoch 6, loss = 1.2086, val.loss = 1.2215, val.acc = 0.5968\n",
      "Epoch 7, loss = 1.1794, val.loss = 1.1991, val.acc = 0.6042\n",
      "Epoch 8, loss = 1.1544, val.loss = 1.1803, val.acc = 0.6102\n",
      "Epoch 9, loss = 1.1327, val.loss = 1.1643, val.acc = 0.6160\n",
      "Epoch 10, loss = 1.1134, val.loss = 1.1504, val.acc = 0.6214\n",
      "Epoch 11, loss = 1.0961, val.loss = 1.1382, val.acc = 0.6254\n",
      "Epoch 12, loss = 1.0804, val.loss = 1.1273, val.acc = 0.6280\n",
      "Epoch 13, loss = 1.0660, val.loss = 1.1176, val.acc = 0.6304\n",
      "Epoch 14, loss = 1.0527, val.loss = 1.1089, val.acc = 0.6342\n",
      "Epoch 15, loss = 1.0403, val.loss = 1.1009, val.acc = 0.6372\n",
      "Epoch 16, loss = 1.0287, val.loss = 1.0936, val.acc = 0.6406\n",
      "Epoch 17, loss = 1.0178, val.loss = 1.0869, val.acc = 0.6430\n",
      "Epoch 18, loss = 1.0075, val.loss = 1.0807, val.acc = 0.6434\n",
      "Epoch 19, loss = 0.9978, val.loss = 1.0750, val.acc = 0.6460\n",
      "Epoch 20, loss = 0.9885, val.loss = 1.0696, val.acc = 0.6492\n",
      "Epoch 21, loss = 0.9796, val.loss = 1.0647, val.acc = 0.6516\n",
      "Epoch 22, loss = 0.9711, val.loss = 1.0600, val.acc = 0.6522\n",
      "Epoch 23, loss = 0.9630, val.loss = 1.0556, val.acc = 0.6522\n",
      "Epoch 24, loss = 0.9552, val.loss = 1.0515, val.acc = 0.6542\n",
      "Epoch 25, loss = 0.9477, val.loss = 1.0476, val.acc = 0.6544\n",
      "Epoch 26, loss = 0.9404, val.loss = 1.0440, val.acc = 0.6550\n",
      "Epoch 27, loss = 0.9334, val.loss = 1.0405, val.acc = 0.6562\n",
      "Epoch 28, loss = 0.9266, val.loss = 1.0372, val.acc = 0.6562\n",
      "Epoch 29, loss = 0.9201, val.loss = 1.0341, val.acc = 0.6568\n",
      "Epoch 30, loss = 0.9137, val.loss = 1.0311, val.acc = 0.6574\n",
      "Epoch 31, loss = 0.9075, val.loss = 1.0283, val.acc = 0.6584\n",
      "Epoch 32, loss = 0.9015, val.loss = 1.0256, val.acc = 0.6596\n",
      "Epoch 33, loss = 0.8957, val.loss = 1.0231, val.acc = 0.6606\n",
      "Epoch 34, loss = 0.8900, val.loss = 1.0206, val.acc = 0.6616\n",
      "Epoch 35, loss = 0.8844, val.loss = 1.0183, val.acc = 0.6618\n",
      "Epoch 36, loss = 0.8790, val.loss = 1.0161, val.acc = 0.6630\n",
      "Epoch 37, loss = 0.8737, val.loss = 1.0139, val.acc = 0.6628\n",
      "Epoch 38, loss = 0.8686, val.loss = 1.0119, val.acc = 0.6630\n",
      "Epoch 39, loss = 0.8635, val.loss = 1.0099, val.acc = 0.6642\n",
      "Epoch 40, loss = 0.8586, val.loss = 1.0080, val.acc = 0.6640\n",
      "Epoch 41, loss = 0.8538, val.loss = 1.0062, val.acc = 0.6648\n",
      "Epoch 42, loss = 0.8491, val.loss = 1.0045, val.acc = 0.6658\n",
      "Epoch 43, loss = 0.8444, val.loss = 1.0028, val.acc = 0.6668\n",
      "Epoch 44, loss = 0.8399, val.loss = 1.0012, val.acc = 0.6668\n",
      "Epoch 45, loss = 0.8355, val.loss = 0.9997, val.acc = 0.6672\n",
      "Epoch 46, loss = 0.8311, val.loss = 0.9982, val.acc = 0.6678\n",
      "Epoch 47, loss = 0.8268, val.loss = 0.9968, val.acc = 0.6670\n",
      "Epoch 48, loss = 0.8226, val.loss = 0.9954, val.acc = 0.6682\n",
      "Epoch 49, loss = 0.8185, val.loss = 0.9941, val.acc = 0.6696\n",
      "Epoch 50, loss = 0.8145, val.loss = 0.9929, val.acc = 0.6696\n",
      "Epoch 51, loss = 0.8105, val.loss = 0.9917, val.acc = 0.6698\n",
      "Epoch 52, loss = 0.8066, val.loss = 0.9905, val.acc = 0.6710\n",
      "Epoch 53, loss = 0.8028, val.loss = 0.9894, val.acc = 0.6710\n",
      "Epoch 54, loss = 0.7990, val.loss = 0.9883, val.acc = 0.6716\n",
      "Epoch 55, loss = 0.7953, val.loss = 0.9873, val.acc = 0.6720\n",
      "Epoch 56, loss = 0.7916, val.loss = 0.9863, val.acc = 0.6718\n",
      "Epoch 57, loss = 0.7880, val.loss = 0.9853, val.acc = 0.6730\n",
      "Epoch 58, loss = 0.7844, val.loss = 0.9844, val.acc = 0.6732\n",
      "Epoch 59, loss = 0.7809, val.loss = 0.9835, val.acc = 0.6742\n",
      "Epoch 60, loss = 0.7775, val.loss = 0.9826, val.acc = 0.6742\n",
      "Epoch 61, loss = 0.7741, val.loss = 0.9818, val.acc = 0.6740\n",
      "Epoch 62, loss = 0.7708, val.loss = 0.9810, val.acc = 0.6748\n",
      "Epoch 63, loss = 0.7675, val.loss = 0.9803, val.acc = 0.6746\n",
      "Epoch 64, loss = 0.7642, val.loss = 0.9795, val.acc = 0.6744\n",
      "Epoch 65, loss = 0.7610, val.loss = 0.9788, val.acc = 0.6754\n",
      "Epoch 66, loss = 0.7579, val.loss = 0.9781, val.acc = 0.6756\n",
      "Epoch 67, loss = 0.7547, val.loss = 0.9775, val.acc = 0.6748\n",
      "Epoch 68, loss = 0.7517, val.loss = 0.9769, val.acc = 0.6754\n",
      "Epoch 69, loss = 0.7486, val.loss = 0.9763, val.acc = 0.6758\n",
      "Epoch 70, loss = 0.7456, val.loss = 0.9757, val.acc = 0.6758\n",
      "Epoch 71, loss = 0.7427, val.loss = 0.9752, val.acc = 0.6756\n",
      "Epoch 72, loss = 0.7397, val.loss = 0.9746, val.acc = 0.6762\n",
      "Epoch 73, loss = 0.7368, val.loss = 0.9741, val.acc = 0.6766\n",
      "Epoch 74, loss = 0.7340, val.loss = 0.9736, val.acc = 0.6774\n",
      "Epoch 75, loss = 0.7312, val.loss = 0.9732, val.acc = 0.6778\n",
      "Epoch 76, loss = 0.7284, val.loss = 0.9728, val.acc = 0.6782\n",
      "Epoch 77, loss = 0.7256, val.loss = 0.9723, val.acc = 0.6790\n",
      "Epoch 78, loss = 0.7229, val.loss = 0.9719, val.acc = 0.6792\n",
      "Epoch 79, loss = 0.7202, val.loss = 0.9716, val.acc = 0.6794\n",
      "Epoch 80, loss = 0.7176, val.loss = 0.9712, val.acc = 0.6794\n",
      "Epoch 81, loss = 0.7150, val.loss = 0.9708, val.acc = 0.6796\n",
      "Epoch 82, loss = 0.7124, val.loss = 0.9705, val.acc = 0.6796\n",
      "Epoch 83, loss = 0.7098, val.loss = 0.9702, val.acc = 0.6792\n",
      "Epoch 84, loss = 0.7072, val.loss = 0.9699, val.acc = 0.6794\n",
      "Epoch 85, loss = 0.7047, val.loss = 0.9696, val.acc = 0.6790\n",
      "Epoch 86, loss = 0.7022, val.loss = 0.9694, val.acc = 0.6790\n",
      "Epoch 87, loss = 0.6998, val.loss = 0.9691, val.acc = 0.6796\n",
      "Epoch 88, loss = 0.6974, val.loss = 0.9689, val.acc = 0.6796\n",
      "Epoch 89, loss = 0.6949, val.loss = 0.9687, val.acc = 0.6806\n",
      "Epoch 90, loss = 0.6926, val.loss = 0.9685, val.acc = 0.6812\n",
      "Epoch 91, loss = 0.6902, val.loss = 0.9683, val.acc = 0.6810\n",
      "Epoch 92, loss = 0.6879, val.loss = 0.9681, val.acc = 0.6802\n",
      "Epoch 93, loss = 0.6855, val.loss = 0.9679, val.acc = 0.6802\n",
      "Epoch 94, loss = 0.6833, val.loss = 0.9678, val.acc = 0.6802\n",
      "Epoch 95, loss = 0.6810, val.loss = 0.9676, val.acc = 0.6808\n",
      "Epoch 96, loss = 0.6787, val.loss = 0.9675, val.acc = 0.6808\n",
      "Epoch 97, loss = 0.6765, val.loss = 0.9674, val.acc = 0.6802\n",
      "Epoch 98, loss = 0.6743, val.loss = 0.9673, val.acc = 0.6798\n",
      "Epoch 99, loss = 0.6721, val.loss = 0.9672, val.acc = 0.6800\n",
      "Rep: 1, te.acc = 0.6580\n",
      "\n",
      "All reps test.acc:\n",
      "[0.658]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 5e-5\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_5e-5')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0001_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.7791, val.loss = 1.4981, val.acc = 0.5184\n",
      "Epoch 1, loss = 1.4033, val.loss = 1.3373, val.acc = 0.5624\n",
      "Epoch 2, loss = 1.2807, val.loss = 1.2574, val.acc = 0.5840\n",
      "Epoch 3, loss = 1.2085, val.loss = 1.2077, val.acc = 0.6022\n",
      "Epoch 4, loss = 1.1581, val.loss = 1.1730, val.acc = 0.6154\n",
      "Epoch 5, loss = 1.1195, val.loss = 1.1469, val.acc = 0.6254\n",
      "Epoch 6, loss = 1.0881, val.loss = 1.1264, val.acc = 0.6316\n",
      "Epoch 7, loss = 1.0616, val.loss = 1.1096, val.acc = 0.6360\n",
      "Epoch 8, loss = 1.0386, val.loss = 1.0956, val.acc = 0.6400\n",
      "Epoch 9, loss = 1.0182, val.loss = 1.0836, val.acc = 0.6414\n",
      "Epoch 10, loss = 0.9998, val.loss = 1.0732, val.acc = 0.6438\n",
      "Epoch 11, loss = 0.9830, val.loss = 1.0641, val.acc = 0.6460\n",
      "Epoch 12, loss = 0.9675, val.loss = 1.0561, val.acc = 0.6476\n",
      "Epoch 13, loss = 0.9532, val.loss = 1.0489, val.acc = 0.6510\n",
      "Epoch 14, loss = 0.9398, val.loss = 1.0424, val.acc = 0.6522\n",
      "Epoch 15, loss = 0.9272, val.loss = 1.0365, val.acc = 0.6542\n",
      "Epoch 16, loss = 0.9154, val.loss = 1.0312, val.acc = 0.6566\n",
      "Epoch 17, loss = 0.9041, val.loss = 1.0264, val.acc = 0.6576\n",
      "Epoch 18, loss = 0.8934, val.loss = 1.0219, val.acc = 0.6598\n",
      "Epoch 19, loss = 0.8833, val.loss = 1.0178, val.acc = 0.6602\n",
      "Epoch 20, loss = 0.8735, val.loss = 1.0141, val.acc = 0.6608\n",
      "Epoch 21, loss = 0.8642, val.loss = 1.0106, val.acc = 0.6606\n",
      "Epoch 22, loss = 0.8553, val.loss = 1.0074, val.acc = 0.6624\n",
      "Epoch 23, loss = 0.8467, val.loss = 1.0045, val.acc = 0.6624\n",
      "Epoch 24, loss = 0.8384, val.loss = 1.0017, val.acc = 0.6648\n",
      "Epoch 25, loss = 0.8304, val.loss = 0.9992, val.acc = 0.6652\n",
      "Epoch 26, loss = 0.8227, val.loss = 0.9968, val.acc = 0.6668\n",
      "Epoch 27, loss = 0.8153, val.loss = 0.9947, val.acc = 0.6666\n",
      "Epoch 28, loss = 0.8080, val.loss = 0.9926, val.acc = 0.6668\n",
      "Epoch 29, loss = 0.8010, val.loss = 0.9907, val.acc = 0.6678\n",
      "Epoch 30, loss = 0.7942, val.loss = 0.9890, val.acc = 0.6688\n",
      "Epoch 31, loss = 0.7876, val.loss = 0.9874, val.acc = 0.6706\n",
      "Epoch 32, loss = 0.7812, val.loss = 0.9859, val.acc = 0.6716\n",
      "Epoch 33, loss = 0.7750, val.loss = 0.9845, val.acc = 0.6712\n",
      "Epoch 34, loss = 0.7689, val.loss = 0.9832, val.acc = 0.6710\n",
      "Epoch 35, loss = 0.7630, val.loss = 0.9820, val.acc = 0.6714\n",
      "Epoch 36, loss = 0.7572, val.loss = 0.9809, val.acc = 0.6714\n",
      "Epoch 37, loss = 0.7515, val.loss = 0.9799, val.acc = 0.6718\n",
      "Epoch 38, loss = 0.7460, val.loss = 0.9790, val.acc = 0.6728\n",
      "Epoch 39, loss = 0.7406, val.loss = 0.9781, val.acc = 0.6730\n",
      "Epoch 40, loss = 0.7354, val.loss = 0.9773, val.acc = 0.6740\n",
      "Epoch 41, loss = 0.7302, val.loss = 0.9766, val.acc = 0.6746\n",
      "Epoch 42, loss = 0.7252, val.loss = 0.9760, val.acc = 0.6744\n",
      "Epoch 43, loss = 0.7203, val.loss = 0.9754, val.acc = 0.6734\n",
      "Epoch 44, loss = 0.7154, val.loss = 0.9749, val.acc = 0.6734\n",
      "Epoch 45, loss = 0.7107, val.loss = 0.9744, val.acc = 0.6738\n",
      "Epoch 46, loss = 0.7061, val.loss = 0.9740, val.acc = 0.6744\n",
      "Epoch 47, loss = 0.7015, val.loss = 0.9736, val.acc = 0.6748\n",
      "Epoch 48, loss = 0.6971, val.loss = 0.9733, val.acc = 0.6748\n",
      "Epoch 49, loss = 0.6927, val.loss = 0.9731, val.acc = 0.6754\n",
      "Epoch 50, loss = 0.6884, val.loss = 0.9728, val.acc = 0.6764\n",
      "Epoch 51, loss = 0.6842, val.loss = 0.9727, val.acc = 0.6764\n",
      "Epoch 52, loss = 0.6800, val.loss = 0.9725, val.acc = 0.6766\n",
      "Epoch 53, loss = 0.6760, val.loss = 0.9724, val.acc = 0.6770\n",
      "Epoch 54, loss = 0.6720, val.loss = 0.9724, val.acc = 0.6770\n",
      "Epoch 55, loss = 0.6680, val.loss = 0.9724, val.acc = 0.6784\n",
      "Epoch 56, loss = 0.6642, val.loss = 0.9724, val.acc = 0.6780\n",
      "Epoch 57, loss = 0.6603, val.loss = 0.9725, val.acc = 0.6778\n",
      "Epoch 58, loss = 0.6566, val.loss = 0.9726, val.acc = 0.6768\n",
      "Epoch 59, loss = 0.6529, val.loss = 0.9727, val.acc = 0.6768\n",
      "Epoch 60, loss = 0.6493, val.loss = 0.9728, val.acc = 0.6764\n",
      "Epoch 61, loss = 0.6457, val.loss = 0.9730, val.acc = 0.6756\n",
      "Epoch 62, loss = 0.6422, val.loss = 0.9732, val.acc = 0.6764\n",
      "Epoch 63, loss = 0.6387, val.loss = 0.9734, val.acc = 0.6756\n",
      "Epoch 64, loss = 0.6353, val.loss = 0.9737, val.acc = 0.6752\n",
      "Epoch 65, loss = 0.6320, val.loss = 0.9739, val.acc = 0.6754\n",
      "Epoch 66, loss = 0.6286, val.loss = 0.9743, val.acc = 0.6756\n",
      "Epoch 67, loss = 0.6254, val.loss = 0.9746, val.acc = 0.6758\n",
      "Epoch 68, loss = 0.6221, val.loss = 0.9750, val.acc = 0.6764\n",
      "Epoch 69, loss = 0.6190, val.loss = 0.9754, val.acc = 0.6774\n",
      "Epoch 70, loss = 0.6158, val.loss = 0.9757, val.acc = 0.6772\n",
      "Epoch 71, loss = 0.6127, val.loss = 0.9762, val.acc = 0.6774\n",
      "Epoch 72, loss = 0.6097, val.loss = 0.9766, val.acc = 0.6772\n",
      "Epoch 73, loss = 0.6067, val.loss = 0.9771, val.acc = 0.6768\n",
      "Epoch 74, loss = 0.6037, val.loss = 0.9776, val.acc = 0.6770\n",
      "Epoch 75, loss = 0.6008, val.loss = 0.9780, val.acc = 0.6770\n",
      "Epoch 76, loss = 0.5979, val.loss = 0.9786, val.acc = 0.6768\n",
      "Epoch 77, loss = 0.5950, val.loss = 0.9791, val.acc = 0.6766\n",
      "Epoch 78, loss = 0.5922, val.loss = 0.9797, val.acc = 0.6768\n",
      "Epoch 79, loss = 0.5894, val.loss = 0.9802, val.acc = 0.6762\n",
      "Epoch 80, loss = 0.5866, val.loss = 0.9808, val.acc = 0.6762\n",
      "Epoch 81, loss = 0.5839, val.loss = 0.9814, val.acc = 0.6764\n",
      "Epoch 82, loss = 0.5812, val.loss = 0.9820, val.acc = 0.6752\n",
      "Epoch 83, loss = 0.5786, val.loss = 0.9826, val.acc = 0.6756\n",
      "Epoch 84, loss = 0.5759, val.loss = 0.9833, val.acc = 0.6760\n",
      "Epoch 85, loss = 0.5733, val.loss = 0.9839, val.acc = 0.6758\n",
      "Epoch 86, loss = 0.5708, val.loss = 0.9846, val.acc = 0.6756\n",
      "Epoch 87, loss = 0.5682, val.loss = 0.9853, val.acc = 0.6758\n",
      "Epoch 88, loss = 0.5657, val.loss = 0.9860, val.acc = 0.6756\n",
      "Epoch 89, loss = 0.5632, val.loss = 0.9867, val.acc = 0.6754\n",
      "Epoch 90, loss = 0.5608, val.loss = 0.9874, val.acc = 0.6754\n",
      "Epoch 91, loss = 0.5583, val.loss = 0.9881, val.acc = 0.6754\n",
      "Epoch 92, loss = 0.5559, val.loss = 0.9888, val.acc = 0.6748\n",
      "Epoch 93, loss = 0.5536, val.loss = 0.9896, val.acc = 0.6740\n",
      "Epoch 94, loss = 0.5512, val.loss = 0.9903, val.acc = 0.6736\n",
      "Epoch 95, loss = 0.5489, val.loss = 0.9911, val.acc = 0.6736\n",
      "Epoch 96, loss = 0.5466, val.loss = 0.9919, val.acc = 0.6734\n",
      "Epoch 97, loss = 0.5443, val.loss = 0.9926, val.acc = 0.6732\n",
      "Epoch 98, loss = 0.5420, val.loss = 0.9934, val.acc = 0.6724\n",
      "Epoch 99, loss = 0.5398, val.loss = 0.9942, val.acc = 0.6722\n",
      "Rep: 1, te.acc = 0.6526\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6526]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 0.0001\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_0_0001')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0002_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.6284, val.loss = 1.3489, val.acc = 0.5592\n",
      "Epoch 1, loss = 1.2695, val.loss = 1.2183, val.acc = 0.5980\n",
      "Epoch 2, loss = 1.1627, val.loss = 1.1576, val.acc = 0.6210\n",
      "Epoch 3, loss = 1.0983, val.loss = 1.1206, val.acc = 0.6334\n",
      "Epoch 4, loss = 1.0516, val.loss = 1.0950, val.acc = 0.6388\n",
      "Epoch 5, loss = 1.0145, val.loss = 1.0760, val.acc = 0.6450\n",
      "Epoch 6, loss = 0.9835, val.loss = 1.0611, val.acc = 0.6498\n",
      "Epoch 7, loss = 0.9567, val.loss = 1.0491, val.acc = 0.6532\n",
      "Epoch 8, loss = 0.9331, val.loss = 1.0391, val.acc = 0.6570\n",
      "Epoch 9, loss = 0.9118, val.loss = 1.0306, val.acc = 0.6600\n",
      "Epoch 10, loss = 0.8924, val.loss = 1.0234, val.acc = 0.6628\n",
      "Epoch 11, loss = 0.8746, val.loss = 1.0172, val.acc = 0.6620\n",
      "Epoch 12, loss = 0.8582, val.loss = 1.0118, val.acc = 0.6636\n",
      "Epoch 13, loss = 0.8428, val.loss = 1.0071, val.acc = 0.6630\n",
      "Epoch 14, loss = 0.8285, val.loss = 1.0030, val.acc = 0.6648\n",
      "Epoch 15, loss = 0.8150, val.loss = 0.9994, val.acc = 0.6678\n",
      "Epoch 16, loss = 0.8022, val.loss = 0.9963, val.acc = 0.6692\n",
      "Epoch 17, loss = 0.7901, val.loss = 0.9935, val.acc = 0.6708\n",
      "Epoch 18, loss = 0.7786, val.loss = 0.9912, val.acc = 0.6722\n",
      "Epoch 19, loss = 0.7677, val.loss = 0.9891, val.acc = 0.6724\n",
      "Epoch 20, loss = 0.7572, val.loss = 0.9873, val.acc = 0.6712\n",
      "Epoch 21, loss = 0.7472, val.loss = 0.9859, val.acc = 0.6726\n",
      "Epoch 22, loss = 0.7377, val.loss = 0.9846, val.acc = 0.6724\n",
      "Epoch 23, loss = 0.7285, val.loss = 0.9836, val.acc = 0.6736\n",
      "Epoch 24, loss = 0.7196, val.loss = 0.9827, val.acc = 0.6756\n",
      "Epoch 25, loss = 0.7111, val.loss = 0.9821, val.acc = 0.6746\n",
      "Epoch 26, loss = 0.7029, val.loss = 0.9816, val.acc = 0.6746\n",
      "Epoch 27, loss = 0.6949, val.loss = 0.9813, val.acc = 0.6750\n",
      "Epoch 28, loss = 0.6872, val.loss = 0.9811, val.acc = 0.6756\n",
      "Epoch 29, loss = 0.6798, val.loss = 0.9811, val.acc = 0.6746\n",
      "Epoch 30, loss = 0.6726, val.loss = 0.9812, val.acc = 0.6740\n",
      "Epoch 31, loss = 0.6656, val.loss = 0.9814, val.acc = 0.6730\n",
      "Epoch 32, loss = 0.6588, val.loss = 0.9817, val.acc = 0.6732\n",
      "Epoch 33, loss = 0.6522, val.loss = 0.9821, val.acc = 0.6740\n",
      "Epoch 34, loss = 0.6458, val.loss = 0.9827, val.acc = 0.6740\n",
      "Epoch 35, loss = 0.6396, val.loss = 0.9833, val.acc = 0.6746\n",
      "Epoch 36, loss = 0.6335, val.loss = 0.9840, val.acc = 0.6748\n",
      "Epoch 37, loss = 0.6276, val.loss = 0.9848, val.acc = 0.6754\n",
      "Epoch 38, loss = 0.6218, val.loss = 0.9856, val.acc = 0.6742\n",
      "Epoch 39, loss = 0.6161, val.loss = 0.9865, val.acc = 0.6732\n",
      "Epoch 40, loss = 0.6106, val.loss = 0.9875, val.acc = 0.6728\n",
      "Epoch 41, loss = 0.6052, val.loss = 0.9885, val.acc = 0.6718\n",
      "Epoch 42, loss = 0.6000, val.loss = 0.9896, val.acc = 0.6710\n",
      "Epoch 43, loss = 0.5948, val.loss = 0.9908, val.acc = 0.6714\n",
      "Epoch 44, loss = 0.5898, val.loss = 0.9920, val.acc = 0.6710\n",
      "Epoch 45, loss = 0.5848, val.loss = 0.9932, val.acc = 0.6698\n",
      "Epoch 46, loss = 0.5800, val.loss = 0.9945, val.acc = 0.6700\n",
      "Epoch 47, loss = 0.5753, val.loss = 0.9958, val.acc = 0.6694\n",
      "Epoch 48, loss = 0.5707, val.loss = 0.9971, val.acc = 0.6696\n",
      "Epoch 49, loss = 0.5661, val.loss = 0.9985, val.acc = 0.6696\n",
      "Epoch 50, loss = 0.5617, val.loss = 0.9999, val.acc = 0.6696\n",
      "Epoch 51, loss = 0.5573, val.loss = 1.0013, val.acc = 0.6698\n",
      "Epoch 52, loss = 0.5530, val.loss = 1.0027, val.acc = 0.6704\n",
      "Epoch 53, loss = 0.5489, val.loss = 1.0042, val.acc = 0.6702\n",
      "Epoch 54, loss = 0.5447, val.loss = 1.0057, val.acc = 0.6702\n",
      "Epoch 55, loss = 0.5407, val.loss = 1.0072, val.acc = 0.6690\n",
      "Epoch 56, loss = 0.5367, val.loss = 1.0087, val.acc = 0.6690\n",
      "Epoch 57, loss = 0.5329, val.loss = 1.0103, val.acc = 0.6684\n",
      "Epoch 58, loss = 0.5290, val.loss = 1.0118, val.acc = 0.6686\n",
      "Epoch 59, loss = 0.5253, val.loss = 1.0135, val.acc = 0.6696\n",
      "Epoch 60, loss = 0.5216, val.loss = 1.0151, val.acc = 0.6688\n",
      "Epoch 61, loss = 0.5180, val.loss = 1.0167, val.acc = 0.6686\n",
      "Epoch 62, loss = 0.5145, val.loss = 1.0183, val.acc = 0.6680\n",
      "Epoch 63, loss = 0.5110, val.loss = 1.0200, val.acc = 0.6676\n",
      "Epoch 64, loss = 0.5076, val.loss = 1.0216, val.acc = 0.6676\n",
      "Epoch 65, loss = 0.5043, val.loss = 1.0233, val.acc = 0.6672\n",
      "Epoch 66, loss = 0.5010, val.loss = 1.0250, val.acc = 0.6676\n",
      "Epoch 67, loss = 0.4978, val.loss = 1.0268, val.acc = 0.6680\n",
      "Epoch 68, loss = 0.4947, val.loss = 1.0285, val.acc = 0.6678\n",
      "Epoch 69, loss = 0.4917, val.loss = 1.0303, val.acc = 0.6680\n",
      "Epoch 70, loss = 0.4887, val.loss = 1.0321, val.acc = 0.6662\n",
      "Epoch 71, loss = 0.4858, val.loss = 1.0339, val.acc = 0.6662\n",
      "Epoch 72, loss = 0.4830, val.loss = 1.0358, val.acc = 0.6644\n",
      "Epoch 73, loss = 0.4803, val.loss = 1.0378, val.acc = 0.6628\n",
      "Epoch 74, loss = 0.4777, val.loss = 1.0398, val.acc = 0.6646\n",
      "Epoch 75, loss = 0.4752, val.loss = 1.0420, val.acc = 0.6634\n",
      "Epoch 76, loss = 0.4728, val.loss = 1.0444, val.acc = 0.6642\n",
      "Epoch 77, loss = 0.4705, val.loss = 1.0471, val.acc = 0.6628\n",
      "Epoch 78, loss = 0.4684, val.loss = 1.0501, val.acc = 0.6622\n",
      "Epoch 79, loss = 0.4662, val.loss = 1.0536, val.acc = 0.6634\n",
      "Epoch 80, loss = 0.4642, val.loss = 1.0575, val.acc = 0.6626\n",
      "Epoch 81, loss = 0.4620, val.loss = 1.0617, val.acc = 0.6604\n",
      "Epoch 82, loss = 0.4598, val.loss = 1.0658, val.acc = 0.6598\n",
      "Epoch 83, loss = 0.4575, val.loss = 1.0696, val.acc = 0.6582\n",
      "Epoch 84, loss = 0.4550, val.loss = 1.0729, val.acc = 0.6574\n",
      "Epoch 85, loss = 0.4525, val.loss = 1.0758, val.acc = 0.6576\n",
      "Epoch 86, loss = 0.4499, val.loss = 1.0781, val.acc = 0.6578\n",
      "Epoch 87, loss = 0.4473, val.loss = 1.0802, val.acc = 0.6566\n",
      "Epoch 88, loss = 0.4449, val.loss = 1.0820, val.acc = 0.6564\n",
      "Epoch 89, loss = 0.4425, val.loss = 1.0837, val.acc = 0.6554\n",
      "Epoch 90, loss = 0.4402, val.loss = 1.0854, val.acc = 0.6546\n",
      "Epoch 91, loss = 0.4381, val.loss = 1.0871, val.acc = 0.6552\n",
      "Epoch 92, loss = 0.4361, val.loss = 1.0888, val.acc = 0.6548\n",
      "Epoch 93, loss = 0.4342, val.loss = 1.0908, val.acc = 0.6552\n",
      "Epoch 94, loss = 0.4323, val.loss = 1.0931, val.acc = 0.6552\n",
      "Epoch 95, loss = 0.4305, val.loss = 1.0958, val.acc = 0.6558\n",
      "Epoch 96, loss = 0.4287, val.loss = 1.0988, val.acc = 0.6554\n",
      "Epoch 97, loss = 0.4268, val.loss = 1.1020, val.acc = 0.6546\n",
      "Epoch 98, loss = 0.4248, val.loss = 1.1050, val.acc = 0.6544\n",
      "Epoch 99, loss = 0.4228, val.loss = 1.1076, val.acc = 0.6538\n",
      "Rep: 1, te.acc = 0.6330\n",
      "\n",
      "All reps test.acc:\n",
      "[0.633]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 0.0002\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_0_0002')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.5039, val.loss = 1.2210, val.acc = 0.5888\n",
      "Epoch 1, loss = 1.1506, val.loss = 1.1252, val.acc = 0.6192\n",
      "Epoch 2, loss = 1.0521, val.loss = 1.0784, val.acc = 0.6372\n",
      "Epoch 3, loss = 0.9876, val.loss = 1.0502, val.acc = 0.6492\n",
      "Epoch 4, loss = 0.9389, val.loss = 1.0310, val.acc = 0.6562\n",
      "Epoch 5, loss = 0.8997, val.loss = 1.0174, val.acc = 0.6604\n",
      "Epoch 6, loss = 0.8667, val.loss = 1.0088, val.acc = 0.6646\n",
      "Epoch 7, loss = 0.8375, val.loss = 1.0051, val.acc = 0.6666\n",
      "Epoch 8, loss = 0.8111, val.loss = 1.0040, val.acc = 0.6686\n",
      "Epoch 9, loss = 0.7872, val.loss = 1.0044, val.acc = 0.6698\n",
      "Epoch 10, loss = 0.7654, val.loss = 1.0061, val.acc = 0.6664\n",
      "Epoch 11, loss = 0.7453, val.loss = 1.0086, val.acc = 0.6638\n",
      "Epoch 12, loss = 0.7266, val.loss = 1.0113, val.acc = 0.6636\n",
      "Epoch 13, loss = 0.7090, val.loss = 1.0135, val.acc = 0.6614\n",
      "Epoch 14, loss = 0.6924, val.loss = 1.0151, val.acc = 0.6608\n",
      "Epoch 15, loss = 0.6766, val.loss = 1.0161, val.acc = 0.6594\n",
      "Epoch 16, loss = 0.6618, val.loss = 1.0171, val.acc = 0.6598\n",
      "Epoch 17, loss = 0.6477, val.loss = 1.0180, val.acc = 0.6604\n",
      "Epoch 18, loss = 0.6345, val.loss = 1.0192, val.acc = 0.6614\n",
      "Epoch 19, loss = 0.6219, val.loss = 1.0206, val.acc = 0.6622\n",
      "Epoch 20, loss = 0.6101, val.loss = 1.0224, val.acc = 0.6608\n",
      "Epoch 21, loss = 0.5988, val.loss = 1.0244, val.acc = 0.6598\n",
      "Epoch 22, loss = 0.5880, val.loss = 1.0268, val.acc = 0.6602\n",
      "Epoch 23, loss = 0.5778, val.loss = 1.0294, val.acc = 0.6602\n",
      "Epoch 24, loss = 0.5681, val.loss = 1.0324, val.acc = 0.6604\n",
      "Epoch 25, loss = 0.5587, val.loss = 1.0355, val.acc = 0.6612\n",
      "Epoch 26, loss = 0.5498, val.loss = 1.0389, val.acc = 0.6612\n",
      "Epoch 27, loss = 0.5412, val.loss = 1.0424, val.acc = 0.6604\n",
      "Epoch 28, loss = 0.5330, val.loss = 1.0460, val.acc = 0.6600\n",
      "Epoch 29, loss = 0.5251, val.loss = 1.0498, val.acc = 0.6606\n",
      "Epoch 30, loss = 0.5176, val.loss = 1.0536, val.acc = 0.6594\n",
      "Epoch 31, loss = 0.5104, val.loss = 1.0575, val.acc = 0.6582\n",
      "Epoch 32, loss = 0.5035, val.loss = 1.0614, val.acc = 0.6582\n",
      "Epoch 33, loss = 0.4968, val.loss = 1.0655, val.acc = 0.6572\n",
      "Epoch 34, loss = 0.4905, val.loss = 1.0698, val.acc = 0.6568\n",
      "Epoch 35, loss = 0.4844, val.loss = 1.0748, val.acc = 0.6556\n",
      "Epoch 36, loss = 0.4786, val.loss = 1.0810, val.acc = 0.6536\n",
      "Epoch 37, loss = 0.4733, val.loss = 1.0884, val.acc = 0.6538\n",
      "Epoch 38, loss = 0.4686, val.loss = 1.0961, val.acc = 0.6532\n",
      "Epoch 39, loss = 0.4652, val.loss = 1.1011, val.acc = 0.6516\n",
      "Epoch 40, loss = 0.4627, val.loss = 1.1044, val.acc = 0.6510\n",
      "Epoch 41, loss = 0.4600, val.loss = 1.1137, val.acc = 0.6504\n",
      "Epoch 42, loss = 0.4568, val.loss = 1.1255, val.acc = 0.6502\n",
      "Epoch 43, loss = 0.4553, val.loss = 1.1428, val.acc = 0.6506\n",
      "Epoch 44, loss = 0.4551, val.loss = 1.1751, val.acc = 0.6418\n",
      "Epoch 45, loss = 0.4536, val.loss = 1.2011, val.acc = 0.6396\n",
      "Epoch 46, loss = 0.4516, val.loss = 1.1931, val.acc = 0.6416\n",
      "Epoch 47, loss = 0.4483, val.loss = 1.1779, val.acc = 0.6404\n",
      "Epoch 48, loss = 0.4423, val.loss = 1.1832, val.acc = 0.6402\n",
      "Epoch 49, loss = 0.4369, val.loss = 1.1937, val.acc = 0.6416\n",
      "Epoch 50, loss = 0.4340, val.loss = 1.2075, val.acc = 0.6360\n",
      "Epoch 51, loss = 0.4335, val.loss = 1.2141, val.acc = 0.6356\n",
      "Epoch 52, loss = 0.4359, val.loss = 1.2022, val.acc = 0.6388\n",
      "Epoch 53, loss = 0.4409, val.loss = 1.1897, val.acc = 0.6450\n",
      "Epoch 54, loss = 0.4439, val.loss = 1.2073, val.acc = 0.6378\n",
      "Epoch 55, loss = 0.4424, val.loss = 1.2517, val.acc = 0.6282\n",
      "Epoch 56, loss = 0.4497, val.loss = 1.2651, val.acc = 0.6270\n",
      "Epoch 57, loss = 0.4572, val.loss = 1.2574, val.acc = 0.6288\n",
      "Epoch 58, loss = 0.4493, val.loss = 1.2346, val.acc = 0.6358\n",
      "Epoch 59, loss = 0.4355, val.loss = 1.2445, val.acc = 0.6362\n",
      "Epoch 60, loss = 0.4227, val.loss = 1.2475, val.acc = 0.6350\n",
      "Epoch 61, loss = 0.4123, val.loss = 1.2452, val.acc = 0.6360\n",
      "Epoch 62, loss = 0.4028, val.loss = 1.2442, val.acc = 0.6380\n",
      "Epoch 63, loss = 0.3939, val.loss = 1.2454, val.acc = 0.6390\n",
      "Epoch 64, loss = 0.3859, val.loss = 1.2478, val.acc = 0.6396\n",
      "Epoch 65, loss = 0.3788, val.loss = 1.2505, val.acc = 0.6394\n",
      "Epoch 66, loss = 0.3725, val.loss = 1.2533, val.acc = 0.6384\n",
      "Epoch 67, loss = 0.3668, val.loss = 1.2562, val.acc = 0.6382\n",
      "Epoch 68, loss = 0.3616, val.loss = 1.2589, val.acc = 0.6398\n",
      "Epoch 69, loss = 0.3568, val.loss = 1.2617, val.acc = 0.6392\n",
      "Epoch 70, loss = 0.3523, val.loss = 1.2646, val.acc = 0.6380\n",
      "Epoch 71, loss = 0.3481, val.loss = 1.2675, val.acc = 0.6356\n",
      "Epoch 72, loss = 0.3441, val.loss = 1.2702, val.acc = 0.6350\n",
      "Epoch 73, loss = 0.3403, val.loss = 1.2732, val.acc = 0.6362\n",
      "Epoch 74, loss = 0.3367, val.loss = 1.2759, val.acc = 0.6362\n",
      "Epoch 75, loss = 0.3332, val.loss = 1.2786, val.acc = 0.6348\n",
      "Epoch 76, loss = 0.3299, val.loss = 1.2815, val.acc = 0.6352\n",
      "Epoch 77, loss = 0.3266, val.loss = 1.2845, val.acc = 0.6356\n",
      "Epoch 78, loss = 0.3235, val.loss = 1.2875, val.acc = 0.6356\n",
      "Epoch 79, loss = 0.3205, val.loss = 1.2908, val.acc = 0.6348\n",
      "Epoch 80, loss = 0.3176, val.loss = 1.2940, val.acc = 0.6344\n",
      "Epoch 81, loss = 0.3148, val.loss = 1.2973, val.acc = 0.6342\n",
      "Epoch 82, loss = 0.3120, val.loss = 1.3008, val.acc = 0.6352\n",
      "Epoch 83, loss = 0.3093, val.loss = 1.3045, val.acc = 0.6346\n",
      "Epoch 84, loss = 0.3066, val.loss = 1.3082, val.acc = 0.6342\n",
      "Epoch 85, loss = 0.3040, val.loss = 1.3121, val.acc = 0.6344\n",
      "Epoch 86, loss = 0.3015, val.loss = 1.3161, val.acc = 0.6346\n",
      "Epoch 87, loss = 0.2989, val.loss = 1.3204, val.acc = 0.6340\n",
      "Epoch 88, loss = 0.2965, val.loss = 1.3247, val.acc = 0.6332\n",
      "Epoch 89, loss = 0.2941, val.loss = 1.3292, val.acc = 0.6334\n",
      "Epoch 90, loss = 0.2917, val.loss = 1.3340, val.acc = 0.6330\n",
      "Epoch 91, loss = 0.2893, val.loss = 1.3389, val.acc = 0.6336\n",
      "Epoch 92, loss = 0.2870, val.loss = 1.3440, val.acc = 0.6328\n",
      "Epoch 93, loss = 0.2848, val.loss = 1.3492, val.acc = 0.6322\n",
      "Epoch 94, loss = 0.2826, val.loss = 1.3547, val.acc = 0.6318\n",
      "Epoch 95, loss = 0.2804, val.loss = 1.3603, val.acc = 0.6314\n",
      "Epoch 96, loss = 0.2783, val.loss = 1.3663, val.acc = 0.6320\n",
      "Epoch 97, loss = 0.2762, val.loss = 1.3724, val.acc = 0.6312\n",
      "Epoch 98, loss = 0.2743, val.loss = 1.3793, val.acc = 0.6312\n",
      "Epoch 99, loss = 0.2724, val.loss = 1.3862, val.acc = 0.6310\n",
      "Rep: 1, te.acc = 0.6130\n",
      "\n",
      "All reps test.acc:\n",
      "[0.613]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 0.0005\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_0_0005')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.5234, val.loss = 1.1641, val.acc = 0.6086\n",
      "Epoch 1, loss = 1.0839, val.loss = 1.0778, val.acc = 0.6340\n",
      "Epoch 2, loss = 0.9838, val.loss = 1.0451, val.acc = 0.6462\n",
      "Epoch 3, loss = 0.9145, val.loss = 1.0264, val.acc = 0.6546\n",
      "Epoch 4, loss = 0.8606, val.loss = 1.0160, val.acc = 0.6556\n",
      "Epoch 5, loss = 0.8172, val.loss = 1.0122, val.acc = 0.6560\n",
      "Epoch 6, loss = 0.7823, val.loss = 1.0153, val.acc = 0.6546\n",
      "Epoch 7, loss = 0.7554, val.loss = 1.0240, val.acc = 0.6526\n",
      "Epoch 8, loss = 0.7395, val.loss = 1.0431, val.acc = 0.6504\n",
      "Epoch 9, loss = 0.7158, val.loss = 1.1005, val.acc = 0.6348\n",
      "Epoch 10, loss = 0.6944, val.loss = 1.0789, val.acc = 0.6400\n",
      "Epoch 11, loss = 0.6750, val.loss = 1.0537, val.acc = 0.6494\n",
      "Epoch 12, loss = 0.6583, val.loss = 1.0524, val.acc = 0.6508\n",
      "Epoch 13, loss = 0.6417, val.loss = 1.1119, val.acc = 0.6426\n",
      "Epoch 14, loss = 0.6243, val.loss = 1.1256, val.acc = 0.6392\n",
      "Epoch 15, loss = 0.6110, val.loss = 1.1445, val.acc = 0.6358\n",
      "Epoch 16, loss = 0.6015, val.loss = 1.1719, val.acc = 0.6314\n",
      "Epoch 17, loss = 0.5981, val.loss = 1.2089, val.acc = 0.6268\n",
      "Epoch 18, loss = 0.6081, val.loss = 1.1992, val.acc = 0.6268\n",
      "Epoch 19, loss = 0.6123, val.loss = 1.1744, val.acc = 0.6370\n",
      "Epoch 20, loss = 0.6089, val.loss = 1.1691, val.acc = 0.6414\n",
      "Epoch 21, loss = 0.6087, val.loss = 1.2077, val.acc = 0.6354\n",
      "Epoch 22, loss = 0.6221, val.loss = 1.2604, val.acc = 0.6294\n",
      "Epoch 23, loss = 0.6249, val.loss = 1.2808, val.acc = 0.6246\n",
      "Epoch 24, loss = 0.6214, val.loss = 1.3229, val.acc = 0.6154\n",
      "Epoch 25, loss = 0.5907, val.loss = 1.2840, val.acc = 0.6196\n",
      "Epoch 26, loss = 0.5386, val.loss = 1.2687, val.acc = 0.6234\n",
      "Epoch 27, loss = 0.5046, val.loss = 1.2542, val.acc = 0.6266\n",
      "Epoch 28, loss = 0.4839, val.loss = 1.2461, val.acc = 0.6288\n",
      "Epoch 29, loss = 0.4695, val.loss = 1.2435, val.acc = 0.6308\n",
      "Epoch 30, loss = 0.4583, val.loss = 1.2450, val.acc = 0.6308\n",
      "Epoch 31, loss = 0.4494, val.loss = 1.2495, val.acc = 0.6306\n",
      "Epoch 32, loss = 0.4431, val.loss = 1.2570, val.acc = 0.6324\n",
      "Epoch 33, loss = 0.4401, val.loss = 1.2716, val.acc = 0.6324\n",
      "Epoch 34, loss = 0.4399, val.loss = 1.2827, val.acc = 0.6326\n",
      "Epoch 35, loss = 0.4364, val.loss = 1.2669, val.acc = 0.6340\n",
      "Epoch 36, loss = 0.4280, val.loss = 1.2635, val.acc = 0.6336\n",
      "Epoch 37, loss = 0.4206, val.loss = 1.2678, val.acc = 0.6336\n",
      "Epoch 38, loss = 0.4163, val.loss = 1.2759, val.acc = 0.6320\n",
      "Epoch 39, loss = 0.4156, val.loss = 1.2887, val.acc = 0.6318\n",
      "Epoch 40, loss = 0.4212, val.loss = 1.3154, val.acc = 0.6318\n",
      "Epoch 41, loss = 0.4364, val.loss = 1.3636, val.acc = 0.6306\n",
      "Epoch 42, loss = 0.4437, val.loss = 1.3547, val.acc = 0.6332\n",
      "Epoch 43, loss = 0.4380, val.loss = 1.3011, val.acc = 0.6410\n",
      "Epoch 44, loss = 0.4389, val.loss = 1.3414, val.acc = 0.6340\n",
      "Epoch 45, loss = 0.4335, val.loss = 1.4103, val.acc = 0.6280\n",
      "Epoch 46, loss = 0.4244, val.loss = 1.4507, val.acc = 0.6286\n",
      "Epoch 47, loss = 0.4156, val.loss = 1.4691, val.acc = 0.6286\n",
      "Epoch 48, loss = 0.4092, val.loss = 1.4596, val.acc = 0.6300\n",
      "Epoch 49, loss = 0.4085, val.loss = 1.4427, val.acc = 0.6296\n",
      "Epoch 50, loss = 0.4157, val.loss = 1.4346, val.acc = 0.6304\n",
      "Epoch 51, loss = 0.4314, val.loss = 1.4445, val.acc = 0.6246\n",
      "Epoch 52, loss = 0.4584, val.loss = 1.5042, val.acc = 0.6202\n",
      "Epoch 53, loss = 0.4680, val.loss = 1.6097, val.acc = 0.6046\n",
      "Epoch 54, loss = 0.4524, val.loss = 1.5755, val.acc = 0.6048\n",
      "Epoch 55, loss = 0.4327, val.loss = 1.6332, val.acc = 0.5942\n",
      "Epoch 56, loss = 0.4195, val.loss = 1.5740, val.acc = 0.6020\n",
      "Epoch 57, loss = 0.4115, val.loss = 1.5537, val.acc = 0.6058\n",
      "Epoch 58, loss = 0.4118, val.loss = 1.5280, val.acc = 0.6142\n",
      "Epoch 59, loss = 0.4049, val.loss = 1.5007, val.acc = 0.6182\n",
      "Epoch 60, loss = 0.3876, val.loss = 1.4971, val.acc = 0.6180\n",
      "Epoch 61, loss = 0.3740, val.loss = 1.5059, val.acc = 0.6160\n",
      "Epoch 62, loss = 0.3664, val.loss = 1.5252, val.acc = 0.6136\n",
      "Epoch 63, loss = 0.3633, val.loss = 1.5539, val.acc = 0.6106\n",
      "Epoch 64, loss = 0.3632, val.loss = 1.5816, val.acc = 0.6090\n",
      "Epoch 65, loss = 0.3654, val.loss = 1.6086, val.acc = 0.6080\n",
      "Epoch 66, loss = 0.3704, val.loss = 1.6334, val.acc = 0.6100\n",
      "Epoch 67, loss = 0.3788, val.loss = 1.6396, val.acc = 0.6120\n",
      "Epoch 68, loss = 0.3870, val.loss = 1.6233, val.acc = 0.6212\n",
      "Epoch 69, loss = 0.4008, val.loss = 1.6208, val.acc = 0.6222\n",
      "Epoch 70, loss = 0.4223, val.loss = 1.6006, val.acc = 0.6226\n",
      "Epoch 71, loss = 0.4271, val.loss = 1.6220, val.acc = 0.6168\n",
      "Epoch 72, loss = 0.4180, val.loss = 1.7042, val.acc = 0.6032\n",
      "Epoch 73, loss = 0.4085, val.loss = 1.7630, val.acc = 0.5946\n",
      "Epoch 74, loss = 0.4005, val.loss = 1.8252, val.acc = 0.5926\n",
      "Epoch 75, loss = 0.3899, val.loss = 1.8484, val.acc = 0.5980\n",
      "Epoch 76, loss = 0.3856, val.loss = 1.8240, val.acc = 0.6032\n",
      "Epoch 77, loss = 0.3843, val.loss = 1.8045, val.acc = 0.6090\n",
      "Epoch 78, loss = 0.3868, val.loss = 1.7897, val.acc = 0.6160\n",
      "Epoch 79, loss = 0.4215, val.loss = 1.6801, val.acc = 0.6264\n",
      "Epoch 80, loss = 0.4042, val.loss = 1.6575, val.acc = 0.6282\n",
      "Epoch 81, loss = 0.3713, val.loss = 1.7094, val.acc = 0.6196\n",
      "Epoch 82, loss = 0.3574, val.loss = 1.8145, val.acc = 0.6028\n",
      "Epoch 83, loss = 0.3562, val.loss = 1.8138, val.acc = 0.6042\n",
      "Epoch 84, loss = 0.3549, val.loss = 1.7133, val.acc = 0.6150\n",
      "Epoch 85, loss = 0.3212, val.loss = 1.7177, val.acc = 0.6104\n",
      "Epoch 86, loss = 0.3043, val.loss = 1.6999, val.acc = 0.6128\n",
      "Epoch 87, loss = 0.3015, val.loss = 1.6899, val.acc = 0.6140\n",
      "Epoch 88, loss = 0.3054, val.loss = 1.6865, val.acc = 0.6168\n",
      "Epoch 89, loss = 0.3144, val.loss = 1.6953, val.acc = 0.6182\n",
      "Epoch 90, loss = 0.3201, val.loss = 1.7234, val.acc = 0.6166\n",
      "Epoch 91, loss = 0.3138, val.loss = 1.7306, val.acc = 0.6150\n",
      "Epoch 92, loss = 0.3074, val.loss = 1.7503, val.acc = 0.6106\n",
      "Epoch 93, loss = 0.3067, val.loss = 1.7744, val.acc = 0.6094\n",
      "Epoch 94, loss = 0.3083, val.loss = 1.8048, val.acc = 0.6034\n",
      "Epoch 95, loss = 0.3102, val.loss = 1.8408, val.acc = 0.6006\n",
      "Epoch 96, loss = 0.3096, val.loss = 1.8686, val.acc = 0.6012\n",
      "Epoch 97, loss = 0.3063, val.loss = 1.8703, val.acc = 0.6008\n",
      "Epoch 98, loss = 0.3049, val.loss = 1.8609, val.acc = 0.6052\n",
      "Epoch 99, loss = 0.3075, val.loss = 1.8885, val.acc = 0.6094\n",
      "Rep: 1, te.acc = 0.5887\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5887]\n"
     ]
    }
   ],
   "source": [
    "pars.clf_lr = 0.001\n",
    "vis = visdom.Visdom(port=8097,env='clf_lr_0_001')\n",
    "train_unsupervised_ae(pars, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "LogisticRegression(n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "LogisticRegression(n_jobs=-1)\n",
      "loss = 2.4894, val.acc = 0.6262\n",
      "Rep: 1, te.acc = 0.6150\n",
      "\n",
      "All reps test.acc:\n",
      "[0.615]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = LogisticRegression(n_jobs=-1)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.001, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.001, n_jobs=-1)\n",
      "loss = 2.4093, val.acc = 0.6156\n",
      "Rep: 1, te.acc = 0.5982\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5982]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.001)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.005, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.005, n_jobs=-1)\n",
      "loss = 6.2010, val.acc = 0.5452\n",
      "Rep: 1, te.acc = 0.5255\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5255]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.005)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.01, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.01, n_jobs=-1)\n",
      "loss = 2.5640, val.acc = 0.6510\n",
      "Rep: 1, te.acc = 0.6282\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6282]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.01)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.02, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.02, n_jobs=-1)\n",
      "loss = 2.6903, val.acc = 0.6436\n",
      "Rep: 1, te.acc = 0.6182\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6182]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.02)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.05, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.05, n_jobs=-1)\n",
      "loss = 2.7702, val.acc = 0.6404\n",
      "Rep: 1, te.acc = 0.6224\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6224]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.05)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.1, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.1, n_jobs=-1)\n",
      "loss = 2.5072, val.acc = 0.6430\n",
      "Rep: 1, te.acc = 0.6260\n",
      "\n",
      "All reps test.acc:\n",
      "[0.626]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.1)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.5, n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(alpha=0.5, n_jobs=-1)\n",
      "loss = 2.5020, val.acc = 0.5952\n",
      "Rep: 1, te.acc = 0.5772\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5772]\n"
     ]
    }
   ],
   "source": [
    "sklearn_classifier = SGDClassifier(n_jobs=-1, alpha=0.5)\n",
    "train_unsupervised_ae(pars,sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.5883, time: 17.8460\n",
      "reconstruction loss = 0.0502, similarity loss: 0.5847\n",
      "Epoch 1, loss = 1.3603, time: 17.5071\n",
      "reconstruction loss = 0.0357, similarity loss: 0.6454\n",
      "Epoch 2, loss = 1.1156, time: 17.9950\n",
      "reconstruction loss = 0.0276, similarity loss: 0.5632\n",
      "Epoch 3, loss = 1.1495, time: 17.2289\n",
      "reconstruction loss = 0.0230, similarity loss: 0.6901\n",
      "Epoch 4, loss = 0.9448, time: 17.8710\n",
      "reconstruction loss = 0.0198, similarity loss: 0.5494\n",
      "Epoch 5, loss = 1.1307, time: 17.8129\n",
      "reconstruction loss = 0.0199, similarity loss: 0.7326\n",
      "Epoch 6, loss = 0.8710, time: 17.8140\n",
      "reconstruction loss = 0.0167, similarity loss: 0.5372\n",
      "Epoch 7, loss = 0.8600, time: 17.3319\n",
      "reconstruction loss = 0.0195, similarity loss: 0.4705\n",
      "Epoch 8, loss = 0.8255, time: 17.1527\n",
      "reconstruction loss = 0.0168, similarity loss: 0.4891\n",
      "Epoch 9, loss = 0.8418, time: 17.2410\n",
      "reconstruction loss = 0.0189, similarity loss: 0.4638\n",
      "Epoch 10, loss = 0.7058, time: 17.1151\n",
      "reconstruction loss = 0.0155, similarity loss: 0.3962\n",
      "Epoch 11, loss = 0.8076, time: 17.2160\n",
      "reconstruction loss = 0.0160, similarity loss: 0.4869\n",
      "Epoch 12, loss = 0.7422, time: 17.1657\n",
      "reconstruction loss = 0.0152, similarity loss: 0.4381\n",
      "Epoch 13, loss = 0.7181, time: 17.0759\n",
      "reconstruction loss = 0.0160, similarity loss: 0.3974\n",
      "Epoch 14, loss = 0.6556, time: 17.1493\n",
      "reconstruction loss = 0.0137, similarity loss: 0.3811\n",
      "Epoch 15, loss = 0.6913, time: 17.2478\n",
      "reconstruction loss = 0.0151, similarity loss: 0.3892\n",
      "Epoch 16, loss = 0.6452, time: 17.1334\n",
      "reconstruction loss = 0.0152, similarity loss: 0.3412\n",
      "Epoch 17, loss = 0.7232, time: 17.2350\n",
      "reconstruction loss = 0.0154, similarity loss: 0.4156\n",
      "Epoch 18, loss = 0.6674, time: 17.1431\n",
      "reconstruction loss = 0.0153, similarity loss: 0.3615\n",
      "Epoch 19, loss = 0.6847, time: 17.6803\n",
      "reconstruction loss = 0.0157, similarity loss: 0.3710\n",
      "Epoch 20, loss = 0.6291, time: 17.7204\n",
      "reconstruction loss = 0.0139, similarity loss: 0.3515\n",
      "Epoch 21, loss = 0.6010, time: 17.1718\n",
      "reconstruction loss = 0.0127, similarity loss: 0.3461\n",
      "Epoch 22, loss = 0.6085, time: 17.0123\n",
      "reconstruction loss = 0.0125, similarity loss: 0.3579\n",
      "Epoch 23, loss = 0.6330, time: 17.1603\n",
      "reconstruction loss = 0.0142, similarity loss: 0.3498\n",
      "Epoch 24, loss = 0.5916, time: 17.0510\n",
      "reconstruction loss = 0.0133, similarity loss: 0.3254\n",
      "Epoch 25, loss = 0.5983, time: 17.1918\n",
      "reconstruction loss = 0.0131, similarity loss: 0.3370\n",
      "Epoch 26, loss = 0.5826, time: 17.1300\n",
      "reconstruction loss = 0.0136, similarity loss: 0.3101\n",
      "Epoch 27, loss = 0.6053, time: 17.1299\n",
      "reconstruction loss = 0.0134, similarity loss: 0.3372\n",
      "Epoch 28, loss = 0.5842, time: 17.0058\n",
      "reconstruction loss = 0.0127, similarity loss: 0.3299\n",
      "Epoch 29, loss = 0.5304, time: 17.2173\n",
      "reconstruction loss = 0.0122, similarity loss: 0.2873\n",
      "Epoch 30, loss = 0.5359, time: 17.1018\n",
      "reconstruction loss = 0.0125, similarity loss: 0.2857\n",
      "Epoch 31, loss = 0.5333, time: 17.1618\n",
      "reconstruction loss = 0.0112, similarity loss: 0.3088\n",
      "Epoch 32, loss = 0.5144, time: 17.0939\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2882\n",
      "Epoch 33, loss = 0.5495, time: 17.1563\n",
      "reconstruction loss = 0.0123, similarity loss: 0.3037\n",
      "Epoch 34, loss = 0.5608, time: 17.0698\n",
      "reconstruction loss = 0.0120, similarity loss: 0.3213\n",
      "Epoch 35, loss = 0.5048, time: 17.6369\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2836\n",
      "Epoch 36, loss = 0.5310, time: 17.8275\n",
      "reconstruction loss = 0.0109, similarity loss: 0.3137\n",
      "Epoch 37, loss = 0.5621, time: 17.6803\n",
      "reconstruction loss = 0.0127, similarity loss: 0.3075\n",
      "Epoch 38, loss = 0.5203, time: 18.1649\n",
      "reconstruction loss = 0.0124, similarity loss: 0.2720\n",
      "Epoch 39, loss = 0.5925, time: 17.7607\n",
      "reconstruction loss = 0.0113, similarity loss: 0.3661\n",
      "Epoch 40, loss = 0.5359, time: 17.9382\n",
      "reconstruction loss = 0.0103, similarity loss: 0.3305\n",
      "Epoch 41, loss = 0.5065, time: 17.7885\n",
      "reconstruction loss = 0.0099, similarity loss: 0.3094\n",
      "Epoch 42, loss = 0.5173, time: 17.7709\n",
      "reconstruction loss = 0.0108, similarity loss: 0.3005\n",
      "Epoch 43, loss = 0.5076, time: 17.7623\n",
      "reconstruction loss = 0.0103, similarity loss: 0.3020\n",
      "Epoch 44, loss = 0.5540, time: 17.0932\n",
      "reconstruction loss = 0.0115, similarity loss: 0.3241\n",
      "Epoch 45, loss = 0.6346, time: 17.5374\n",
      "reconstruction loss = 0.0132, similarity loss: 0.3709\n",
      "Epoch 46, loss = 0.5966, time: 17.9215\n",
      "reconstruction loss = 0.0120, similarity loss: 0.3564\n",
      "Epoch 47, loss = 0.5085, time: 17.2134\n",
      "reconstruction loss = 0.0105, similarity loss: 0.2991\n",
      "Epoch 48, loss = 0.5253, time: 17.3349\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3109\n",
      "Epoch 49, loss = 0.4682, time: 17.6205\n",
      "reconstruction loss = 0.0100, similarity loss: 0.2677\n",
      "Epoch 50, loss = 0.4756, time: 17.4099\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2736\n",
      "Epoch 51, loss = 0.4915, time: 17.3572\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2940\n",
      "Epoch 52, loss = 0.5742, time: 17.2499\n",
      "reconstruction loss = 0.0111, similarity loss: 0.3518\n",
      "Epoch 53, loss = 0.5075, time: 17.2579\n",
      "reconstruction loss = 0.0101, similarity loss: 0.3049\n",
      "Epoch 54, loss = 0.4640, time: 17.2201\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2654\n",
      "Epoch 55, loss = 0.5386, time: 17.6364\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3247\n",
      "Epoch 56, loss = 0.4913, time: 17.8690\n",
      "reconstruction loss = 0.0094, similarity loss: 0.3040\n",
      "Epoch 57, loss = 0.4607, time: 17.6871\n",
      "reconstruction loss = 0.0106, similarity loss: 0.2478\n",
      "Epoch 58, loss = 0.4468, time: 17.6318\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2708\n",
      "Epoch 59, loss = 0.4534, time: 17.6434\n",
      "reconstruction loss = 0.0092, similarity loss: 0.2691\n",
      "Epoch 60, loss = 0.4707, time: 17.6023\n",
      "reconstruction loss = 0.0093, similarity loss: 0.2851\n",
      "Epoch 61, loss = 0.4776, time: 17.6943\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2823\n",
      "Epoch 62, loss = 0.4743, time: 17.7131\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2944\n",
      "Epoch 63, loss = 0.4760, time: 17.6591\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2732\n",
      "Epoch 64, loss = 0.4669, time: 17.6502\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2686\n",
      "Epoch 65, loss = 0.4872, time: 17.7257\n",
      "reconstruction loss = 0.0092, similarity loss: 0.3039\n",
      "Epoch 66, loss = 0.4652, time: 17.8060\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2862\n",
      "Epoch 67, loss = 0.4546, time: 17.7971\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2596\n",
      "Epoch 68, loss = 0.4587, time: 17.7202\n",
      "reconstruction loss = 0.0093, similarity loss: 0.2731\n",
      "Epoch 69, loss = 0.4590, time: 17.7122\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2714\n",
      "Epoch 70, loss = 0.4339, time: 17.6602\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2605\n",
      "Epoch 71, loss = 0.3929, time: 17.5831\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2215\n",
      "Epoch 72, loss = 0.5011, time: 17.6070\n",
      "reconstruction loss = 0.0110, similarity loss: 0.2804\n",
      "Epoch 73, loss = 0.4626, time: 17.6538\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2718\n",
      "Epoch 74, loss = 0.4565, time: 17.6193\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2795\n",
      "Epoch 75, loss = 0.5157, time: 17.4844\n",
      "reconstruction loss = 0.0110, similarity loss: 0.2949\n",
      "Epoch 76, loss = 0.5128, time: 17.7087\n",
      "reconstruction loss = 0.0096, similarity loss: 0.3200\n",
      "Epoch 77, loss = 0.4404, time: 17.5403\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2726\n",
      "Epoch 78, loss = 0.4692, time: 17.7380\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2966\n",
      "Epoch 79, loss = 0.4336, time: 17.6147\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2699\n",
      "Epoch 80, loss = 0.4394, time: 17.9695\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2602\n",
      "Epoch 81, loss = 0.3979, time: 18.1585\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2461\n",
      "Epoch 82, loss = 0.4563, time: 18.1432\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2743\n",
      "Epoch 83, loss = 0.4607, time: 18.2634\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2789\n",
      "Epoch 84, loss = 0.4558, time: 18.3358\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2760\n",
      "Epoch 85, loss = 0.4208, time: 18.2637\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2537\n",
      "Epoch 86, loss = 0.4450, time: 18.5081\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2995\n",
      "Epoch 87, loss = 0.4348, time: 18.3567\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2635\n",
      "Epoch 88, loss = 0.4235, time: 18.3190\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2622\n",
      "Epoch 89, loss = 0.4444, time: 18.6350\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2636\n",
      "Epoch 90, loss = 0.4202, time: 17.5156\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2388\n",
      "Epoch 91, loss = 0.4560, time: 17.5671\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2750\n",
      "Epoch 92, loss = 0.4172, time: 17.6471\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2728\n",
      "Epoch 93, loss = 0.4538, time: 17.5648\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2872\n",
      "Epoch 94, loss = 0.4658, time: 20.1888\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2887\n",
      "Epoch 95, loss = 0.4457, time: 17.9793\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2687\n",
      "Epoch 96, loss = 0.4799, time: 17.8120\n",
      "reconstruction loss = 0.0092, similarity loss: 0.2968\n",
      "Epoch 97, loss = 0.4262, time: 17.3686\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2610\n",
      "Epoch 98, loss = 0.4532, time: 17.6327\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2824\n",
      "Epoch 99, loss = 0.4711, time: 18.4265\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2886\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "loss = 2.7329, val.acc = 0.5918\n",
      "Rep: 1, te.acc = 0.5735\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5735]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_20_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=0, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T00:33:01.078568Z",
     "start_time": "2022-02-25T00:33:01.072567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 50\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 50\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T02:11:03.806209Z",
     "start_time": "2022-02-25T00:33:02.541912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_50\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 3.1972, time: 29.1962\n",
      "reconstruction loss = 0.0505, similarity loss: 0.6714\n",
      "Epoch 1, loss = 2.3577, time: 17.7477\n",
      "reconstruction loss = 0.0324, similarity loss: 0.7393\n",
      "Epoch 2, loss = 1.8067, time: 17.0167\n",
      "reconstruction loss = 0.0232, similarity loss: 0.6486\n",
      "Epoch 3, loss = 1.6327, time: 17.7776\n",
      "reconstruction loss = 0.0191, similarity loss: 0.6765\n",
      "Epoch 4, loss = 1.4194, time: 16.2257\n",
      "reconstruction loss = 0.0160, similarity loss: 0.6182\n",
      "Epoch 5, loss = 1.3038, time: 16.0997\n",
      "reconstruction loss = 0.0135, similarity loss: 0.6309\n",
      "Epoch 6, loss = 1.4257, time: 16.1634\n",
      "reconstruction loss = 0.0154, similarity loss: 0.6537\n",
      "Epoch 7, loss = 1.1346, time: 16.8816\n",
      "reconstruction loss = 0.0131, similarity loss: 0.4788\n",
      "Epoch 8, loss = 1.0751, time: 16.6482\n",
      "reconstruction loss = 0.0127, similarity loss: 0.4380\n",
      "Epoch 9, loss = 1.0501, time: 16.5047\n",
      "reconstruction loss = 0.0127, similarity loss: 0.4164\n",
      "Epoch 10, loss = 1.1564, time: 16.5686\n",
      "reconstruction loss = 0.0128, similarity loss: 0.5166\n",
      "Epoch 11, loss = 1.1622, time: 16.4238\n",
      "reconstruction loss = 0.0132, similarity loss: 0.5039\n",
      "Epoch 12, loss = 1.1486, time: 16.5334\n",
      "reconstruction loss = 0.0131, similarity loss: 0.4928\n",
      "Epoch 13, loss = 1.0547, time: 16.6235\n",
      "reconstruction loss = 0.0116, similarity loss: 0.4755\n",
      "Epoch 14, loss = 1.1048, time: 16.5575\n",
      "reconstruction loss = 0.0126, similarity loss: 0.4770\n",
      "Epoch 15, loss = 0.9819, time: 16.5539\n",
      "reconstruction loss = 0.0116, similarity loss: 0.3996\n",
      "Epoch 16, loss = 0.8939, time: 16.6058\n",
      "reconstruction loss = 0.0103, similarity loss: 0.3771\n",
      "Epoch 17, loss = 1.0067, time: 16.3267\n",
      "reconstruction loss = 0.0111, similarity loss: 0.4493\n",
      "Epoch 18, loss = 1.0045, time: 16.4938\n",
      "reconstruction loss = 0.0113, similarity loss: 0.4413\n",
      "Epoch 19, loss = 0.9862, time: 16.5647\n",
      "reconstruction loss = 0.0108, similarity loss: 0.4452\n",
      "Epoch 20, loss = 0.9191, time: 16.4905\n",
      "reconstruction loss = 0.0101, similarity loss: 0.4147\n",
      "Epoch 21, loss = 0.8778, time: 16.3997\n",
      "reconstruction loss = 0.0098, similarity loss: 0.3877\n",
      "Epoch 22, loss = 0.8637, time: 16.4692\n",
      "reconstruction loss = 0.0099, similarity loss: 0.3663\n",
      "Epoch 23, loss = 0.9272, time: 16.8302\n",
      "reconstruction loss = 0.0103, similarity loss: 0.4124\n",
      "Epoch 24, loss = 0.9325, time: 16.4753\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3953\n",
      "Epoch 25, loss = 0.8673, time: 16.6911\n",
      "reconstruction loss = 0.0097, similarity loss: 0.3833\n",
      "Epoch 26, loss = 0.8872, time: 16.5888\n",
      "reconstruction loss = 0.0101, similarity loss: 0.3809\n",
      "Epoch 27, loss = 0.9379, time: 16.3837\n",
      "reconstruction loss = 0.0104, similarity loss: 0.4165\n",
      "Epoch 28, loss = 0.8472, time: 16.5389\n",
      "reconstruction loss = 0.0095, similarity loss: 0.3724\n",
      "Epoch 29, loss = 0.9195, time: 17.2745\n",
      "reconstruction loss = 0.0105, similarity loss: 0.3966\n",
      "Epoch 30, loss = 0.8805, time: 17.0219\n",
      "reconstruction loss = 0.0103, similarity loss: 0.3639\n",
      "Epoch 31, loss = 0.9246, time: 17.2182\n",
      "reconstruction loss = 0.0098, similarity loss: 0.4331\n",
      "Epoch 32, loss = 0.8136, time: 17.7876\n",
      "reconstruction loss = 0.0092, similarity loss: 0.3534\n",
      "Epoch 33, loss = 0.8632, time: 17.0912\n",
      "reconstruction loss = 0.0091, similarity loss: 0.4106\n",
      "Epoch 34, loss = 0.7872, time: 16.4244\n",
      "reconstruction loss = 0.0092, similarity loss: 0.3265\n",
      "Epoch 35, loss = 0.8100, time: 16.5141\n",
      "reconstruction loss = 0.0085, similarity loss: 0.3844\n",
      "Epoch 36, loss = 0.7483, time: 16.3773\n",
      "reconstruction loss = 0.0085, similarity loss: 0.3209\n",
      "Epoch 37, loss = 0.8475, time: 16.7909\n",
      "reconstruction loss = 0.0103, similarity loss: 0.3338\n",
      "Epoch 38, loss = 0.8575, time: 16.9707\n",
      "reconstruction loss = 0.0088, similarity loss: 0.4169\n",
      "Epoch 39, loss = 0.7569, time: 16.6704\n",
      "reconstruction loss = 0.0087, similarity loss: 0.3215\n",
      "Epoch 40, loss = 0.8319, time: 16.6685\n",
      "reconstruction loss = 0.0097, similarity loss: 0.3445\n",
      "Epoch 41, loss = 0.7469, time: 16.9212\n",
      "reconstruction loss = 0.0079, similarity loss: 0.3498\n",
      "Epoch 42, loss = 0.7235, time: 16.7472\n",
      "reconstruction loss = 0.0081, similarity loss: 0.3191\n",
      "Epoch 43, loss = 0.8424, time: 16.9824\n",
      "reconstruction loss = 0.0091, similarity loss: 0.3876\n",
      "Epoch 44, loss = 0.7608, time: 17.3106\n",
      "reconstruction loss = 0.0085, similarity loss: 0.3353\n",
      "Epoch 45, loss = 0.7983, time: 17.6595\n",
      "reconstruction loss = 0.0087, similarity loss: 0.3631\n",
      "Epoch 46, loss = 0.7067, time: 17.6596\n",
      "reconstruction loss = 0.0073, similarity loss: 0.3442\n",
      "Epoch 47, loss = 0.7196, time: 17.6626\n",
      "reconstruction loss = 0.0080, similarity loss: 0.3210\n",
      "Epoch 48, loss = 0.7520, time: 18.9438\n",
      "reconstruction loss = 0.0082, similarity loss: 0.3423\n",
      "Epoch 49, loss = 0.7098, time: 19.4145\n",
      "reconstruction loss = 0.0076, similarity loss: 0.3297\n",
      "Epoch 50, loss = 0.6496, time: 18.4494\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2899\n",
      "Epoch 51, loss = 0.6810, time: 17.7339\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2870\n",
      "Epoch 52, loss = 0.6874, time: 17.8088\n",
      "reconstruction loss = 0.0075, similarity loss: 0.3128\n",
      "Epoch 53, loss = 0.7472, time: 17.8460\n",
      "reconstruction loss = 0.0078, similarity loss: 0.3575\n",
      "Epoch 54, loss = 0.6510, time: 17.7566\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2863\n",
      "Epoch 55, loss = 0.7876, time: 17.8612\n",
      "reconstruction loss = 0.0082, similarity loss: 0.3771\n",
      "Epoch 56, loss = 0.6985, time: 17.9481\n",
      "reconstruction loss = 0.0073, similarity loss: 0.3341\n",
      "Epoch 57, loss = 0.6538, time: 18.1371\n",
      "reconstruction loss = 0.0068, similarity loss: 0.3147\n",
      "Epoch 58, loss = 0.6260, time: 17.6783\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2942\n",
      "Epoch 59, loss = 0.7874, time: 17.8235\n",
      "reconstruction loss = 0.0081, similarity loss: 0.3810\n",
      "Epoch 60, loss = 0.6446, time: 17.8345\n",
      "reconstruction loss = 0.0065, similarity loss: 0.3171\n",
      "Epoch 61, loss = 0.8023, time: 17.8290\n",
      "reconstruction loss = 0.0082, similarity loss: 0.3903\n",
      "Epoch 62, loss = 0.6983, time: 17.6929\n",
      "reconstruction loss = 0.0074, similarity loss: 0.3258\n",
      "Epoch 63, loss = 0.7081, time: 17.6907\n",
      "reconstruction loss = 0.0070, similarity loss: 0.3574\n",
      "Epoch 64, loss = 0.6919, time: 17.6509\n",
      "reconstruction loss = 0.0070, similarity loss: 0.3421\n",
      "Epoch 65, loss = 0.6455, time: 17.7223\n",
      "reconstruction loss = 0.0068, similarity loss: 0.3061\n",
      "Epoch 66, loss = 0.6396, time: 17.7225\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2982\n",
      "Epoch 67, loss = 0.6057, time: 17.7659\n",
      "reconstruction loss = 0.0060, similarity loss: 0.3052\n",
      "Epoch 68, loss = 0.6141, time: 17.7365\n",
      "reconstruction loss = 0.0056, similarity loss: 0.3324\n",
      "Epoch 69, loss = 0.6989, time: 17.4537\n",
      "reconstruction loss = 0.0075, similarity loss: 0.3231\n",
      "Epoch 70, loss = 0.6870, time: 17.3344\n",
      "reconstruction loss = 0.0065, similarity loss: 0.3619\n",
      "Epoch 71, loss = 0.5611, time: 17.2155\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2715\n",
      "Epoch 72, loss = 0.6318, time: 17.3811\n",
      "reconstruction loss = 0.0060, similarity loss: 0.3307\n",
      "Epoch 73, loss = 0.5971, time: 17.1888\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2588\n",
      "Epoch 74, loss = 0.6536, time: 17.3710\n",
      "reconstruction loss = 0.0070, similarity loss: 0.3041\n",
      "Epoch 75, loss = 0.5956, time: 17.4677\n",
      "reconstruction loss = 0.0058, similarity loss: 0.3051\n",
      "Epoch 76, loss = 0.6132, time: 18.5980\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2963\n",
      "Epoch 77, loss = 0.5972, time: 18.2606\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2863\n",
      "Epoch 78, loss = 0.6362, time: 18.1415\n",
      "reconstruction loss = 0.0057, similarity loss: 0.3517\n",
      "Epoch 79, loss = 0.6154, time: 18.4322\n",
      "reconstruction loss = 0.0059, similarity loss: 0.3215\n",
      "Epoch 80, loss = 0.5883, time: 18.3537\n",
      "reconstruction loss = 0.0058, similarity loss: 0.3002\n",
      "Epoch 81, loss = 0.6201, time: 18.8470\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2882\n",
      "Epoch 82, loss = 0.6014, time: 19.5353\n",
      "reconstruction loss = 0.0059, similarity loss: 0.3061\n",
      "Epoch 83, loss = 0.5772, time: 18.6920\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2797\n",
      "Epoch 84, loss = 0.5719, time: 18.3979\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2732\n",
      "Epoch 85, loss = 0.6162, time: 18.9215\n",
      "reconstruction loss = 0.0062, similarity loss: 0.3085\n",
      "Epoch 86, loss = 0.5843, time: 18.7633\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2929\n",
      "Epoch 87, loss = 0.6297, time: 18.6463\n",
      "reconstruction loss = 0.0060, similarity loss: 0.3276\n",
      "Epoch 88, loss = 0.5429, time: 18.5304\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2647\n",
      "Epoch 89, loss = 0.5760, time: 18.4885\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2620\n",
      "Epoch 90, loss = 0.5765, time: 17.8276\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2500\n",
      "Epoch 91, loss = 0.6320, time: 17.0296\n",
      "reconstruction loss = 0.0061, similarity loss: 0.3275\n",
      "Epoch 92, loss = 0.5825, time: 17.2651\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2899\n",
      "Epoch 93, loss = 0.6313, time: 17.7108\n",
      "reconstruction loss = 0.0066, similarity loss: 0.3000\n",
      "Epoch 94, loss = 0.6037, time: 16.6359\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2744\n",
      "Epoch 95, loss = 0.5733, time: 16.5257\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2895\n",
      "Epoch 96, loss = 0.5985, time: 16.4735\n",
      "reconstruction loss = 0.0056, similarity loss: 0.3198\n",
      "Epoch 97, loss = 0.5985, time: 16.4936\n",
      "reconstruction loss = 0.0056, similarity loss: 0.3162\n",
      "Epoch 98, loss = 0.6368, time: 16.5725\n",
      "reconstruction loss = 0.0065, similarity loss: 0.3134\n",
      "Epoch 99, loss = 0.5946, time: 16.5595\n",
      "reconstruction loss = 0.0054, similarity loss: 0.3241\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "SGDClassifier(n_jobs=-1)\n",
      "loss = 3.4917, val.acc = 0.5840\n",
      "Rep: 1, te.acc = 0.5691\n",
      "\n",
      "All reps test.acc:\n",
      "[0.5691]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_50_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=0, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T02:11:03.822212Z",
     "start_time": "2022-02-25T02:11:03.808209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 100\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 100\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 100\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T03:37:21.444047Z",
     "start_time": "2022-02-25T02:11:03.823214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_100_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_50\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 3.8044, time: 16.6198\n",
      "reconstruction loss = 0.0637, similarity loss: 0.6175\n",
      "Epoch 1, loss = 2.5428, time: 17.1017\n",
      "reconstruction loss = 0.0342, similarity loss: 0.8324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28072/3081207618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8097\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lam_100_sklearn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msklearn_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_unsupervised_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msklearn_classifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msklearn_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_unsupervised_ae\u001b[1;34m(pars, criterion_re, criterion_sim, clf_criterion, sklearn_classifier, optimizer, vis)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mtrain_model_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_sim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Classifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_model_ae\u001b[1;34m(data, fix, model, decoder, pars, ep_loss, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[0mx_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mhardtanh\u001b[1;34m(input, min_val, max_val, inplace)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhardtanh_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_100_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=0, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T03:37:21.460050Z",
     "start_time": "2022-02-25T03:37:21.446047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.95\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 100\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 500\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T05:39:50.842279Z",
     "start_time": "2022-02-25T03:37:21.462051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_300_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_0.95\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.0750, time: 17.4149\n",
      "reconstruction loss = 0.0474, similarity loss: 0.5982\n",
      "Epoch 1, loss = 0.0658, time: 16.6508\n",
      "reconstruction loss = 0.0341, similarity loss: 0.6681\n",
      "Epoch 2, loss = 0.0578, time: 17.6970\n",
      "reconstruction loss = 0.0295, similarity loss: 0.5954\n",
      "Epoch 3, loss = 0.0482, time: 17.1919\n",
      "reconstruction loss = 0.0239, similarity loss: 0.5099\n",
      "Epoch 4, loss = 0.0409, time: 16.7988\n",
      "reconstruction loss = 0.0196, similarity loss: 0.4447\n",
      "Epoch 5, loss = 0.0491, time: 17.0570\n",
      "reconstruction loss = 0.0185, similarity loss: 0.6297\n",
      "Epoch 6, loss = 0.0428, time: 17.3189\n",
      "reconstruction loss = 0.0174, similarity loss: 0.5261\n",
      "Epoch 7, loss = 0.0417, time: 17.0468\n",
      "reconstruction loss = 0.0173, similarity loss: 0.5056\n",
      "Epoch 8, loss = 0.0430, time: 16.7648\n",
      "reconstruction loss = 0.0185, similarity loss: 0.5085\n",
      "Epoch 9, loss = 0.0391, time: 16.8208\n",
      "reconstruction loss = 0.0173, similarity loss: 0.4549\n",
      "Epoch 10, loss = 0.0381, time: 16.8548\n",
      "reconstruction loss = 0.0160, similarity loss: 0.4585\n",
      "Epoch 11, loss = 0.0362, time: 16.9838\n",
      "reconstruction loss = 0.0164, similarity loss: 0.4128\n",
      "Epoch 12, loss = 0.0374, time: 17.1489\n",
      "reconstruction loss = 0.0162, similarity loss: 0.4400\n",
      "Epoch 13, loss = 0.0350, time: 18.2141\n",
      "reconstruction loss = 0.0151, similarity loss: 0.4135\n",
      "Epoch 14, loss = 0.0316, time: 16.7558\n",
      "reconstruction loss = 0.0152, similarity loss: 0.3435\n",
      "Epoch 15, loss = 0.0290, time: 21.6366\n",
      "reconstruction loss = 0.0138, similarity loss: 0.3182\n",
      "Epoch 16, loss = 0.0312, time: 20.7206\n",
      "reconstruction loss = 0.0135, similarity loss: 0.3680\n",
      "Epoch 17, loss = 0.0320, time: 21.7168\n",
      "reconstruction loss = 0.0161, similarity loss: 0.3342\n",
      "Epoch 18, loss = 0.0296, time: 23.3411\n",
      "reconstruction loss = 0.0149, similarity loss: 0.3078\n",
      "Epoch 19, loss = 0.0299, time: 21.7893\n",
      "reconstruction loss = 0.0136, similarity loss: 0.3408\n",
      "Epoch 20, loss = 0.0287, time: 20.2383\n",
      "reconstruction loss = 0.0130, similarity loss: 0.3269\n",
      "Epoch 21, loss = 0.0312, time: 24.5193\n",
      "reconstruction loss = 0.0134, similarity loss: 0.3678\n",
      "Epoch 22, loss = 0.0265, time: 22.6825\n",
      "reconstruction loss = 0.0129, similarity loss: 0.2856\n",
      "Epoch 23, loss = 0.0285, time: 29.6186\n",
      "reconstruction loss = 0.0122, similarity loss: 0.3367\n",
      "Epoch 24, loss = 0.0263, time: 29.5923\n",
      "reconstruction loss = 0.0108, similarity loss: 0.3219\n",
      "Epoch 25, loss = 0.0274, time: 26.8142\n",
      "reconstruction loss = 0.0123, similarity loss: 0.3138\n",
      "Epoch 26, loss = 0.0288, time: 23.6467\n",
      "reconstruction loss = 0.0132, similarity loss: 0.3246\n",
      "Epoch 27, loss = 0.0259, time: 20.6410\n",
      "reconstruction loss = 0.0125, similarity loss: 0.2807\n",
      "Epoch 28, loss = 0.0287, time: 20.3266\n",
      "reconstruction loss = 0.0134, similarity loss: 0.3200\n",
      "Epoch 29, loss = 0.0277, time: 21.7042\n",
      "reconstruction loss = 0.0114, similarity loss: 0.3384\n",
      "Epoch 30, loss = 0.0274, time: 21.7199\n",
      "reconstruction loss = 0.0114, similarity loss: 0.3314\n",
      "Epoch 31, loss = 0.0260, time: 21.8809\n",
      "reconstruction loss = 0.0121, similarity loss: 0.2905\n",
      "Epoch 32, loss = 0.0269, time: 22.0485\n",
      "reconstruction loss = 0.0111, similarity loss: 0.3274\n",
      "Epoch 33, loss = 0.0284, time: 22.1043\n",
      "reconstruction loss = 0.0123, similarity loss: 0.3344\n",
      "Epoch 34, loss = 0.0255, time: 22.3060\n",
      "reconstruction loss = 0.0101, similarity loss: 0.3183\n",
      "Epoch 35, loss = 0.0229, time: 22.4425\n",
      "reconstruction loss = 0.0100, similarity loss: 0.2685\n",
      "Epoch 36, loss = 0.0264, time: 22.9701\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3249\n",
      "Epoch 37, loss = 0.0287, time: 21.7477\n",
      "reconstruction loss = 0.0115, similarity loss: 0.3556\n",
      "Epoch 38, loss = 0.0245, time: 21.4318\n",
      "reconstruction loss = 0.0116, similarity loss: 0.2705\n",
      "Epoch 39, loss = 0.0256, time: 23.1004\n",
      "reconstruction loss = 0.0109, similarity loss: 0.3038\n",
      "Epoch 40, loss = 0.0251, time: 26.4276\n",
      "reconstruction loss = 0.0115, similarity loss: 0.2826\n",
      "Epoch 41, loss = 0.0259, time: 27.7821\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3150\n",
      "Epoch 42, loss = 0.0247, time: 24.3637\n",
      "reconstruction loss = 0.0107, similarity loss: 0.2907\n",
      "Epoch 43, loss = 0.0254, time: 24.7726\n",
      "reconstruction loss = 0.0102, similarity loss: 0.3153\n",
      "Epoch 44, loss = 0.0234, time: 22.4018\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2576\n",
      "Epoch 45, loss = 0.0233, time: 22.1957\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2519\n",
      "Epoch 46, loss = 0.0274, time: 22.8409\n",
      "reconstruction loss = 0.0107, similarity loss: 0.3446\n",
      "Epoch 47, loss = 0.0252, time: 22.7564\n",
      "reconstruction loss = 0.0120, similarity loss: 0.2760\n",
      "Epoch 48, loss = 0.0250, time: 23.0015\n",
      "reconstruction loss = 0.0105, similarity loss: 0.3007\n",
      "Epoch 49, loss = 0.0247, time: 21.3964\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2878\n",
      "Epoch 50, loss = 0.0253, time: 26.7682\n",
      "reconstruction loss = 0.0105, similarity loss: 0.3065\n",
      "Epoch 51, loss = 0.0245, time: 23.9719\n",
      "reconstruction loss = 0.0110, similarity loss: 0.2806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, loss = 0.0219, time: 20.1205\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2530\n",
      "Epoch 53, loss = 0.0241, time: 27.6657\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2755\n",
      "Epoch 54, loss = 0.0230, time: 30.1208\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2814\n",
      "Epoch 55, loss = 0.0235, time: 28.3908\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2908\n",
      "Epoch 56, loss = 0.0213, time: 29.3284\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2598\n",
      "Epoch 57, loss = 0.0222, time: 30.7263\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2657\n",
      "Epoch 58, loss = 0.0242, time: 29.3793\n",
      "reconstruction loss = 0.0100, similarity loss: 0.2938\n",
      "Epoch 59, loss = 0.0266, time: 28.3193\n",
      "reconstruction loss = 0.0126, similarity loss: 0.2928\n",
      "Epoch 60, loss = 0.0218, time: 27.9703\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2747\n",
      "Epoch 61, loss = 0.0238, time: 25.6389\n",
      "reconstruction loss = 0.0102, similarity loss: 0.2834\n",
      "Epoch 62, loss = 0.0250, time: 24.8097\n",
      "reconstruction loss = 0.0096, similarity loss: 0.3183\n",
      "Epoch 63, loss = 0.0217, time: 26.9470\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2748\n",
      "Epoch 64, loss = 0.0200, time: 22.1788\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2376\n",
      "Epoch 65, loss = 0.0225, time: 27.8092\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2818\n",
      "Epoch 66, loss = 0.0223, time: 29.8770\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2795\n",
      "Epoch 67, loss = 0.0225, time: 31.9701\n",
      "reconstruction loss = 0.0096, similarity loss: 0.2676\n",
      "Epoch 68, loss = 0.0231, time: 26.5948\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2733\n",
      "Epoch 69, loss = 0.0247, time: 25.1295\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2968\n",
      "Epoch 70, loss = 0.0224, time: 30.4013\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2906\n",
      "Epoch 71, loss = 0.0209, time: 29.3347\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2572\n",
      "Epoch 72, loss = 0.0236, time: 28.9057\n",
      "reconstruction loss = 0.0096, similarity loss: 0.2893\n",
      "Epoch 73, loss = 0.0217, time: 22.3043\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2811\n",
      "Epoch 74, loss = 0.0186, time: 24.6107\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2356\n",
      "Epoch 75, loss = 0.0227, time: 29.9637\n",
      "reconstruction loss = 0.0096, similarity loss: 0.2713\n",
      "Epoch 76, loss = 0.0229, time: 31.8527\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2880\n",
      "Epoch 77, loss = 0.0207, time: 27.1623\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2693\n",
      "Epoch 78, loss = 0.0212, time: 33.1052\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2728\n",
      "Epoch 79, loss = 0.0213, time: 22.6474\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2628\n",
      "Epoch 80, loss = 0.0239, time: 23.0451\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2960\n",
      "Epoch 81, loss = 0.0218, time: 28.5247\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2726\n",
      "Epoch 82, loss = 0.0212, time: 31.8876\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2721\n",
      "Epoch 83, loss = 0.0217, time: 27.7573\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2657\n",
      "Epoch 84, loss = 0.0216, time: 18.2365\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2785\n",
      "Epoch 85, loss = 0.0205, time: 18.3830\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2570\n",
      "Epoch 86, loss = 0.0220, time: 25.5776\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2839\n",
      "Epoch 87, loss = 0.0216, time: 21.9357\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2700\n",
      "Epoch 88, loss = 0.0231, time: 30.8108\n",
      "reconstruction loss = 0.0080, similarity loss: 0.3086\n",
      "Epoch 89, loss = 0.0237, time: 30.5390\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2951\n",
      "Epoch 90, loss = 0.0212, time: 24.9073\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2732\n",
      "Epoch 91, loss = 0.0218, time: 31.3254\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2658\n",
      "Epoch 92, loss = 0.0220, time: 30.2801\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2701\n",
      "Epoch 93, loss = 0.0225, time: 30.4822\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2941\n",
      "Epoch 94, loss = 0.0226, time: 24.4814\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2978\n",
      "Epoch 95, loss = 0.0204, time: 29.8346\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2611\n",
      "Epoch 96, loss = 0.0219, time: 23.7213\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2723\n",
      "Epoch 97, loss = 0.0207, time: 20.4854\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2489\n",
      "Epoch 98, loss = 0.0185, time: 24.0193\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2474\n",
      "Epoch 99, loss = 0.0241, time: 18.8029\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2940\n",
      "Epoch 100, loss = 0.0202, time: 19.7533\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2635\n",
      "Epoch 101, loss = 0.0248, time: 18.2903\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2891\n",
      "Epoch 102, loss = 0.0224, time: 19.0309\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2955\n",
      "Epoch 103, loss = 0.0212, time: 18.1321\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2704\n",
      "Epoch 104, loss = 0.0198, time: 18.6503\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2421\n",
      "Epoch 105, loss = 0.0186, time: 18.7081\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2245\n",
      "Epoch 106, loss = 0.0214, time: 18.3140\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2675\n",
      "Epoch 107, loss = 0.0211, time: 18.3623\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2815\n",
      "Epoch 108, loss = 0.0208, time: 18.7157\n",
      "reconstruction loss = 0.0092, similarity loss: 0.2402\n",
      "Epoch 109, loss = 0.0210, time: 18.9205\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2745\n",
      "Epoch 110, loss = 0.0224, time: 18.8499\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2986\n",
      "Epoch 111, loss = 0.0206, time: 20.4640\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2521\n",
      "Epoch 112, loss = 0.0208, time: 21.2117\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2785\n",
      "Epoch 113, loss = 0.0203, time: 19.9477\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2695\n",
      "Epoch 114, loss = 0.0214, time: 25.0472\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2687\n",
      "Epoch 115, loss = 0.0191, time: 19.8684\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2429\n",
      "Epoch 116, loss = 0.0179, time: 19.7004\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2076\n",
      "Epoch 117, loss = 0.0212, time: 19.5622\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2765\n",
      "Epoch 118, loss = 0.0202, time: 19.6998\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2528\n",
      "Epoch 119, loss = 0.0213, time: 20.1723\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2858\n",
      "Epoch 120, loss = 0.0222, time: 20.1580\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2750\n",
      "Epoch 121, loss = 0.0192, time: 19.9384\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2262\n",
      "Epoch 122, loss = 0.0217, time: 20.0637\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2713\n",
      "Epoch 123, loss = 0.0205, time: 26.9891\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2618\n",
      "Epoch 124, loss = 0.0225, time: 23.1219\n",
      "reconstruction loss = 0.0075, similarity loss: 0.3074\n",
      "Epoch 125, loss = 0.0222, time: 22.7020\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2726\n",
      "Epoch 126, loss = 0.0181, time: 24.7283\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2361\n",
      "Epoch 127, loss = 0.0198, time: 29.6908\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2621\n",
      "Epoch 128, loss = 0.0196, time: 23.7569\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2578\n",
      "Epoch 129, loss = 0.0201, time: 22.2679\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2635\n",
      "Epoch 130, loss = 0.0201, time: 31.0758\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2698\n",
      "Epoch 131, loss = 0.0202, time: 30.0105\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2705\n",
      "Epoch 132, loss = 0.0222, time: 28.5097\n",
      "reconstruction loss = 0.0070, similarity loss: 0.3108\n",
      "Epoch 133, loss = 0.0191, time: 27.4472\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2500\n",
      "Epoch 134, loss = 0.0231, time: 27.0767\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2938\n",
      "Epoch 135, loss = 0.0198, time: 26.8452\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2501\n",
      "Epoch 136, loss = 0.0209, time: 28.5024\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2789\n",
      "Epoch 137, loss = 0.0199, time: 26.2647\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2724\n",
      "Epoch 138, loss = 0.0197, time: 26.7298\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2592\n",
      "Epoch 139, loss = 0.0203, time: 28.2560\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, loss = 0.0222, time: 31.6562\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2882\n",
      "Epoch 141, loss = 0.0184, time: 27.7980\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2355\n",
      "Epoch 142, loss = 0.0206, time: 30.2612\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2830\n",
      "Epoch 143, loss = 0.0184, time: 24.5558\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2453\n",
      "Epoch 144, loss = 0.0193, time: 30.7128\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2579\n",
      "Epoch 145, loss = 0.0190, time: 31.2786\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2522\n",
      "Epoch 146, loss = 0.0203, time: 27.6539\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2746\n",
      "Epoch 147, loss = 0.0195, time: 31.2839\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2553\n",
      "Epoch 148, loss = 0.0195, time: 29.9488\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2525\n",
      "Epoch 149, loss = 0.0193, time: 24.8118\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2552\n",
      "Epoch 150, loss = 0.0201, time: 21.3542\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2784\n",
      "Epoch 151, loss = 0.0208, time: 20.5312\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2745\n",
      "Epoch 152, loss = 0.0204, time: 26.9765\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2786\n",
      "Epoch 153, loss = 0.0211, time: 31.2345\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2936\n",
      "Epoch 154, loss = 0.0188, time: 33.9882\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2582\n",
      "Epoch 155, loss = 0.0195, time: 32.7052\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2658\n",
      "Epoch 156, loss = 0.0189, time: 33.7456\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2520\n",
      "Epoch 157, loss = 0.0207, time: 30.3695\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2699\n",
      "Epoch 158, loss = 0.0183, time: 31.5189\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2473\n",
      "Epoch 159, loss = 0.0178, time: 27.7627\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2439\n",
      "Epoch 160, loss = 0.0180, time: 26.6434\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2471\n",
      "Epoch 161, loss = 0.0206, time: 27.5981\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2713\n",
      "Epoch 162, loss = 0.0185, time: 28.8295\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2447\n",
      "Epoch 163, loss = 0.0198, time: 21.8650\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2690\n",
      "Epoch 164, loss = 0.0188, time: 23.0896\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2555\n",
      "Epoch 165, loss = 0.0195, time: 26.5134\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2680\n",
      "Epoch 166, loss = 0.0176, time: 24.6133\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2379\n",
      "Epoch 167, loss = 0.0204, time: 24.4309\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2699\n",
      "Epoch 168, loss = 0.0202, time: 24.6878\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2770\n",
      "Epoch 169, loss = 0.0199, time: 24.6956\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2737\n",
      "Epoch 170, loss = 0.0190, time: 24.2681\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2455\n",
      "Epoch 171, loss = 0.0188, time: 20.7487\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2666\n",
      "Epoch 172, loss = 0.0172, time: 18.2917\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2310\n",
      "Epoch 173, loss = 0.0179, time: 24.2264\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2470\n",
      "Epoch 174, loss = 0.0197, time: 24.7331\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2577\n",
      "Epoch 175, loss = 0.0163, time: 25.4243\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2207\n",
      "Epoch 176, loss = 0.0198, time: 40.8791\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2697\n",
      "Epoch 177, loss = 0.0221, time: 32.6659\n",
      "reconstruction loss = 0.0073, similarity loss: 0.3027\n",
      "Epoch 178, loss = 0.0187, time: 35.5176\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2379\n",
      "Epoch 179, loss = 0.0181, time: 32.1209\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2543\n",
      "Epoch 180, loss = 0.0184, time: 32.2255\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2426\n",
      "Epoch 181, loss = 0.0201, time: 28.4001\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2623\n",
      "Epoch 182, loss = 0.0183, time: 33.7092\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2441\n",
      "Epoch 183, loss = 0.0193, time: 33.4942\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2718\n",
      "Epoch 184, loss = 0.0172, time: 27.8093\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2353\n",
      "Epoch 185, loss = 0.0197, time: 26.3082\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2780\n",
      "Epoch 186, loss = 0.0174, time: 24.8687\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2364\n",
      "Epoch 187, loss = 0.0176, time: 28.2953\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2481\n",
      "Epoch 188, loss = 0.0203, time: 24.2265\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2683\n",
      "Epoch 189, loss = 0.0173, time: 28.9675\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2226\n",
      "Epoch 190, loss = 0.0191, time: 27.6716\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2584\n",
      "Epoch 191, loss = 0.0189, time: 28.8946\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2563\n",
      "Epoch 192, loss = 0.0207, time: 30.1599\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2900\n",
      "Epoch 193, loss = 0.0168, time: 28.5759\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2262\n",
      "Epoch 194, loss = 0.0188, time: 28.9177\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2618\n",
      "Epoch 195, loss = 0.0178, time: 27.5086\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2416\n",
      "Epoch 196, loss = 0.0192, time: 26.7863\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2776\n",
      "Epoch 197, loss = 0.0181, time: 26.0302\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2473\n",
      "Epoch 198, loss = 0.0186, time: 35.9643\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2415\n",
      "Epoch 199, loss = 0.0180, time: 24.1491\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2470\n",
      "Epoch 200, loss = 0.0173, time: 26.7296\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2357\n",
      "Epoch 201, loss = 0.0187, time: 28.5493\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2626\n",
      "Epoch 202, loss = 0.0181, time: 27.9687\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2574\n",
      "Epoch 203, loss = 0.0194, time: 23.4651\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2818\n",
      "Epoch 204, loss = 0.0189, time: 28.4355\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2498\n",
      "Epoch 205, loss = 0.0183, time: 27.6501\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2620\n",
      "Epoch 206, loss = 0.0203, time: 20.7564\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2790\n",
      "Epoch 207, loss = 0.0168, time: 20.4643\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2145\n",
      "Epoch 208, loss = 0.0182, time: 22.7861\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2420\n",
      "Epoch 209, loss = 0.0178, time: 24.2830\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2507\n",
      "Epoch 210, loss = 0.0209, time: 23.7300\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2807\n",
      "Epoch 211, loss = 0.0166, time: 21.8367\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2258\n",
      "Epoch 212, loss = 0.0186, time: 25.5702\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2517\n",
      "Epoch 213, loss = 0.0181, time: 26.3748\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2506\n",
      "Epoch 214, loss = 0.0199, time: 26.6171\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2733\n",
      "Epoch 215, loss = 0.0157, time: 25.9448\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2163\n",
      "Epoch 216, loss = 0.0182, time: 22.1377\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2550\n",
      "Epoch 217, loss = 0.0195, time: 21.3850\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2682\n",
      "Epoch 218, loss = 0.0192, time: 25.8681\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2715\n",
      "Epoch 219, loss = 0.0178, time: 25.9184\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2571\n",
      "Epoch 220, loss = 0.0181, time: 25.6695\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2430\n",
      "Epoch 221, loss = 0.0197, time: 20.9249\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2662\n",
      "Epoch 222, loss = 0.0166, time: 25.9086\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2152\n",
      "Epoch 223, loss = 0.0189, time: 25.8669\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2690\n",
      "Epoch 224, loss = 0.0182, time: 26.1711\n",
      "reconstruction loss = 0.0054, similarity loss: 0.2614\n",
      "Epoch 225, loss = 0.0169, time: 26.2820\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2313\n",
      "Epoch 226, loss = 0.0192, time: 25.4204\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2612\n",
      "Epoch 227, loss = 0.0184, time: 23.4774\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228, loss = 0.0173, time: 25.3660\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2378\n",
      "Epoch 229, loss = 0.0167, time: 25.1947\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2250\n",
      "Epoch 230, loss = 0.0198, time: 23.5214\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2820\n",
      "Epoch 231, loss = 0.0195, time: 19.9417\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2722\n",
      "Epoch 232, loss = 0.0193, time: 19.8857\n",
      "reconstruction loss = 0.0075, similarity loss: 0.2435\n",
      "Epoch 233, loss = 0.0177, time: 24.0190\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2458\n",
      "Epoch 234, loss = 0.0184, time: 24.4978\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2457\n",
      "Epoch 235, loss = 0.0199, time: 20.9471\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2808\n",
      "Epoch 236, loss = 0.0197, time: 20.7490\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2692\n",
      "Epoch 237, loss = 0.0192, time: 20.8075\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2526\n",
      "Epoch 238, loss = 0.0182, time: 20.8566\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2489\n",
      "Epoch 239, loss = 0.0158, time: 21.0922\n",
      "reconstruction loss = 0.0049, similarity loss: 0.2220\n",
      "Epoch 240, loss = 0.0186, time: 23.1886\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2537\n",
      "Epoch 241, loss = 0.0173, time: 22.4770\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2416\n",
      "Epoch 242, loss = 0.0195, time: 20.2097\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2687\n",
      "Epoch 243, loss = 0.0185, time: 19.8879\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2653\n",
      "Epoch 244, loss = 0.0185, time: 19.7074\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2568\n",
      "Epoch 245, loss = 0.0188, time: 21.2194\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2501\n",
      "Epoch 246, loss = 0.0178, time: 20.5935\n",
      "reconstruction loss = 0.0053, similarity loss: 0.2566\n",
      "Epoch 247, loss = 0.0194, time: 30.9588\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2755\n",
      "Epoch 248, loss = 0.0206, time: 32.6966\n",
      "reconstruction loss = 0.0075, similarity loss: 0.2695\n",
      "Epoch 249, loss = 0.0213, time: 29.8382\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2644\n",
      "Epoch 250, loss = 0.0178, time: 27.4824\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2497\n",
      "Epoch 251, loss = 0.0202, time: 20.6552\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2593\n",
      "Epoch 252, loss = 0.0178, time: 22.1299\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2570\n",
      "Epoch 253, loss = 0.0171, time: 23.7691\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2332\n",
      "Epoch 254, loss = 0.0173, time: 23.8824\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2350\n",
      "Epoch 255, loss = 0.0193, time: 20.1049\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2742\n",
      "Epoch 256, loss = 0.0167, time: 27.0967\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2296\n",
      "Epoch 257, loss = 0.0209, time: 20.8801\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2928\n",
      "Epoch 258, loss = 0.0176, time: 20.7907\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2446\n",
      "Epoch 259, loss = 0.0178, time: 20.8580\n",
      "reconstruction loss = 0.0053, similarity loss: 0.2536\n",
      "Epoch 260, loss = 0.0183, time: 21.0537\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2433\n",
      "Epoch 261, loss = 0.0169, time: 20.8311\n",
      "reconstruction loss = 0.0053, similarity loss: 0.2379\n",
      "Epoch 262, loss = 0.0174, time: 20.9462\n",
      "reconstruction loss = 0.0051, similarity loss: 0.2494\n",
      "Epoch 263, loss = 0.0161, time: 21.0365\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2134\n",
      "Epoch 264, loss = 0.0188, time: 20.9478\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2272\n",
      "Epoch 265, loss = 0.0205, time: 25.2186\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2764\n",
      "Epoch 266, loss = 0.0172, time: 34.1555\n",
      "reconstruction loss = 0.0050, similarity loss: 0.2481\n",
      "Epoch 267, loss = 0.0171, time: 23.0142\n",
      "reconstruction loss = 0.0054, similarity loss: 0.2382\n",
      "Epoch 268, loss = 0.0168, time: 22.6279\n",
      "reconstruction loss = 0.0051, similarity loss: 0.2403\n",
      "Epoch 269, loss = 0.0182, time: 21.1606\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2407\n",
      "Epoch 270, loss = 0.0175, time: 20.4502\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2269\n",
      "Epoch 271, loss = 0.0157, time: 23.8934\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2100\n",
      "Epoch 272, loss = 0.0163, time: 20.9027\n",
      "reconstruction loss = 0.0053, similarity loss: 0.2248\n",
      "Epoch 273, loss = 0.0179, time: 20.9057\n",
      "reconstruction loss = 0.0051, similarity loss: 0.2603\n",
      "Epoch 274, loss = 0.0162, time: 20.8502\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2248\n",
      "Epoch 275, loss = 0.0172, time: 17.4020\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2330\n",
      "Epoch 276, loss = 0.0167, time: 17.1515\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2277\n",
      "Epoch 277, loss = 0.0184, time: 17.0630\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2557\n",
      "Epoch 278, loss = 0.0193, time: 16.3667\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2574\n",
      "Epoch 279, loss = 0.0179, time: 17.1214\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2359\n",
      "Epoch 280, loss = 0.0164, time: 16.5794\n",
      "reconstruction loss = 0.0050, similarity loss: 0.2334\n",
      "Epoch 281, loss = 0.0188, time: 16.5577\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2449\n",
      "Epoch 282, loss = 0.0205, time: 16.6713\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2774\n",
      "Epoch 283, loss = 0.0172, time: 16.3395\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2269\n",
      "Epoch 284, loss = 0.0155, time: 16.5706\n",
      "reconstruction loss = 0.0046, similarity loss: 0.2219\n",
      "Epoch 285, loss = 0.0170, time: 16.3767\n",
      "reconstruction loss = 0.0054, similarity loss: 0.2364\n",
      "Epoch 286, loss = 0.0179, time: 16.4643\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2454\n",
      "Epoch 287, loss = 0.0166, time: 16.5623\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2316\n",
      "Epoch 288, loss = 0.0194, time: 18.2469\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2556\n",
      "Epoch 289, loss = 0.0192, time: 17.7576\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2672\n",
      "Epoch 290, loss = 0.0182, time: 16.7709\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2570\n",
      "Epoch 291, loss = 0.0190, time: 16.5912\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2596\n",
      "Epoch 292, loss = 0.0173, time: 16.2023\n",
      "reconstruction loss = 0.0051, similarity loss: 0.2495\n",
      "Epoch 293, loss = 0.0164, time: 16.3448\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2225\n",
      "Epoch 294, loss = 0.0171, time: 16.2481\n",
      "reconstruction loss = 0.0048, similarity loss: 0.2508\n",
      "Epoch 295, loss = 0.0173, time: 16.5205\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2383\n",
      "Epoch 296, loss = 0.0168, time: 16.8941\n",
      "reconstruction loss = 0.0050, similarity loss: 0.2413\n",
      "Epoch 297, loss = 0.0162, time: 19.0514\n",
      "reconstruction loss = 0.0054, similarity loss: 0.2216\n",
      "Epoch 298, loss = 0.0171, time: 17.7008\n",
      "reconstruction loss = 0.0052, similarity loss: 0.2431\n",
      "Epoch 299, loss = 0.0178, time: 17.2242\n",
      "reconstruction loss = 0.0057, similarity loss: 0.2473\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 1.5091, val.acc = 0.5900\n",
      "Epoch 1, loss = 1.1561, val.acc = 0.6230\n",
      "Epoch 2, loss = 1.0570, val.acc = 0.6394\n",
      "Epoch 3, loss = 0.9917, val.acc = 0.6480\n",
      "Epoch 4, loss = 0.9422, val.acc = 0.6540\n",
      "Epoch 5, loss = 0.9016, val.acc = 0.6598\n",
      "Epoch 6, loss = 0.8669, val.acc = 0.6614\n",
      "Epoch 7, loss = 0.8366, val.acc = 0.6622\n",
      "Epoch 8, loss = 0.8097, val.acc = 0.6670\n",
      "Epoch 9, loss = 0.7856, val.acc = 0.6662\n",
      "Epoch 10, loss = 0.7636, val.acc = 0.6656\n",
      "Epoch 11, loss = 0.7435, val.acc = 0.6640\n",
      "Epoch 12, loss = 0.7249, val.acc = 0.6624\n",
      "Epoch 13, loss = 0.7077, val.acc = 0.6632\n",
      "Epoch 14, loss = 0.6916, val.acc = 0.6626\n",
      "Epoch 15, loss = 0.6766, val.acc = 0.6620\n",
      "Epoch 16, loss = 0.6624, val.acc = 0.6614\n",
      "Epoch 17, loss = 0.6489, val.acc = 0.6604\n",
      "Epoch 18, loss = 0.6362, val.acc = 0.6602\n",
      "Epoch 19, loss = 0.6241, val.acc = 0.6602\n",
      "Epoch 20, loss = 0.6125, val.acc = 0.6602\n",
      "Epoch 21, loss = 0.6015, val.acc = 0.6582\n",
      "Epoch 22, loss = 0.5909, val.acc = 0.6590\n",
      "Epoch 23, loss = 0.5808, val.acc = 0.6594\n",
      "Epoch 24, loss = 0.5710, val.acc = 0.6592\n",
      "Epoch 25, loss = 0.5617, val.acc = 0.6588\n",
      "Epoch 26, loss = 0.5528, val.acc = 0.6606\n",
      "Epoch 27, loss = 0.5441, val.acc = 0.6600\n",
      "Epoch 28, loss = 0.5359, val.acc = 0.6596\n",
      "Epoch 29, loss = 0.5280, val.acc = 0.6586\n",
      "Epoch 30, loss = 0.5204, val.acc = 0.6582\n",
      "Epoch 31, loss = 0.5132, val.acc = 0.6590\n",
      "Epoch 32, loss = 0.5063, val.acc = 0.6580\n",
      "Epoch 33, loss = 0.4998, val.acc = 0.6586\n",
      "Epoch 34, loss = 0.4937, val.acc = 0.6568\n",
      "Epoch 35, loss = 0.4881, val.acc = 0.6554\n",
      "Epoch 36, loss = 0.4833, val.acc = 0.6562\n",
      "Epoch 37, loss = 0.4796, val.acc = 0.6546\n",
      "Epoch 38, loss = 0.4766, val.acc = 0.6564\n",
      "Epoch 39, loss = 0.4724, val.acc = 0.6528\n",
      "Epoch 40, loss = 0.4683, val.acc = 0.6516\n",
      "Epoch 41, loss = 0.4660, val.acc = 0.6484\n",
      "Epoch 42, loss = 0.4651, val.acc = 0.6448\n",
      "Epoch 43, loss = 0.4643, val.acc = 0.6410\n",
      "Epoch 44, loss = 0.4627, val.acc = 0.6444\n",
      "Epoch 45, loss = 0.4584, val.acc = 0.6426\n",
      "Epoch 46, loss = 0.4524, val.acc = 0.6426\n",
      "Epoch 47, loss = 0.4471, val.acc = 0.6416\n",
      "Epoch 48, loss = 0.4440, val.acc = 0.6428\n",
      "Epoch 49, loss = 0.4430, val.acc = 0.6412\n",
      "Epoch 50, loss = 0.4438, val.acc = 0.6412\n",
      "Epoch 51, loss = 0.4445, val.acc = 0.6370\n",
      "Epoch 52, loss = 0.4424, val.acc = 0.6356\n",
      "Epoch 53, loss = 0.4399, val.acc = 0.6282\n",
      "Epoch 54, loss = 0.4422, val.acc = 0.6210\n",
      "Epoch 55, loss = 0.4532, val.acc = 0.6232\n",
      "Epoch 56, loss = 0.4764, val.acc = 0.6352\n",
      "Epoch 57, loss = 0.4536, val.acc = 0.6364\n",
      "Epoch 58, loss = 0.4363, val.acc = 0.6326\n",
      "Epoch 59, loss = 0.4259, val.acc = 0.6356\n",
      "Epoch 60, loss = 0.4155, val.acc = 0.6396\n",
      "Epoch 61, loss = 0.4053, val.acc = 0.6398\n",
      "Epoch 62, loss = 0.3965, val.acc = 0.6390\n",
      "Epoch 63, loss = 0.3889, val.acc = 0.6388\n",
      "Epoch 64, loss = 0.3822, val.acc = 0.6406\n",
      "Epoch 65, loss = 0.3762, val.acc = 0.6408\n",
      "Epoch 66, loss = 0.3708, val.acc = 0.6402\n",
      "Epoch 67, loss = 0.3658, val.acc = 0.6384\n",
      "Epoch 68, loss = 0.3611, val.acc = 0.6402\n",
      "Epoch 69, loss = 0.3567, val.acc = 0.6400\n",
      "Epoch 70, loss = 0.3525, val.acc = 0.6400\n",
      "Epoch 71, loss = 0.3486, val.acc = 0.6388\n",
      "Epoch 72, loss = 0.3448, val.acc = 0.6370\n",
      "Epoch 73, loss = 0.3411, val.acc = 0.6370\n",
      "Epoch 74, loss = 0.3377, val.acc = 0.6368\n",
      "Epoch 75, loss = 0.3343, val.acc = 0.6358\n",
      "Epoch 76, loss = 0.3311, val.acc = 0.6362\n",
      "Epoch 77, loss = 0.3279, val.acc = 0.6346\n",
      "Epoch 78, loss = 0.3248, val.acc = 0.6330\n",
      "Epoch 79, loss = 0.3219, val.acc = 0.6334\n",
      "Epoch 80, loss = 0.3190, val.acc = 0.6336\n",
      "Epoch 81, loss = 0.3161, val.acc = 0.6330\n",
      "Epoch 82, loss = 0.3134, val.acc = 0.6332\n",
      "Epoch 83, loss = 0.3106, val.acc = 0.6330\n",
      "Epoch 84, loss = 0.3080, val.acc = 0.6328\n",
      "Epoch 85, loss = 0.3053, val.acc = 0.6332\n",
      "Epoch 86, loss = 0.3028, val.acc = 0.6346\n",
      "Epoch 87, loss = 0.3002, val.acc = 0.6340\n",
      "Epoch 88, loss = 0.2977, val.acc = 0.6336\n",
      "Epoch 89, loss = 0.2953, val.acc = 0.6338\n",
      "Epoch 90, loss = 0.2929, val.acc = 0.6338\n",
      "Epoch 91, loss = 0.2905, val.acc = 0.6350\n",
      "Epoch 92, loss = 0.2881, val.acc = 0.6342\n",
      "Epoch 93, loss = 0.2858, val.acc = 0.6340\n",
      "Epoch 94, loss = 0.2835, val.acc = 0.6334\n",
      "Epoch 95, loss = 0.2813, val.acc = 0.6334\n",
      "Epoch 96, loss = 0.2791, val.acc = 0.6326\n",
      "Epoch 97, loss = 0.2770, val.acc = 0.6322\n",
      "Epoch 98, loss = 0.2748, val.acc = 0.6312\n",
      "Epoch 99, loss = 0.2728, val.acc = 0.6306\n",
      "Rep: 1, te.acc = 0.6146\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6146]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_500_sklearn')\n",
    "sklearn_classifier = SGDClassifier(verbose=0, n_jobs=-1)\n",
    "train_unsupervised_ae(pars,vis=vis, sklearn_classifier=sklearn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average reconstruction loss\n",
    "- reconstruction result\n",
    "- zero weight on BT\n",
    "- lambda from small to large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take the square root\n",
    "- change lambda\n",
    "- change the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.loss = \"BarlowTwins\"\n",
    "pars.lam = 0.5\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BarlowTwinsLoss(pars.batch_size, pars.lam, pars.device)\n",
    "train_unsupervised(pars, criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd034ed54560f0698c2946b7ca675e493afbd7ee3c0ecf162ae3deac3cf4477b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
