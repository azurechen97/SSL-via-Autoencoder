{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "datadirs = ''\n",
    "sys.path.insert(1, datadirs)\n",
    "savepath = datadirs+'save/'\n",
    "datapath = datadirs+'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pars import PARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "from setup_net import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: BarlowTwins\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 3\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "BTlambda: 0.5\n",
      "auxnonlinear: None\n",
      "unsupervised: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.loss = \"BarlowTwins\"\n",
    "pars.lam = 0.5\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: BarlowTwins\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 3\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "BTlambda: 0.5\n",
      "auxnonlinear: None\n",
      "unsupervised: True\n",
      "\n",
      "save/CONV6/BarlowTwins/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lambda_0.5\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead4): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BarlowTwinsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 89.0901, time: 24.4807\n",
      "Epoch 1, loss = 59.9343, time: 16.2096\n",
      "Epoch 2, loss = 58.8422, time: 16.5327\n",
      "Epoch 3, loss = 57.5996, time: 16.5677\n",
      "Epoch 4, loss = 53.3677, time: 21.2097\n",
      "Epoch 5, loss = 52.8654, time: 19.0128\n",
      "Epoch 6, loss = 52.8058, time: 16.0056\n",
      "Epoch 7, loss = 51.0934, time: 16.5587\n",
      "Epoch 8, loss = 51.0642, time: 16.9047\n",
      "Epoch 9, loss = 53.5063, time: 16.2736\n",
      "Epoch 10, loss = 52.3528, time: 16.0686\n",
      "Epoch 11, loss = 51.2936, time: 16.0416\n",
      "Epoch 12, loss = 52.1284, time: 16.1906\n",
      "Epoch 13, loss = 50.2724, time: 17.4379\n",
      "Epoch 14, loss = 49.4269, time: 16.0186\n",
      "Epoch 15, loss = 49.4364, time: 16.1976\n",
      "Epoch 16, loss = 49.8539, time: 15.9306\n",
      "Epoch 17, loss = 45.7378, time: 15.7385\n",
      "Epoch 18, loss = 47.4326, time: 15.6615\n",
      "Epoch 19, loss = 48.3680, time: 16.0416\n",
      "Epoch 20, loss = 47.5249, time: 15.8235\n",
      "Epoch 21, loss = 46.8415, time: 15.7125\n",
      "Epoch 22, loss = 45.0612, time: 16.0616\n",
      "Epoch 23, loss = 47.5349, time: 15.6925\n",
      "Epoch 24, loss = 47.0085, time: 15.6485\n",
      "Epoch 25, loss = 43.5710, time: 15.8225\n",
      "Epoch 26, loss = 45.3337, time: 16.5147\n",
      "Epoch 27, loss = 43.8228, time: 17.6419\n",
      "Epoch 28, loss = 44.3009, time: 16.8638\n",
      "Epoch 29, loss = 45.3196, time: 16.4307\n",
      "Epoch 30, loss = 42.7299, time: 16.8768\n",
      "Epoch 31, loss = 42.7332, time: 17.3389\n",
      "Epoch 32, loss = 45.2224, time: 16.4147\n",
      "Epoch 33, loss = 44.6772, time: 16.6407\n",
      "Epoch 34, loss = 41.9051, time: 16.9668\n",
      "Epoch 35, loss = 41.3324, time: 16.9878\n",
      "Epoch 36, loss = 44.0670, time: 17.4089\n",
      "Epoch 37, loss = 43.6503, time: 15.7455\n",
      "Epoch 38, loss = 43.2293, time: 15.9206\n",
      "Epoch 39, loss = 40.3304, time: 15.8595\n",
      "Epoch 40, loss = 42.1439, time: 15.8655\n",
      "Epoch 41, loss = 39.7131, time: 15.8085\n",
      "Epoch 42, loss = 40.0624, time: 15.7785\n",
      "Epoch 43, loss = 41.6042, time: 15.7875\n",
      "Epoch 44, loss = 42.6738, time: 15.8005\n",
      "Epoch 45, loss = 41.8202, time: 15.8625\n",
      "Epoch 46, loss = 39.6132, time: 16.0526\n",
      "Epoch 47, loss = 39.4926, time: 16.0396\n",
      "Epoch 48, loss = 40.4637, time: 15.5795\n",
      "Epoch 49, loss = 40.2253, time: 15.6915\n",
      "Epoch 50, loss = 40.0659, time: 16.6777\n",
      "Epoch 51, loss = 40.3506, time: 17.5479\n",
      "Epoch 52, loss = 40.0202, time: 16.4107\n",
      "Epoch 53, loss = 41.0462, time: 15.9361\n",
      "Epoch 54, loss = 35.7337, time: 15.8775\n",
      "Epoch 55, loss = 37.6525, time: 15.7415\n",
      "Epoch 56, loss = 41.6484, time: 15.6705\n",
      "Epoch 57, loss = 38.1283, time: 15.7985\n",
      "Epoch 58, loss = 37.7508, time: 16.0126\n",
      "Epoch 59, loss = 40.4334, time: 15.6845\n",
      "Epoch 60, loss = 38.1648, time: 16.1466\n",
      "Epoch 61, loss = 39.0931, time: 15.8625\n",
      "Epoch 62, loss = 39.6849, time: 15.8595\n",
      "Epoch 63, loss = 38.4197, time: 15.9896\n",
      "Epoch 64, loss = 38.4090, time: 16.0996\n",
      "Epoch 65, loss = 38.1509, time: 17.1368\n",
      "Epoch 66, loss = 39.0201, time: 16.6387\n",
      "Epoch 67, loss = 35.8360, time: 16.8078\n",
      "Epoch 68, loss = 37.8608, time: 16.3416\n",
      "Epoch 69, loss = 38.5162, time: 18.0750\n",
      "Epoch 70, loss = 36.6316, time: 15.8645\n",
      "Epoch 71, loss = 37.5951, time: 16.7907\n",
      "Epoch 72, loss = 35.1992, time: 16.7897\n",
      "Epoch 73, loss = 38.7682, time: 16.4287\n",
      "Epoch 74, loss = 34.5237, time: 17.4849\n",
      "Epoch 75, loss = 39.7904, time: 16.1616\n",
      "Epoch 76, loss = 37.5505, time: 16.0836\n",
      "Epoch 77, loss = 34.8374, time: 16.0336\n",
      "Epoch 78, loss = 39.7451, time: 15.7625\n",
      "Epoch 79, loss = 36.6056, time: 16.0316\n",
      "Epoch 80, loss = 35.6746, time: 16.5787\n",
      "Epoch 81, loss = 38.8885, time: 16.0086\n",
      "Epoch 82, loss = 36.0457, time: 16.1676\n",
      "Epoch 83, loss = 36.1098, time: 16.1756\n",
      "Epoch 84, loss = 35.2123, time: 15.9946\n",
      "Epoch 85, loss = 34.4913, time: 16.3466\n",
      "Epoch 86, loss = 37.3131, time: 15.9896\n",
      "Epoch 87, loss = 33.0998, time: 15.9426\n",
      "Epoch 88, loss = 35.4722, time: 15.9146\n",
      "Epoch 89, loss = 37.2662, time: 16.8598\n",
      "Epoch 90, loss = 33.8919, time: 15.8795\n",
      "Epoch 91, loss = 35.4127, time: 15.7565\n",
      "Epoch 92, loss = 34.8284, time: 15.8675\n",
      "Epoch 93, loss = 32.5333, time: 15.6685\n",
      "Epoch 94, loss = 35.4991, time: 15.6195\n",
      "Epoch 95, loss = 34.1600, time: 16.6367\n",
      "Epoch 96, loss = 33.7699, time: 15.6765\n",
      "Epoch 97, loss = 34.0856, time: 15.7965\n",
      "Epoch 98, loss = 33.9188, time: 16.0206\n",
      "Epoch 99, loss = 32.1429, time: 16.1256\n",
      "Epoch 100, loss = 33.2090, time: 15.9616\n",
      "Epoch 101, loss = 34.6937, time: 15.6375\n",
      "Epoch 102, loss = 33.4933, time: 15.7375\n",
      "Epoch 103, loss = 32.6766, time: 15.7305\n",
      "Epoch 104, loss = 34.7614, time: 15.8455\n",
      "Epoch 105, loss = 35.9560, time: 16.3657\n",
      "Epoch 106, loss = 33.0569, time: 16.5447\n",
      "Epoch 107, loss = 32.1872, time: 16.2936\n",
      "Epoch 108, loss = 35.8856, time: 16.3236\n",
      "Epoch 109, loss = 32.9614, time: 15.8535\n",
      "Epoch 110, loss = 34.9500, time: 16.0086\n",
      "Epoch 111, loss = 33.6619, time: 15.4965\n",
      "Epoch 112, loss = 34.5401, time: 15.6475\n",
      "Epoch 113, loss = 35.0632, time: 15.6955\n",
      "Epoch 114, loss = 33.3046, time: 15.7815\n",
      "Epoch 115, loss = 34.5414, time: 15.8455\n",
      "Epoch 116, loss = 34.8414, time: 15.6695\n",
      "Epoch 117, loss = 30.7481, time: 15.8585\n",
      "Epoch 118, loss = 32.5020, time: 15.8335\n",
      "Epoch 119, loss = 32.8833, time: 16.0186\n",
      "Epoch 120, loss = 32.1610, time: 15.7855\n",
      "Epoch 121, loss = 33.6825, time: 15.6855\n",
      "Epoch 122, loss = 32.4203, time: 15.7975\n",
      "Epoch 123, loss = 32.6226, time: 15.7585\n",
      "Epoch 124, loss = 35.3090, time: 15.6935\n",
      "Epoch 125, loss = 32.9362, time: 15.8735\n",
      "Epoch 126, loss = 36.4391, time: 15.7935\n",
      "Epoch 127, loss = 34.2583, time: 15.6485\n",
      "Epoch 128, loss = 32.2959, time: 16.6787\n",
      "Epoch 129, loss = 32.3973, time: 16.8968\n",
      "Epoch 130, loss = 33.5538, time: 16.0866\n",
      "Epoch 131, loss = 32.8431, time: 15.8855\n",
      "Epoch 132, loss = 32.5826, time: 15.9296\n",
      "Epoch 133, loss = 30.6653, time: 15.7485\n",
      "Epoch 134, loss = 32.4414, time: 15.9936\n",
      "Epoch 135, loss = 31.5008, time: 16.0856\n",
      "Epoch 136, loss = 33.3039, time: 15.9886\n",
      "Epoch 137, loss = 27.8594, time: 15.7775\n",
      "Epoch 138, loss = 31.6022, time: 15.7655\n",
      "Epoch 139, loss = 33.7055, time: 15.7995\n",
      "Epoch 140, loss = 28.8161, time: 15.8875\n",
      "Epoch 141, loss = 30.7226, time: 15.8095\n",
      "Epoch 142, loss = 31.7082, time: 15.8575\n",
      "Epoch 143, loss = 31.2406, time: 15.7935\n",
      "Epoch 144, loss = 33.5814, time: 15.7485\n",
      "Epoch 145, loss = 34.5738, time: 15.9115\n",
      "Epoch 146, loss = 30.6528, time: 16.2286\n",
      "Epoch 147, loss = 32.8047, time: 15.9446\n",
      "Epoch 148, loss = 31.2798, time: 15.8165\n",
      "Epoch 149, loss = 32.4433, time: 15.8515\n",
      "Epoch 150, loss = 29.7632, time: 15.7725\n",
      "Epoch 151, loss = 32.1838, time: 15.8315\n",
      "Epoch 152, loss = 29.2778, time: 15.8055\n",
      "Epoch 153, loss = 32.2664, time: 15.7875\n",
      "Epoch 154, loss = 33.9154, time: 15.8595\n",
      "Epoch 155, loss = 33.7512, time: 15.8535\n",
      "Epoch 156, loss = 32.1973, time: 15.9336\n",
      "Epoch 157, loss = 30.7866, time: 15.8245\n",
      "Epoch 158, loss = 32.2908, time: 15.8105\n",
      "Epoch 159, loss = 32.1574, time: 15.7255\n",
      "Epoch 160, loss = 32.4443, time: 15.8105\n",
      "Epoch 161, loss = 30.0951, time: 15.8505\n",
      "Epoch 162, loss = 30.8996, time: 15.7915\n",
      "Epoch 163, loss = 31.8085, time: 16.0766\n",
      "Epoch 164, loss = 32.3888, time: 16.5707\n",
      "Epoch 165, loss = 31.3701, time: 16.4497\n",
      "Epoch 166, loss = 29.6376, time: 16.7477\n",
      "Epoch 167, loss = 33.4757, time: 16.1076\n",
      "Epoch 168, loss = 31.1806, time: 15.9346\n",
      "Epoch 169, loss = 32.6326, time: 15.7415\n",
      "Epoch 170, loss = 30.8250, time: 15.8285\n",
      "Epoch 171, loss = 33.0398, time: 15.8985\n",
      "Epoch 172, loss = 32.5704, time: 15.7395\n",
      "Epoch 173, loss = 29.5063, time: 16.3286\n",
      "Epoch 174, loss = 30.6236, time: 16.1066\n",
      "Epoch 175, loss = 31.5953, time: 15.7585\n",
      "Epoch 176, loss = 32.1888, time: 16.1426\n",
      "Epoch 177, loss = 31.0533, time: 16.0206\n",
      "Epoch 178, loss = 31.2403, time: 15.9866\n",
      "Epoch 179, loss = 29.5670, time: 16.2736\n",
      "Epoch 180, loss = 30.4384, time: 16.1606\n",
      "Epoch 181, loss = 29.5616, time: 16.2576\n",
      "Epoch 182, loss = 32.1035, time: 16.2286\n",
      "Epoch 183, loss = 31.5430, time: 17.5079\n",
      "Epoch 184, loss = 31.3090, time: 17.0328\n",
      "Epoch 185, loss = 30.2676, time: 16.3186\n",
      "Epoch 186, loss = 31.2578, time: 16.3036\n",
      "Epoch 187, loss = 33.8178, time: 16.0896\n",
      "Epoch 188, loss = 28.5344, time: 16.2016\n",
      "Epoch 189, loss = 28.9487, time: 16.7997\n",
      "Epoch 190, loss = 29.5368, time: 16.2926\n",
      "Epoch 191, loss = 31.5000, time: 16.9318\n",
      "Epoch 192, loss = 32.8290, time: 16.4627\n",
      "Epoch 193, loss = 30.7915, time: 16.2216\n",
      "Epoch 194, loss = 30.6418, time: 16.3807\n",
      "Epoch 195, loss = 30.3253, time: 16.3316\n",
      "Epoch 196, loss = 31.2129, time: 16.4017\n",
      "Epoch 197, loss = 32.0937, time: 16.3967\n",
      "Epoch 198, loss = 31.5132, time: 16.2026\n",
      "Epoch 199, loss = 31.3741, time: 16.0626\n",
      "Epoch 200, loss = 27.0106, time: 17.0908\n",
      "Epoch 201, loss = 28.3911, time: 16.4827\n",
      "Epoch 202, loss = 29.3803, time: 15.6555\n",
      "Epoch 203, loss = 29.4272, time: 15.8385\n",
      "Epoch 204, loss = 30.7196, time: 15.9456\n",
      "Epoch 205, loss = 31.2756, time: 15.8175\n",
      "Epoch 206, loss = 32.0408, time: 15.6355\n",
      "Epoch 207, loss = 32.6737, time: 15.5995\n",
      "Epoch 208, loss = 28.5453, time: 16.3186\n",
      "Epoch 209, loss = 30.2829, time: 15.8635\n",
      "Epoch 210, loss = 28.7964, time: 16.5127\n",
      "Epoch 211, loss = 31.2550, time: 16.3356\n",
      "Epoch 212, loss = 32.3243, time: 16.2506\n",
      "Epoch 213, loss = 32.6879, time: 16.0086\n",
      "Epoch 214, loss = 29.7239, time: 16.8348\n",
      "Epoch 215, loss = 29.0752, time: 16.4827\n",
      "Epoch 216, loss = 30.6614, time: 16.6447\n",
      "Epoch 217, loss = 31.0654, time: 16.2946\n",
      "Epoch 218, loss = 30.3462, time: 16.0976\n",
      "Epoch 219, loss = 29.1316, time: 16.7187\n",
      "Epoch 220, loss = 28.1657, time: 16.4487\n",
      "Epoch 221, loss = 28.8920, time: 16.5327\n",
      "Epoch 222, loss = 31.5417, time: 16.1056\n",
      "Epoch 223, loss = 28.5424, time: 16.1316\n",
      "Epoch 224, loss = 29.8327, time: 16.2206\n",
      "Epoch 225, loss = 29.6969, time: 16.5937\n",
      "Epoch 226, loss = 29.7101, time: 16.3406\n",
      "Epoch 227, loss = 29.3452, time: 16.5917\n",
      "Epoch 228, loss = 29.0658, time: 16.3667\n",
      "Epoch 229, loss = 28.5986, time: 16.0706\n",
      "Epoch 230, loss = 31.4705, time: 15.7835\n",
      "Epoch 231, loss = 29.1270, time: 15.7695\n",
      "Epoch 232, loss = 27.9568, time: 15.7635\n",
      "Epoch 233, loss = 29.6121, time: 16.9738\n",
      "Epoch 234, loss = 29.1912, time: 16.2606\n",
      "Epoch 235, loss = 28.2998, time: 15.9776\n",
      "Epoch 236, loss = 28.0775, time: 15.9386\n",
      "Epoch 237, loss = 30.5355, time: 16.0996\n",
      "Epoch 238, loss = 29.3976, time: 16.0276\n",
      "Epoch 239, loss = 29.0232, time: 16.2556\n",
      "Epoch 240, loss = 30.2952, time: 16.5287\n",
      "Epoch 241, loss = 31.9825, time: 16.6617\n",
      "Epoch 242, loss = 30.2333, time: 15.7555\n",
      "Epoch 243, loss = 28.8466, time: 15.8175\n",
      "Epoch 244, loss = 27.7048, time: 15.9636\n",
      "Epoch 245, loss = 31.3497, time: 16.2096\n",
      "Epoch 246, loss = 27.4414, time: 16.2066\n",
      "Epoch 247, loss = 30.1060, time: 15.8205\n",
      "Epoch 248, loss = 29.7765, time: 15.7755\n",
      "Epoch 249, loss = 28.5461, time: 15.9776\n",
      "Epoch 250, loss = 28.3096, time: 15.8245\n",
      "Epoch 251, loss = 31.5686, time: 15.8765\n",
      "Epoch 252, loss = 29.7633, time: 15.6975\n",
      "Epoch 253, loss = 30.1019, time: 15.7315\n",
      "Epoch 254, loss = 31.3842, time: 15.7545\n",
      "Epoch 255, loss = 27.3349, time: 16.0796\n",
      "Epoch 256, loss = 28.8252, time: 15.9706\n",
      "Epoch 257, loss = 30.8531, time: 15.7995\n",
      "Epoch 258, loss = 27.8111, time: 16.2626\n",
      "Epoch 259, loss = 29.7580, time: 16.5217\n",
      "Epoch 260, loss = 27.4445, time: 16.2536\n",
      "Epoch 261, loss = 28.6106, time: 15.8245\n",
      "Epoch 262, loss = 29.2078, time: 16.5207\n",
      "Epoch 263, loss = 28.5090, time: 16.0836\n",
      "Epoch 264, loss = 29.8614, time: 15.9015\n",
      "Epoch 265, loss = 27.0259, time: 16.6557\n",
      "Epoch 266, loss = 30.2374, time: 16.2896\n",
      "Epoch 267, loss = 26.2288, time: 16.3126\n",
      "Epoch 268, loss = 31.0538, time: 16.5627\n",
      "Epoch 269, loss = 30.5520, time: 17.6409\n",
      "Epoch 270, loss = 27.7906, time: 16.9048\n",
      "Epoch 271, loss = 29.5141, time: 15.8925\n",
      "Epoch 272, loss = 26.8082, time: 15.8825\n",
      "Epoch 273, loss = 28.3314, time: 16.0796\n",
      "Epoch 274, loss = 30.6231, time: 16.2236\n",
      "Epoch 275, loss = 29.2817, time: 16.3316\n",
      "Epoch 276, loss = 29.0840, time: 15.8181\n",
      "Epoch 277, loss = 28.9666, time: 15.7755\n",
      "Epoch 278, loss = 28.3682, time: 16.0806\n",
      "Epoch 279, loss = 29.2533, time: 15.7135\n",
      "Epoch 280, loss = 27.9832, time: 15.8895\n",
      "Epoch 281, loss = 28.7395, time: 16.0116\n",
      "Epoch 282, loss = 30.1207, time: 16.1386\n",
      "Epoch 283, loss = 29.7464, time: 16.2116\n",
      "Epoch 284, loss = 29.4060, time: 16.2486\n",
      "Epoch 285, loss = 29.3025, time: 16.8293\n",
      "Epoch 286, loss = 27.6795, time: 16.5527\n",
      "Epoch 287, loss = 29.9801, time: 16.5007\n",
      "Epoch 288, loss = 29.0698, time: 15.9901\n",
      "Epoch 289, loss = 26.7144, time: 17.2879\n",
      "Epoch 290, loss = 27.1036, time: 17.7060\n",
      "Epoch 291, loss = 28.7212, time: 15.7295\n",
      "Epoch 292, loss = 26.2645, time: 15.6665\n",
      "Epoch 293, loss = 27.1248, time: 15.9035\n",
      "Epoch 294, loss = 28.3971, time: 16.6877\n",
      "Epoch 295, loss = 26.0813, time: 16.9818\n",
      "Epoch 296, loss = 26.9740, time: 16.7737\n",
      "Epoch 297, loss = 29.9566, time: 16.5847\n",
      "Epoch 298, loss = 26.9866, time: 16.6967\n",
      "Epoch 299, loss = 27.9261, time: 16.5777\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.4982, val.acc = 0.6030\n",
      "Epoch 1, loss = 1.1502, val.acc = 0.6468\n",
      "Epoch 2, loss = 1.0501, val.acc = 0.6650\n",
      "Epoch 3, loss = 0.9890, val.acc = 0.6766\n",
      "Epoch 4, loss = 0.9445, val.acc = 0.6884\n",
      "Epoch 5, loss = 0.9095, val.acc = 0.6968\n",
      "Epoch 6, loss = 0.8805, val.acc = 0.7028\n",
      "Epoch 7, loss = 0.8557, val.acc = 0.7066\n",
      "Epoch 8, loss = 0.8340, val.acc = 0.7126\n",
      "Epoch 9, loss = 0.8148, val.acc = 0.7182\n",
      "Epoch 10, loss = 0.7975, val.acc = 0.7210\n",
      "Epoch 11, loss = 0.7817, val.acc = 0.7242\n",
      "Epoch 12, loss = 0.7673, val.acc = 0.7276\n",
      "Epoch 13, loss = 0.7539, val.acc = 0.7300\n",
      "Epoch 14, loss = 0.7414, val.acc = 0.7316\n",
      "Epoch 15, loss = 0.7298, val.acc = 0.7344\n",
      "Epoch 16, loss = 0.7189, val.acc = 0.7366\n",
      "Epoch 17, loss = 0.7085, val.acc = 0.7382\n",
      "Epoch 18, loss = 0.6988, val.acc = 0.7402\n",
      "Epoch 19, loss = 0.6895, val.acc = 0.7408\n",
      "Epoch 20, loss = 0.6806, val.acc = 0.7426\n",
      "Epoch 21, loss = 0.6722, val.acc = 0.7436\n",
      "Epoch 22, loss = 0.6641, val.acc = 0.7436\n",
      "Epoch 23, loss = 0.6564, val.acc = 0.7448\n",
      "Epoch 24, loss = 0.6489, val.acc = 0.7450\n",
      "Epoch 25, loss = 0.6418, val.acc = 0.7476\n",
      "Epoch 26, loss = 0.6348, val.acc = 0.7472\n",
      "Epoch 27, loss = 0.6282, val.acc = 0.7482\n",
      "Epoch 28, loss = 0.6217, val.acc = 0.7486\n",
      "Epoch 29, loss = 0.6155, val.acc = 0.7498\n",
      "Epoch 30, loss = 0.6094, val.acc = 0.7510\n",
      "Epoch 31, loss = 0.6035, val.acc = 0.7518\n",
      "Epoch 32, loss = 0.5978, val.acc = 0.7532\n",
      "Epoch 33, loss = 0.5923, val.acc = 0.7538\n",
      "Epoch 34, loss = 0.5869, val.acc = 0.7548\n",
      "Epoch 35, loss = 0.5816, val.acc = 0.7538\n",
      "Epoch 36, loss = 0.5765, val.acc = 0.7544\n",
      "Epoch 37, loss = 0.5715, val.acc = 0.7542\n",
      "Epoch 38, loss = 0.5667, val.acc = 0.7542\n",
      "Epoch 39, loss = 0.5619, val.acc = 0.7548\n",
      "Epoch 40, loss = 0.5573, val.acc = 0.7556\n",
      "Epoch 41, loss = 0.5527, val.acc = 0.7550\n",
      "Epoch 42, loss = 0.5483, val.acc = 0.7554\n",
      "Epoch 43, loss = 0.5440, val.acc = 0.7552\n",
      "Epoch 44, loss = 0.5397, val.acc = 0.7556\n",
      "Epoch 45, loss = 0.5355, val.acc = 0.7558\n",
      "Epoch 46, loss = 0.5315, val.acc = 0.7560\n",
      "Epoch 47, loss = 0.5275, val.acc = 0.7554\n",
      "Epoch 48, loss = 0.5235, val.acc = 0.7558\n",
      "Epoch 49, loss = 0.5197, val.acc = 0.7554\n",
      "Epoch 50, loss = 0.5159, val.acc = 0.7552\n",
      "Epoch 51, loss = 0.5122, val.acc = 0.7554\n",
      "Epoch 52, loss = 0.5086, val.acc = 0.7556\n",
      "Epoch 53, loss = 0.5050, val.acc = 0.7554\n",
      "Epoch 54, loss = 0.5015, val.acc = 0.7548\n",
      "Epoch 55, loss = 0.4980, val.acc = 0.7552\n",
      "Epoch 56, loss = 0.4946, val.acc = 0.7554\n",
      "Epoch 57, loss = 0.4913, val.acc = 0.7554\n",
      "Epoch 58, loss = 0.4880, val.acc = 0.7550\n",
      "Epoch 59, loss = 0.4848, val.acc = 0.7554\n",
      "Epoch 60, loss = 0.4816, val.acc = 0.7556\n",
      "Epoch 61, loss = 0.4784, val.acc = 0.7550\n",
      "Epoch 62, loss = 0.4754, val.acc = 0.7550\n",
      "Epoch 63, loss = 0.4723, val.acc = 0.7552\n",
      "Epoch 64, loss = 0.4693, val.acc = 0.7552\n",
      "Epoch 65, loss = 0.4664, val.acc = 0.7554\n",
      "Epoch 66, loss = 0.4635, val.acc = 0.7552\n",
      "Epoch 67, loss = 0.4606, val.acc = 0.7548\n",
      "Epoch 68, loss = 0.4578, val.acc = 0.7554\n",
      "Epoch 69, loss = 0.4550, val.acc = 0.7552\n",
      "Epoch 70, loss = 0.4523, val.acc = 0.7546\n",
      "Epoch 71, loss = 0.4496, val.acc = 0.7542\n",
      "Epoch 72, loss = 0.4469, val.acc = 0.7544\n",
      "Epoch 73, loss = 0.4443, val.acc = 0.7548\n",
      "Epoch 74, loss = 0.4417, val.acc = 0.7540\n",
      "Epoch 75, loss = 0.4391, val.acc = 0.7542\n",
      "Epoch 76, loss = 0.4366, val.acc = 0.7536\n",
      "Epoch 77, loss = 0.4341, val.acc = 0.7538\n",
      "Epoch 78, loss = 0.4316, val.acc = 0.7534\n",
      "Epoch 79, loss = 0.4292, val.acc = 0.7536\n",
      "Epoch 80, loss = 0.4268, val.acc = 0.7532\n",
      "Epoch 81, loss = 0.4244, val.acc = 0.7528\n",
      "Epoch 82, loss = 0.4221, val.acc = 0.7528\n",
      "Epoch 83, loss = 0.4197, val.acc = 0.7528\n",
      "Epoch 84, loss = 0.4174, val.acc = 0.7532\n",
      "Epoch 85, loss = 0.4152, val.acc = 0.7526\n",
      "Epoch 86, loss = 0.4129, val.acc = 0.7524\n",
      "Epoch 87, loss = 0.4107, val.acc = 0.7518\n",
      "Epoch 88, loss = 0.4085, val.acc = 0.7516\n",
      "Epoch 89, loss = 0.4064, val.acc = 0.7512\n",
      "Epoch 90, loss = 0.4043, val.acc = 0.7512\n",
      "Epoch 91, loss = 0.4021, val.acc = 0.7510\n",
      "Epoch 92, loss = 0.4000, val.acc = 0.7508\n",
      "Epoch 93, loss = 0.3980, val.acc = 0.7512\n",
      "Epoch 94, loss = 0.3959, val.acc = 0.7510\n",
      "Epoch 95, loss = 0.3939, val.acc = 0.7510\n",
      "Epoch 96, loss = 0.3919, val.acc = 0.7508\n",
      "Epoch 97, loss = 0.3899, val.acc = 0.7506\n",
      "Epoch 98, loss = 0.3880, val.acc = 0.7508\n",
      "Epoch 99, loss = 0.3860, val.acc = 0.7510\n",
      "Rep: 1, te.acc = 0.7322\n",
      "\n",
      "Rep 2\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead4): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BarlowTwinsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 79.4148, time: 15.9125\n",
      "Epoch 1, loss = 59.0271, time: 15.8165\n",
      "Epoch 2, loss = 57.6406, time: 16.2836\n",
      "Epoch 3, loss = 53.4218, time: 16.6227\n",
      "Epoch 4, loss = 52.9991, time: 17.0318\n",
      "Epoch 5, loss = 53.2532, time: 16.6917\n",
      "Epoch 6, loss = 52.3774, time: 16.7127\n",
      "Epoch 7, loss = 52.3639, time: 16.2386\n",
      "Epoch 8, loss = 51.4747, time: 16.1716\n",
      "Epoch 9, loss = 52.8582, time: 16.1136\n",
      "Epoch 10, loss = 51.7500, time: 16.8708\n",
      "Epoch 11, loss = 48.2121, time: 16.6377\n",
      "Epoch 12, loss = 50.3812, time: 17.1178\n",
      "Epoch 13, loss = 48.7410, time: 15.9316\n",
      "Epoch 14, loss = 47.9692, time: 15.9326\n",
      "Epoch 15, loss = 49.4184, time: 15.7135\n",
      "Epoch 16, loss = 49.1250, time: 15.7275\n",
      "Epoch 17, loss = 47.4617, time: 15.8195\n",
      "Epoch 18, loss = 45.8101, time: 16.0776\n",
      "Epoch 19, loss = 44.0936, time: 17.8600\n",
      "Epoch 20, loss = 45.4391, time: 17.5989\n",
      "Epoch 21, loss = 45.5158, time: 15.8885\n",
      "Epoch 22, loss = 44.5911, time: 15.8015\n",
      "Epoch 23, loss = 42.6975, time: 15.7945\n",
      "Epoch 24, loss = 44.5739, time: 15.7945\n",
      "Epoch 25, loss = 44.5413, time: 17.2468\n",
      "Epoch 26, loss = 45.4687, time: 16.0686\n",
      "Epoch 27, loss = 44.1521, time: 16.4727\n",
      "Epoch 28, loss = 43.9360, time: 15.8765\n",
      "Epoch 29, loss = 43.1851, time: 15.5755\n",
      "Epoch 30, loss = 45.2234, time: 16.2296\n",
      "Epoch 31, loss = 45.0070, time: 17.2258\n",
      "Epoch 32, loss = 44.3088, time: 16.5387\n",
      "Epoch 33, loss = 41.0268, time: 15.8885\n",
      "Epoch 34, loss = 42.1270, time: 16.1926\n",
      "Epoch 35, loss = 40.7427, time: 16.0506\n",
      "Epoch 36, loss = 39.9797, time: 15.9566\n",
      "Epoch 37, loss = 40.6623, time: 15.9336\n",
      "Epoch 38, loss = 41.6989, time: 16.1636\n",
      "Epoch 39, loss = 40.5750, time: 16.2486\n",
      "Epoch 40, loss = 44.1351, time: 16.3567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_84412/2636605875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBarlowTwinsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBTlambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_unsupervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_criterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[1;34m(pars, criterion, clf_criterion, optimizer)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m                 print('Epoch %d, loss = %.4f, time: %0.4f' %\n",
      "\u001b[0;32m    246\u001b[0m                       (e, running_loss, end_time))\n",
      "\u001b[1;32m--> 247\u001b[1;33m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_tar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    249\u001b[0m                 print('Epoch %d, loss = %.4f, val.acc = %.4f' %\n",
      "\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data, fix, model, pars, ep_loss, ep_acc, criterion, optimizer)\u001b[0m\n",
      "\u001b[0;32m    129\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    130\u001b[0m                     y = torch.from_numpy(\n",
      "\u001b[1;32m--> 131\u001b[1;33m                         train_tar[j:j+pars.batch_size]).to(device=device, dtype=torch.long)\n",
      "\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    133\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mget_BYOL_transforms\u001b[1;34m(is_first)\u001b[0m\n",
      "\u001b[0;32m     29\u001b[0m         \u001b[0mTF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomSolarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     30\u001b[0m     )\n",
      "\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mscripted_transforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscripted_transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_script.py\u001b[0m in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n",
      "\u001b[0;32m   1255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1256\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1257\u001b[1;33m         return torch.jit._recursive.create_script_module(\n",
      "\u001b[0m\u001b[0;32m   1258\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1259\u001b[0m         )\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n",
      "\u001b[0;32m    448\u001b[0m     \u001b[0mconcrete_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_module_concrete_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    449\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 450\u001b[1;33m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    451\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_check.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, nn_module)\u001b[0m\n",
      "\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musing_deprecated_ast\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 65\u001b[1;33m         \u001b[0msource_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m         \u001b[1;31m# This AST only contains the `__init__` method of the nn.Module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsource\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m   1022\u001b[0m     \u001b[1;32mor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mAn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1023\u001b[0m     OSError is raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[1;32m-> 1024\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1025\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m   1004\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[0;32m   1005\u001b[0m     \u001b[0mobject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1006\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1008\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m    815\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 817\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    818\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    819\u001b[0m         \u001b[1;31m# Invalidate cache if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m    704\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n",
      "\u001b[0;32m    705\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 706\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    707\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    708\u001b[0m     \u001b[1;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = BarlowTwinsLoss(pars.batch_size, pars.lam, pars.device)\n",
    "train_unsupervised(pars, criterion=criterion, clf_criterion=None, optimizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "train_unsupervised: True\n",
      "NUM_LAYER: 5\n",
      "\n",
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lam_1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.8132, time: 18.1821\n",
      "Epoch 1, loss = 0.6385, time: 17.2434\n",
      "Epoch 2, loss = 0.6473, time: 17.7809\n",
      "Epoch 3, loss = 0.6643, time: 17.4155\n",
      "Epoch 4, loss = 0.5067, time: 18.1811\n",
      "Epoch 5, loss = 0.4903, time: 17.1151\n",
      "Epoch 6, loss = 0.4694, time: 17.5653\n",
      "Epoch 7, loss = 0.4363, time: 17.1347\n",
      "Epoch 8, loss = 0.4073, time: 17.5262\n",
      "Epoch 9, loss = 0.3759, time: 16.8365\n",
      "Epoch 10, loss = 0.4121, time: 17.7623\n",
      "Epoch 11, loss = 0.3478, time: 17.1400\n",
      "Epoch 12, loss = 0.3511, time: 18.0677\n",
      "Epoch 13, loss = 0.3734, time: 17.6536\n",
      "Epoch 14, loss = 0.4019, time: 18.3632\n",
      "Epoch 15, loss = 0.3649, time: 17.7816\n",
      "Epoch 16, loss = 0.3221, time: 18.0817\n",
      "Epoch 17, loss = 0.3458, time: 17.3311\n",
      "Epoch 18, loss = 0.3175, time: 16.2092\n",
      "Epoch 19, loss = 0.3737, time: 16.5511\n",
      "Epoch 20, loss = 0.3826, time: 16.5848\n",
      "Epoch 21, loss = 0.3432, time: 16.4834\n",
      "Epoch 22, loss = 0.3437, time: 16.2985\n",
      "Epoch 23, loss = 0.3686, time: 16.0448\n",
      "Epoch 24, loss = 0.3351, time: 16.3060\n",
      "Epoch 25, loss = 0.3383, time: 16.0028\n",
      "Epoch 26, loss = 0.3065, time: 16.5917\n",
      "Epoch 27, loss = 0.3327, time: 16.1628\n",
      "Epoch 28, loss = 0.3301, time: 16.4421\n",
      "Epoch 29, loss = 0.3606, time: 16.0669\n",
      "Epoch 30, loss = 0.3244, time: 16.4019\n",
      "Epoch 31, loss = 0.3499, time: 16.3471\n",
      "Epoch 32, loss = 0.3423, time: 16.3855\n",
      "Epoch 33, loss = 0.3302, time: 16.2690\n",
      "Epoch 34, loss = 0.3125, time: 16.9637\n",
      "Epoch 35, loss = 0.3490, time: 16.1777\n",
      "Epoch 36, loss = 0.3169, time: 16.3770\n",
      "Epoch 37, loss = 0.3135, time: 16.2667\n",
      "Epoch 38, loss = 0.3294, time: 16.5967\n",
      "Epoch 39, loss = 0.2952, time: 17.0449\n",
      "Epoch 40, loss = 0.3475, time: 16.7751\n",
      "Epoch 41, loss = 0.3280, time: 16.2506\n",
      "Epoch 42, loss = 0.2872, time: 16.7363\n",
      "Epoch 43, loss = 0.3361, time: 15.9389\n",
      "Epoch 44, loss = 0.3215, time: 16.6789\n",
      "Epoch 45, loss = 0.2953, time: 16.2573\n",
      "Epoch 46, loss = 0.3268, time: 16.5321\n",
      "Epoch 47, loss = 0.3389, time: 16.0835\n",
      "Epoch 48, loss = 0.3104, time: 16.3050\n",
      "Epoch 49, loss = 0.3366, time: 16.2713\n",
      "Epoch 50, loss = 0.3252, time: 16.4576\n",
      "Epoch 51, loss = 0.3188, time: 16.3764\n",
      "Epoch 52, loss = 0.2921, time: 16.8065\n",
      "Epoch 53, loss = 0.3382, time: 15.9668\n",
      "Epoch 54, loss = 0.3236, time: 16.3649\n",
      "Epoch 55, loss = 0.3250, time: 16.1397\n",
      "Epoch 56, loss = 0.3262, time: 16.4155\n",
      "Epoch 57, loss = 0.3249, time: 16.0955\n",
      "Epoch 58, loss = 0.3177, time: 16.2597\n",
      "Epoch 59, loss = 0.3126, time: 16.1925\n",
      "Epoch 60, loss = 0.2902, time: 16.0963\n",
      "Epoch 61, loss = 0.2956, time: 15.7159\n",
      "Epoch 62, loss = 0.3024, time: 16.0764\n",
      "Epoch 63, loss = 0.3067, time: 15.6150\n",
      "Epoch 64, loss = 0.3076, time: 16.1137\n",
      "Epoch 65, loss = 0.3167, time: 15.5130\n",
      "Epoch 66, loss = 0.2720, time: 15.7458\n",
      "Epoch 67, loss = 0.2915, time: 15.7415\n",
      "Epoch 68, loss = 0.2745, time: 15.8780\n",
      "Epoch 69, loss = 0.2773, time: 15.5786\n",
      "Epoch 70, loss = 0.3019, time: 15.8976\n",
      "Epoch 71, loss = 0.3218, time: 15.5959\n",
      "Epoch 72, loss = 0.2736, time: 16.0294\n",
      "Epoch 73, loss = 0.2872, time: 15.7052\n",
      "Epoch 74, loss = 0.2718, time: 15.8743\n",
      "Epoch 75, loss = 0.3240, time: 15.7766\n",
      "Epoch 76, loss = 0.2988, time: 15.9391\n",
      "Epoch 77, loss = 0.3031, time: 15.6692\n",
      "Epoch 78, loss = 0.2979, time: 15.9449\n",
      "Epoch 79, loss = 0.2716, time: 15.6658\n",
      "Epoch 80, loss = 0.2964, time: 15.6877\n",
      "Epoch 81, loss = 0.2771, time: 15.8315\n",
      "Epoch 82, loss = 0.2701, time: 15.5443\n",
      "Epoch 83, loss = 0.2984, time: 16.0431\n",
      "Epoch 84, loss = 0.3201, time: 15.6338\n",
      "Epoch 85, loss = 0.3115, time: 15.7358\n",
      "Epoch 86, loss = 0.3048, time: 15.4971\n",
      "Epoch 87, loss = 0.2723, time: 15.7026\n",
      "Epoch 88, loss = 0.2869, time: 15.7631\n",
      "Epoch 89, loss = 0.2577, time: 16.4611\n",
      "Epoch 90, loss = 0.2661, time: 16.0724\n",
      "Epoch 91, loss = 0.3203, time: 16.3015\n",
      "Epoch 92, loss = 0.2621, time: 16.1233\n",
      "Epoch 93, loss = 0.2783, time: 16.1232\n",
      "Epoch 94, loss = 0.2715, time: 16.1636\n",
      "Epoch 95, loss = 0.2673, time: 16.0082\n",
      "Epoch 96, loss = 0.2482, time: 16.2516\n",
      "Epoch 97, loss = 0.2880, time: 16.2092\n",
      "Epoch 98, loss = 0.3027, time: 16.1676\n",
      "Epoch 99, loss = 0.2935, time: 16.2032\n",
      "Epoch 100, loss = 0.2972, time: 16.2774\n",
      "Epoch 101, loss = 0.2547, time: 16.0512\n",
      "Epoch 102, loss = 0.2587, time: 16.1214\n",
      "Epoch 103, loss = 0.2826, time: 16.0499\n",
      "Epoch 104, loss = 0.3130, time: 16.2008\n",
      "Epoch 105, loss = 0.2704, time: 16.1225\n",
      "Epoch 106, loss = 0.2552, time: 16.1276\n",
      "Epoch 107, loss = 0.2599, time: 16.3867\n",
      "Epoch 108, loss = 0.2979, time: 16.2235\n",
      "Epoch 109, loss = 0.3143, time: 16.1557\n",
      "Epoch 110, loss = 0.2599, time: 16.1593\n",
      "Epoch 111, loss = 0.2701, time: 16.1942\n",
      "Epoch 112, loss = 0.2888, time: 16.1273\n",
      "Epoch 113, loss = 0.2660, time: 16.1929\n",
      "Epoch 114, loss = 0.2509, time: 16.0684\n",
      "Epoch 115, loss = 0.2981, time: 16.3208\n",
      "Epoch 116, loss = 0.2557, time: 16.1581\n",
      "Epoch 117, loss = 0.2706, time: 16.1225\n",
      "Epoch 118, loss = 0.2948, time: 16.3075\n",
      "Epoch 119, loss = 0.2767, time: 16.2383\n",
      "Epoch 120, loss = 0.2694, time: 16.0930\n",
      "Epoch 121, loss = 0.2773, time: 15.9559\n",
      "Epoch 122, loss = 0.2660, time: 16.0481\n",
      "Epoch 123, loss = 0.2573, time: 16.0609\n",
      "Epoch 124, loss = 0.2380, time: 16.0951\n",
      "Epoch 125, loss = 0.2768, time: 16.0256\n",
      "Epoch 126, loss = 0.2917, time: 15.7569\n",
      "Epoch 127, loss = 0.2931, time: 15.5765\n",
      "Epoch 128, loss = 0.3005, time: 15.5287\n",
      "Epoch 129, loss = 0.2801, time: 15.6126\n",
      "Epoch 130, loss = 0.2382, time: 15.6360\n",
      "Epoch 131, loss = 0.2724, time: 15.5975\n",
      "Epoch 132, loss = 0.2834, time: 15.6692\n",
      "Epoch 133, loss = 0.2655, time: 15.7377\n",
      "Epoch 134, loss = 0.2516, time: 15.6425\n",
      "Epoch 135, loss = 0.2642, time: 15.6451\n",
      "Epoch 136, loss = 0.2787, time: 15.7071\n",
      "Epoch 137, loss = 0.2836, time: 15.5283\n",
      "Epoch 138, loss = 0.2548, time: 15.6739\n",
      "Epoch 139, loss = 0.2605, time: 15.5609\n",
      "Epoch 140, loss = 0.2986, time: 15.7102\n",
      "Epoch 141, loss = 0.2559, time: 15.5868\n",
      "Epoch 142, loss = 0.2637, time: 15.5400\n",
      "Epoch 143, loss = 0.2601, time: 15.5755\n",
      "Epoch 144, loss = 0.3025, time: 15.8288\n",
      "Epoch 145, loss = 0.2706, time: 16.1431\n",
      "Epoch 146, loss = 0.2461, time: 16.1235\n",
      "Epoch 147, loss = 0.2671, time: 16.0308\n",
      "Epoch 148, loss = 0.2485, time: 16.0115\n",
      "Epoch 149, loss = 0.2613, time: 16.1101\n",
      "Epoch 150, loss = 0.2738, time: 15.8922\n",
      "Epoch 151, loss = 0.2697, time: 16.0494\n",
      "Epoch 152, loss = 0.2676, time: 16.0660\n",
      "Epoch 153, loss = 0.2657, time: 16.1675\n",
      "Epoch 154, loss = 0.2655, time: 16.1365\n",
      "Epoch 155, loss = 0.2623, time: 16.0364\n",
      "Epoch 156, loss = 0.2667, time: 16.1635\n",
      "Epoch 157, loss = 0.2549, time: 16.0671\n",
      "Epoch 158, loss = 0.2611, time: 16.1091\n",
      "Epoch 159, loss = 0.2672, time: 16.1773\n",
      "Epoch 160, loss = 0.2822, time: 16.1403\n",
      "Epoch 161, loss = 0.2506, time: 16.0372\n",
      "Epoch 162, loss = 0.2669, time: 16.5276\n",
      "Epoch 163, loss = 0.2581, time: 16.0748\n",
      "Epoch 164, loss = 0.2745, time: 16.0149\n",
      "Epoch 165, loss = 0.2428, time: 16.1133\n",
      "Epoch 166, loss = 0.2566, time: 16.0054\n",
      "Epoch 167, loss = 0.3119, time: 16.0747\n",
      "Epoch 168, loss = 0.2583, time: 15.9492\n",
      "Epoch 169, loss = 0.2873, time: 15.9865\n",
      "Epoch 170, loss = 0.2549, time: 15.8971\n",
      "Epoch 171, loss = 0.2892, time: 16.3762\n",
      "Epoch 172, loss = 0.2309, time: 16.1811\n",
      "Epoch 173, loss = 0.2702, time: 15.9199\n",
      "Epoch 174, loss = 0.2622, time: 15.9891\n",
      "Epoch 175, loss = 0.2581, time: 16.1605\n",
      "Epoch 176, loss = 0.2544, time: 16.1287\n",
      "Epoch 177, loss = 0.2525, time: 16.0943\n",
      "Epoch 178, loss = 0.2439, time: 15.9861\n",
      "Epoch 179, loss = 0.2560, time: 16.0000\n",
      "Epoch 180, loss = 0.2490, time: 16.2305\n",
      "Epoch 181, loss = 0.2520, time: 15.9610\n",
      "Epoch 182, loss = 0.2810, time: 16.0585\n",
      "Epoch 183, loss = 0.2257, time: 16.0371\n",
      "Epoch 184, loss = 0.2551, time: 15.9738\n",
      "Epoch 185, loss = 0.2622, time: 15.9454\n",
      "Epoch 186, loss = 0.2379, time: 16.2170\n",
      "Epoch 187, loss = 0.2559, time: 15.8923\n",
      "Epoch 188, loss = 0.2750, time: 16.1415\n",
      "Epoch 189, loss = 0.2195, time: 16.0153\n",
      "Epoch 190, loss = 0.2915, time: 16.0143\n",
      "Epoch 191, loss = 0.2775, time: 15.7795\n",
      "Epoch 192, loss = 0.2297, time: 16.0289\n",
      "Epoch 193, loss = 0.2199, time: 16.1390\n",
      "Epoch 194, loss = 0.2540, time: 15.9326\n",
      "Epoch 195, loss = 0.2390, time: 15.7908\n",
      "Epoch 196, loss = 0.2721, time: 16.0684\n",
      "Epoch 197, loss = 0.2459, time: 15.9423\n",
      "Epoch 198, loss = 0.2580, time: 15.9991\n",
      "Epoch 199, loss = 0.2663, time: 15.8677\n",
      "Epoch 200, loss = 0.2270, time: 16.3343\n",
      "Epoch 201, loss = 0.2359, time: 16.5211\n",
      "Epoch 202, loss = 0.2548, time: 16.6242\n",
      "Epoch 203, loss = 0.2567, time: 16.6973\n",
      "Epoch 204, loss = 0.2354, time: 16.7043\n",
      "Epoch 205, loss = 0.2615, time: 16.7163\n",
      "Epoch 206, loss = 0.2586, time: 16.5747\n",
      "Epoch 207, loss = 0.2342, time: 16.6137\n",
      "Epoch 208, loss = 0.2695, time: 16.6124\n",
      "Epoch 209, loss = 0.2106, time: 16.7319\n",
      "Epoch 210, loss = 0.2725, time: 16.6621\n",
      "Epoch 211, loss = 0.2790, time: 16.7312\n",
      "Epoch 212, loss = 0.2443, time: 16.7960\n",
      "Epoch 213, loss = 0.2347, time: 16.8236\n",
      "Epoch 214, loss = 0.2351, time: 16.6952\n",
      "Epoch 215, loss = 0.2857, time: 16.5887\n",
      "Epoch 216, loss = 0.2709, time: 16.7578\n",
      "Epoch 217, loss = 0.2350, time: 16.6336\n",
      "Epoch 218, loss = 0.2465, time: 16.2391\n",
      "Epoch 219, loss = 0.2185, time: 16.1358\n",
      "Epoch 220, loss = 0.2796, time: 16.1001\n",
      "Epoch 221, loss = 0.2759, time: 16.1299\n",
      "Epoch 222, loss = 0.2525, time: 16.1064\n",
      "Epoch 223, loss = 0.2163, time: 16.0953\n",
      "Epoch 224, loss = 0.2564, time: 16.1141\n",
      "Epoch 225, loss = 0.2323, time: 16.0730\n",
      "Epoch 226, loss = 0.2023, time: 16.1450\n",
      "Epoch 227, loss = 0.2305, time: 16.0857\n",
      "Epoch 228, loss = 0.2325, time: 16.1611\n",
      "Epoch 229, loss = 0.2529, time: 16.2206\n",
      "Epoch 230, loss = 0.2373, time: 16.1672\n",
      "Epoch 231, loss = 0.2435, time: 16.2528\n",
      "Epoch 232, loss = 0.2404, time: 16.0596\n",
      "Epoch 233, loss = 0.2201, time: 16.1009\n",
      "Epoch 234, loss = 0.2376, time: 16.0795\n",
      "Epoch 235, loss = 0.2325, time: 16.1685\n",
      "Epoch 236, loss = 0.2706, time: 16.0396\n",
      "Epoch 237, loss = 0.2283, time: 15.9471\n",
      "Epoch 238, loss = 0.2746, time: 15.5886\n",
      "Epoch 239, loss = 0.2783, time: 15.6953\n",
      "Epoch 240, loss = 0.2294, time: 15.7286\n",
      "Epoch 241, loss = 0.2549, time: 15.6136\n",
      "Epoch 242, loss = 0.2078, time: 15.7193\n",
      "Epoch 243, loss = 0.2409, time: 15.7734\n",
      "Epoch 244, loss = 0.2517, time: 15.6734\n",
      "Epoch 245, loss = 0.2208, time: 15.6398\n",
      "Epoch 246, loss = 0.2173, time: 15.5966\n",
      "Epoch 247, loss = 0.2422, time: 15.7321\n",
      "Epoch 248, loss = 0.2540, time: 15.6262\n",
      "Epoch 249, loss = 0.2450, time: 15.6214\n",
      "Epoch 250, loss = 0.2225, time: 15.6596\n",
      "Epoch 251, loss = 0.2536, time: 15.7176\n",
      "Epoch 252, loss = 0.2436, time: 15.5770\n",
      "Epoch 253, loss = 0.2315, time: 15.5975\n",
      "Epoch 254, loss = 0.2176, time: 15.6728\n",
      "Epoch 255, loss = 0.2380, time: 15.5701\n",
      "Epoch 256, loss = 0.2635, time: 15.8766\n",
      "Epoch 257, loss = 0.2286, time: 16.0512\n",
      "Epoch 258, loss = 0.2593, time: 16.0995\n",
      "Epoch 259, loss = 0.2352, time: 16.1216\n",
      "Epoch 260, loss = 0.2491, time: 15.9860\n",
      "Epoch 261, loss = 0.2348, time: 16.2137\n",
      "Epoch 262, loss = 0.2327, time: 16.0839\n",
      "Epoch 263, loss = 0.2198, time: 16.1175\n",
      "Epoch 264, loss = 0.2476, time: 16.0771\n",
      "Epoch 265, loss = 0.2231, time: 16.1741\n",
      "Epoch 266, loss = 0.2493, time: 16.1278\n",
      "Epoch 267, loss = 0.2543, time: 16.1206\n",
      "Epoch 268, loss = 0.2241, time: 16.0543\n",
      "Epoch 269, loss = 0.2248, time: 16.0604\n",
      "Epoch 270, loss = 0.2288, time: 16.0675\n",
      "Epoch 271, loss = 0.2668, time: 15.9927\n",
      "Epoch 272, loss = 0.2215, time: 16.0570\n",
      "Epoch 273, loss = 0.2508, time: 16.0745\n",
      "Epoch 274, loss = 0.2296, time: 15.9952\n",
      "Epoch 275, loss = 0.2413, time: 16.1571\n",
      "Epoch 276, loss = 0.2302, time: 16.0122\n",
      "Epoch 277, loss = 0.2426, time: 16.0684\n",
      "Epoch 278, loss = 0.2369, time: 16.1366\n",
      "Epoch 279, loss = 0.2632, time: 16.5969\n",
      "Epoch 280, loss = 0.2464, time: 16.2423\n",
      "Epoch 281, loss = 0.2139, time: 16.0421\n",
      "Epoch 282, loss = 0.2778, time: 16.0511\n",
      "Epoch 283, loss = 0.2231, time: 16.1385\n",
      "Epoch 284, loss = 0.2631, time: 16.1451\n",
      "Epoch 285, loss = 0.2535, time: 16.1354\n",
      "Epoch 286, loss = 0.2464, time: 16.1901\n",
      "Epoch 287, loss = 0.2129, time: 16.0483\n",
      "Epoch 288, loss = 0.2102, time: 16.2185\n",
      "Epoch 289, loss = 0.2638, time: 16.1106\n",
      "Epoch 290, loss = 0.2452, time: 16.1269\n",
      "Epoch 291, loss = 0.2648, time: 16.0485\n",
      "Epoch 292, loss = 0.2470, time: 16.2331\n",
      "Epoch 293, loss = 0.2593, time: 15.8291\n",
      "Epoch 294, loss = 0.2376, time: 15.7620\n",
      "Epoch 295, loss = 0.2133, time: 15.5643\n",
      "Epoch 296, loss = 0.2470, time: 15.6049\n",
      "Epoch 297, loss = 0.2415, time: 15.6638\n",
      "Epoch 298, loss = 0.2270, time: 15.6213\n",
      "Epoch 299, loss = 0.2117, time: 15.6914\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.7193, val.acc = 0.5344\n",
      "Epoch 1, loss = 1.3829, val.acc = 0.5758\n",
      "Epoch 2, loss = 1.2750, val.acc = 0.5910\n",
      "Epoch 3, loss = 1.2103, val.acc = 0.6036\n",
      "Epoch 4, loss = 1.1640, val.acc = 0.6154\n",
      "Epoch 5, loss = 1.1277, val.acc = 0.6226\n",
      "Epoch 6, loss = 1.0976, val.acc = 0.6286\n",
      "Epoch 7, loss = 1.0716, val.acc = 0.6342\n",
      "Epoch 8, loss = 1.0486, val.acc = 0.6384\n",
      "Epoch 9, loss = 1.0280, val.acc = 0.6404\n",
      "Epoch 10, loss = 1.0091, val.acc = 0.6434\n",
      "Epoch 11, loss = 0.9918, val.acc = 0.6442\n",
      "Epoch 12, loss = 0.9759, val.acc = 0.6472\n",
      "Epoch 13, loss = 0.9610, val.acc = 0.6472\n",
      "Epoch 14, loss = 0.9471, val.acc = 0.6474\n",
      "Epoch 15, loss = 0.9340, val.acc = 0.6472\n",
      "Epoch 16, loss = 0.9216, val.acc = 0.6490\n",
      "Epoch 17, loss = 0.9099, val.acc = 0.6502\n",
      "Epoch 18, loss = 0.8988, val.acc = 0.6498\n",
      "Epoch 19, loss = 0.8882, val.acc = 0.6514\n",
      "Epoch 20, loss = 0.8781, val.acc = 0.6510\n",
      "Epoch 21, loss = 0.8684, val.acc = 0.6520\n",
      "Epoch 22, loss = 0.8591, val.acc = 0.6518\n",
      "Epoch 23, loss = 0.8502, val.acc = 0.6536\n",
      "Epoch 24, loss = 0.8416, val.acc = 0.6532\n",
      "Epoch 25, loss = 0.8334, val.acc = 0.6530\n",
      "Epoch 26, loss = 0.8254, val.acc = 0.6526\n",
      "Epoch 27, loss = 0.8177, val.acc = 0.6532\n",
      "Epoch 28, loss = 0.8102, val.acc = 0.6530\n",
      "Epoch 29, loss = 0.8030, val.acc = 0.6532\n",
      "Epoch 30, loss = 0.7960, val.acc = 0.6540\n",
      "Epoch 31, loss = 0.7893, val.acc = 0.6544\n",
      "Epoch 32, loss = 0.7827, val.acc = 0.6552\n",
      "Epoch 33, loss = 0.7763, val.acc = 0.6556\n",
      "Epoch 34, loss = 0.7700, val.acc = 0.6552\n",
      "Epoch 35, loss = 0.7640, val.acc = 0.6552\n",
      "Epoch 36, loss = 0.7581, val.acc = 0.6554\n",
      "Epoch 37, loss = 0.7524, val.acc = 0.6552\n",
      "Epoch 38, loss = 0.7468, val.acc = 0.6542\n",
      "Epoch 39, loss = 0.7413, val.acc = 0.6540\n",
      "Epoch 40, loss = 0.7360, val.acc = 0.6538\n",
      "Epoch 41, loss = 0.7308, val.acc = 0.6536\n",
      "Epoch 42, loss = 0.7257, val.acc = 0.6544\n",
      "Epoch 43, loss = 0.7208, val.acc = 0.6540\n",
      "Epoch 44, loss = 0.7159, val.acc = 0.6544\n",
      "Epoch 45, loss = 0.7112, val.acc = 0.6548\n",
      "Epoch 46, loss = 0.7065, val.acc = 0.6548\n",
      "Epoch 47, loss = 0.7020, val.acc = 0.6544\n",
      "Epoch 48, loss = 0.6976, val.acc = 0.6540\n",
      "Epoch 49, loss = 0.6932, val.acc = 0.6542\n",
      "Epoch 50, loss = 0.6890, val.acc = 0.6546\n",
      "Epoch 51, loss = 0.6848, val.acc = 0.6544\n",
      "Epoch 52, loss = 0.6808, val.acc = 0.6546\n",
      "Epoch 53, loss = 0.6768, val.acc = 0.6546\n",
      "Epoch 54, loss = 0.6729, val.acc = 0.6560\n",
      "Epoch 55, loss = 0.6691, val.acc = 0.6556\n",
      "Epoch 56, loss = 0.6654, val.acc = 0.6558\n",
      "Epoch 57, loss = 0.6617, val.acc = 0.6560\n",
      "Epoch 58, loss = 0.6582, val.acc = 0.6562\n",
      "Epoch 59, loss = 0.6547, val.acc = 0.6550\n",
      "Epoch 60, loss = 0.6513, val.acc = 0.6548\n",
      "Epoch 61, loss = 0.6480, val.acc = 0.6554\n",
      "Epoch 62, loss = 0.6447, val.acc = 0.6552\n",
      "Epoch 63, loss = 0.6415, val.acc = 0.6560\n",
      "Epoch 64, loss = 0.6384, val.acc = 0.6566\n",
      "Epoch 65, loss = 0.6353, val.acc = 0.6568\n",
      "Epoch 66, loss = 0.6323, val.acc = 0.6572\n",
      "Epoch 67, loss = 0.6294, val.acc = 0.6562\n",
      "Epoch 68, loss = 0.6265, val.acc = 0.6558\n",
      "Epoch 69, loss = 0.6238, val.acc = 0.6552\n",
      "Epoch 70, loss = 0.6211, val.acc = 0.6558\n",
      "Epoch 71, loss = 0.6184, val.acc = 0.6560\n",
      "Epoch 72, loss = 0.6159, val.acc = 0.6544\n",
      "Epoch 73, loss = 0.6134, val.acc = 0.6538\n",
      "Epoch 74, loss = 0.6110, val.acc = 0.6548\n",
      "Epoch 75, loss = 0.6086, val.acc = 0.6548\n",
      "Epoch 76, loss = 0.6063, val.acc = 0.6556\n",
      "Epoch 77, loss = 0.6040, val.acc = 0.6558\n",
      "Epoch 78, loss = 0.6017, val.acc = 0.6572\n",
      "Epoch 79, loss = 0.5994, val.acc = 0.6568\n",
      "Epoch 80, loss = 0.5972, val.acc = 0.6556\n",
      "Epoch 81, loss = 0.5950, val.acc = 0.6554\n",
      "Epoch 82, loss = 0.5929, val.acc = 0.6560\n",
      "Epoch 83, loss = 0.5908, val.acc = 0.6562\n",
      "Epoch 84, loss = 0.5886, val.acc = 0.6560\n",
      "Epoch 85, loss = 0.5864, val.acc = 0.6556\n",
      "Epoch 86, loss = 0.5839, val.acc = 0.6542\n",
      "Epoch 87, loss = 0.5813, val.acc = 0.6546\n",
      "Epoch 88, loss = 0.5785, val.acc = 0.6548\n",
      "Epoch 89, loss = 0.5756, val.acc = 0.6556\n",
      "Epoch 90, loss = 0.5727, val.acc = 0.6540\n",
      "Epoch 91, loss = 0.5698, val.acc = 0.6526\n",
      "Epoch 92, loss = 0.5670, val.acc = 0.6516\n",
      "Epoch 93, loss = 0.5644, val.acc = 0.6512\n",
      "Epoch 94, loss = 0.5618, val.acc = 0.6506\n",
      "Epoch 95, loss = 0.5593, val.acc = 0.6498\n",
      "Epoch 96, loss = 0.5568, val.acc = 0.6486\n",
      "Epoch 97, loss = 0.5545, val.acc = 0.6492\n",
      "Epoch 98, loss = 0.5522, val.acc = 0.6486\n",
      "Epoch 99, loss = 0.5500, val.acc = 0.6482\n",
      "Rep: 1, te.acc = 0.6363\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6363]\n"
     ]
    }
   ],
   "source": [
    "train_unsupervised_ae(pars)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd034ed54560f0698c2946b7ca675e493afbd7ee3c0ecf162ae3deac3cf4477b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
