{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "datadirs = ''\n",
    "sys.path.insert(1, datadirs)\n",
    "savepath = datadirs+'save/'\n",
    "datapath = datadirs+'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pars import PARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "from setup_net import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: BarlowTwins\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 3\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "BTlambda: 0.5\n",
      "auxnonlinear: None\n",
      "unsupervised: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.loss = \"BarlowTwins\"\n",
    "pars.lam = 0.5\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: BarlowTwins\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 3\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "BTlambda: 0.5\n",
      "auxnonlinear: None\n",
      "unsupervised: True\n",
      "\n",
      "save/CONV6/BarlowTwins/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lambda_0.5\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead4): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BarlowTwinsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 89.0901, time: 24.4807\n",
      "Epoch 1, loss = 59.9343, time: 16.2096\n",
      "Epoch 2, loss = 58.8422, time: 16.5327\n",
      "Epoch 3, loss = 57.5996, time: 16.5677\n",
      "Epoch 4, loss = 53.3677, time: 21.2097\n",
      "Epoch 5, loss = 52.8654, time: 19.0128\n",
      "Epoch 6, loss = 52.8058, time: 16.0056\n",
      "Epoch 7, loss = 51.0934, time: 16.5587\n",
      "Epoch 8, loss = 51.0642, time: 16.9047\n",
      "Epoch 9, loss = 53.5063, time: 16.2736\n",
      "Epoch 10, loss = 52.3528, time: 16.0686\n",
      "Epoch 11, loss = 51.2936, time: 16.0416\n",
      "Epoch 12, loss = 52.1284, time: 16.1906\n",
      "Epoch 13, loss = 50.2724, time: 17.4379\n",
      "Epoch 14, loss = 49.4269, time: 16.0186\n",
      "Epoch 15, loss = 49.4364, time: 16.1976\n",
      "Epoch 16, loss = 49.8539, time: 15.9306\n",
      "Epoch 17, loss = 45.7378, time: 15.7385\n",
      "Epoch 18, loss = 47.4326, time: 15.6615\n",
      "Epoch 19, loss = 48.3680, time: 16.0416\n",
      "Epoch 20, loss = 47.5249, time: 15.8235\n",
      "Epoch 21, loss = 46.8415, time: 15.7125\n",
      "Epoch 22, loss = 45.0612, time: 16.0616\n",
      "Epoch 23, loss = 47.5349, time: 15.6925\n",
      "Epoch 24, loss = 47.0085, time: 15.6485\n",
      "Epoch 25, loss = 43.5710, time: 15.8225\n",
      "Epoch 26, loss = 45.3337, time: 16.5147\n",
      "Epoch 27, loss = 43.8228, time: 17.6419\n",
      "Epoch 28, loss = 44.3009, time: 16.8638\n",
      "Epoch 29, loss = 45.3196, time: 16.4307\n",
      "Epoch 30, loss = 42.7299, time: 16.8768\n",
      "Epoch 31, loss = 42.7332, time: 17.3389\n",
      "Epoch 32, loss = 45.2224, time: 16.4147\n",
      "Epoch 33, loss = 44.6772, time: 16.6407\n",
      "Epoch 34, loss = 41.9051, time: 16.9668\n",
      "Epoch 35, loss = 41.3324, time: 16.9878\n",
      "Epoch 36, loss = 44.0670, time: 17.4089\n",
      "Epoch 37, loss = 43.6503, time: 15.7455\n",
      "Epoch 38, loss = 43.2293, time: 15.9206\n",
      "Epoch 39, loss = 40.3304, time: 15.8595\n",
      "Epoch 40, loss = 42.1439, time: 15.8655\n",
      "Epoch 41, loss = 39.7131, time: 15.8085\n",
      "Epoch 42, loss = 40.0624, time: 15.7785\n",
      "Epoch 43, loss = 41.6042, time: 15.7875\n",
      "Epoch 44, loss = 42.6738, time: 15.8005\n",
      "Epoch 45, loss = 41.8202, time: 15.8625\n",
      "Epoch 46, loss = 39.6132, time: 16.0526\n",
      "Epoch 47, loss = 39.4926, time: 16.0396\n",
      "Epoch 48, loss = 40.4637, time: 15.5795\n",
      "Epoch 49, loss = 40.2253, time: 15.6915\n",
      "Epoch 50, loss = 40.0659, time: 16.6777\n",
      "Epoch 51, loss = 40.3506, time: 17.5479\n",
      "Epoch 52, loss = 40.0202, time: 16.4107\n",
      "Epoch 53, loss = 41.0462, time: 15.9361\n",
      "Epoch 54, loss = 35.7337, time: 15.8775\n",
      "Epoch 55, loss = 37.6525, time: 15.7415\n",
      "Epoch 56, loss = 41.6484, time: 15.6705\n",
      "Epoch 57, loss = 38.1283, time: 15.7985\n",
      "Epoch 58, loss = 37.7508, time: 16.0126\n",
      "Epoch 59, loss = 40.4334, time: 15.6845\n",
      "Epoch 60, loss = 38.1648, time: 16.1466\n",
      "Epoch 61, loss = 39.0931, time: 15.8625\n",
      "Epoch 62, loss = 39.6849, time: 15.8595\n",
      "Epoch 63, loss = 38.4197, time: 15.9896\n",
      "Epoch 64, loss = 38.4090, time: 16.0996\n",
      "Epoch 65, loss = 38.1509, time: 17.1368\n",
      "Epoch 66, loss = 39.0201, time: 16.6387\n",
      "Epoch 67, loss = 35.8360, time: 16.8078\n",
      "Epoch 68, loss = 37.8608, time: 16.3416\n",
      "Epoch 69, loss = 38.5162, time: 18.0750\n",
      "Epoch 70, loss = 36.6316, time: 15.8645\n",
      "Epoch 71, loss = 37.5951, time: 16.7907\n",
      "Epoch 72, loss = 35.1992, time: 16.7897\n",
      "Epoch 73, loss = 38.7682, time: 16.4287\n",
      "Epoch 74, loss = 34.5237, time: 17.4849\n",
      "Epoch 75, loss = 39.7904, time: 16.1616\n",
      "Epoch 76, loss = 37.5505, time: 16.0836\n",
      "Epoch 77, loss = 34.8374, time: 16.0336\n",
      "Epoch 78, loss = 39.7451, time: 15.7625\n",
      "Epoch 79, loss = 36.6056, time: 16.0316\n",
      "Epoch 80, loss = 35.6746, time: 16.5787\n",
      "Epoch 81, loss = 38.8885, time: 16.0086\n",
      "Epoch 82, loss = 36.0457, time: 16.1676\n",
      "Epoch 83, loss = 36.1098, time: 16.1756\n",
      "Epoch 84, loss = 35.2123, time: 15.9946\n",
      "Epoch 85, loss = 34.4913, time: 16.3466\n",
      "Epoch 86, loss = 37.3131, time: 15.9896\n",
      "Epoch 87, loss = 33.0998, time: 15.9426\n",
      "Epoch 88, loss = 35.4722, time: 15.9146\n",
      "Epoch 89, loss = 37.2662, time: 16.8598\n",
      "Epoch 90, loss = 33.8919, time: 15.8795\n",
      "Epoch 91, loss = 35.4127, time: 15.7565\n",
      "Epoch 92, loss = 34.8284, time: 15.8675\n",
      "Epoch 93, loss = 32.5333, time: 15.6685\n",
      "Epoch 94, loss = 35.4991, time: 15.6195\n",
      "Epoch 95, loss = 34.1600, time: 16.6367\n",
      "Epoch 96, loss = 33.7699, time: 15.6765\n",
      "Epoch 97, loss = 34.0856, time: 15.7965\n",
      "Epoch 98, loss = 33.9188, time: 16.0206\n",
      "Epoch 99, loss = 32.1429, time: 16.1256\n",
      "Epoch 100, loss = 33.2090, time: 15.9616\n",
      "Epoch 101, loss = 34.6937, time: 15.6375\n",
      "Epoch 102, loss = 33.4933, time: 15.7375\n",
      "Epoch 103, loss = 32.6766, time: 15.7305\n",
      "Epoch 104, loss = 34.7614, time: 15.8455\n",
      "Epoch 105, loss = 35.9560, time: 16.3657\n",
      "Epoch 106, loss = 33.0569, time: 16.5447\n",
      "Epoch 107, loss = 32.1872, time: 16.2936\n",
      "Epoch 108, loss = 35.8856, time: 16.3236\n",
      "Epoch 109, loss = 32.9614, time: 15.8535\n",
      "Epoch 110, loss = 34.9500, time: 16.0086\n",
      "Epoch 111, loss = 33.6619, time: 15.4965\n",
      "Epoch 112, loss = 34.5401, time: 15.6475\n",
      "Epoch 113, loss = 35.0632, time: 15.6955\n",
      "Epoch 114, loss = 33.3046, time: 15.7815\n",
      "Epoch 115, loss = 34.5414, time: 15.8455\n",
      "Epoch 116, loss = 34.8414, time: 15.6695\n",
      "Epoch 117, loss = 30.7481, time: 15.8585\n",
      "Epoch 118, loss = 32.5020, time: 15.8335\n",
      "Epoch 119, loss = 32.8833, time: 16.0186\n",
      "Epoch 120, loss = 32.1610, time: 15.7855\n",
      "Epoch 121, loss = 33.6825, time: 15.6855\n",
      "Epoch 122, loss = 32.4203, time: 15.7975\n",
      "Epoch 123, loss = 32.6226, time: 15.7585\n",
      "Epoch 124, loss = 35.3090, time: 15.6935\n",
      "Epoch 125, loss = 32.9362, time: 15.8735\n",
      "Epoch 126, loss = 36.4391, time: 15.7935\n",
      "Epoch 127, loss = 34.2583, time: 15.6485\n",
      "Epoch 128, loss = 32.2959, time: 16.6787\n",
      "Epoch 129, loss = 32.3973, time: 16.8968\n",
      "Epoch 130, loss = 33.5538, time: 16.0866\n",
      "Epoch 131, loss = 32.8431, time: 15.8855\n",
      "Epoch 132, loss = 32.5826, time: 15.9296\n",
      "Epoch 133, loss = 30.6653, time: 15.7485\n",
      "Epoch 134, loss = 32.4414, time: 15.9936\n",
      "Epoch 135, loss = 31.5008, time: 16.0856\n",
      "Epoch 136, loss = 33.3039, time: 15.9886\n",
      "Epoch 137, loss = 27.8594, time: 15.7775\n",
      "Epoch 138, loss = 31.6022, time: 15.7655\n",
      "Epoch 139, loss = 33.7055, time: 15.7995\n",
      "Epoch 140, loss = 28.8161, time: 15.8875\n",
      "Epoch 141, loss = 30.7226, time: 15.8095\n",
      "Epoch 142, loss = 31.7082, time: 15.8575\n",
      "Epoch 143, loss = 31.2406, time: 15.7935\n",
      "Epoch 144, loss = 33.5814, time: 15.7485\n",
      "Epoch 145, loss = 34.5738, time: 15.9115\n",
      "Epoch 146, loss = 30.6528, time: 16.2286\n",
      "Epoch 147, loss = 32.8047, time: 15.9446\n",
      "Epoch 148, loss = 31.2798, time: 15.8165\n",
      "Epoch 149, loss = 32.4433, time: 15.8515\n",
      "Epoch 150, loss = 29.7632, time: 15.7725\n",
      "Epoch 151, loss = 32.1838, time: 15.8315\n",
      "Epoch 152, loss = 29.2778, time: 15.8055\n",
      "Epoch 153, loss = 32.2664, time: 15.7875\n",
      "Epoch 154, loss = 33.9154, time: 15.8595\n",
      "Epoch 155, loss = 33.7512, time: 15.8535\n",
      "Epoch 156, loss = 32.1973, time: 15.9336\n",
      "Epoch 157, loss = 30.7866, time: 15.8245\n",
      "Epoch 158, loss = 32.2908, time: 15.8105\n",
      "Epoch 159, loss = 32.1574, time: 15.7255\n",
      "Epoch 160, loss = 32.4443, time: 15.8105\n",
      "Epoch 161, loss = 30.0951, time: 15.8505\n",
      "Epoch 162, loss = 30.8996, time: 15.7915\n",
      "Epoch 163, loss = 31.8085, time: 16.0766\n",
      "Epoch 164, loss = 32.3888, time: 16.5707\n",
      "Epoch 165, loss = 31.3701, time: 16.4497\n",
      "Epoch 166, loss = 29.6376, time: 16.7477\n",
      "Epoch 167, loss = 33.4757, time: 16.1076\n",
      "Epoch 168, loss = 31.1806, time: 15.9346\n",
      "Epoch 169, loss = 32.6326, time: 15.7415\n",
      "Epoch 170, loss = 30.8250, time: 15.8285\n",
      "Epoch 171, loss = 33.0398, time: 15.8985\n",
      "Epoch 172, loss = 32.5704, time: 15.7395\n",
      "Epoch 173, loss = 29.5063, time: 16.3286\n",
      "Epoch 174, loss = 30.6236, time: 16.1066\n",
      "Epoch 175, loss = 31.5953, time: 15.7585\n",
      "Epoch 176, loss = 32.1888, time: 16.1426\n",
      "Epoch 177, loss = 31.0533, time: 16.0206\n",
      "Epoch 178, loss = 31.2403, time: 15.9866\n",
      "Epoch 179, loss = 29.5670, time: 16.2736\n",
      "Epoch 180, loss = 30.4384, time: 16.1606\n",
      "Epoch 181, loss = 29.5616, time: 16.2576\n",
      "Epoch 182, loss = 32.1035, time: 16.2286\n",
      "Epoch 183, loss = 31.5430, time: 17.5079\n",
      "Epoch 184, loss = 31.3090, time: 17.0328\n",
      "Epoch 185, loss = 30.2676, time: 16.3186\n",
      "Epoch 186, loss = 31.2578, time: 16.3036\n",
      "Epoch 187, loss = 33.8178, time: 16.0896\n",
      "Epoch 188, loss = 28.5344, time: 16.2016\n",
      "Epoch 189, loss = 28.9487, time: 16.7997\n",
      "Epoch 190, loss = 29.5368, time: 16.2926\n",
      "Epoch 191, loss = 31.5000, time: 16.9318\n",
      "Epoch 192, loss = 32.8290, time: 16.4627\n",
      "Epoch 193, loss = 30.7915, time: 16.2216\n",
      "Epoch 194, loss = 30.6418, time: 16.3807\n",
      "Epoch 195, loss = 30.3253, time: 16.3316\n",
      "Epoch 196, loss = 31.2129, time: 16.4017\n",
      "Epoch 197, loss = 32.0937, time: 16.3967\n",
      "Epoch 198, loss = 31.5132, time: 16.2026\n",
      "Epoch 199, loss = 31.3741, time: 16.0626\n",
      "Epoch 200, loss = 27.0106, time: 17.0908\n",
      "Epoch 201, loss = 28.3911, time: 16.4827\n",
      "Epoch 202, loss = 29.3803, time: 15.6555\n",
      "Epoch 203, loss = 29.4272, time: 15.8385\n",
      "Epoch 204, loss = 30.7196, time: 15.9456\n",
      "Epoch 205, loss = 31.2756, time: 15.8175\n",
      "Epoch 206, loss = 32.0408, time: 15.6355\n",
      "Epoch 207, loss = 32.6737, time: 15.5995\n",
      "Epoch 208, loss = 28.5453, time: 16.3186\n",
      "Epoch 209, loss = 30.2829, time: 15.8635\n",
      "Epoch 210, loss = 28.7964, time: 16.5127\n",
      "Epoch 211, loss = 31.2550, time: 16.3356\n",
      "Epoch 212, loss = 32.3243, time: 16.2506\n",
      "Epoch 213, loss = 32.6879, time: 16.0086\n",
      "Epoch 214, loss = 29.7239, time: 16.8348\n",
      "Epoch 215, loss = 29.0752, time: 16.4827\n",
      "Epoch 216, loss = 30.6614, time: 16.6447\n",
      "Epoch 217, loss = 31.0654, time: 16.2946\n",
      "Epoch 218, loss = 30.3462, time: 16.0976\n",
      "Epoch 219, loss = 29.1316, time: 16.7187\n",
      "Epoch 220, loss = 28.1657, time: 16.4487\n",
      "Epoch 221, loss = 28.8920, time: 16.5327\n",
      "Epoch 222, loss = 31.5417, time: 16.1056\n",
      "Epoch 223, loss = 28.5424, time: 16.1316\n",
      "Epoch 224, loss = 29.8327, time: 16.2206\n",
      "Epoch 225, loss = 29.6969, time: 16.5937\n",
      "Epoch 226, loss = 29.7101, time: 16.3406\n",
      "Epoch 227, loss = 29.3452, time: 16.5917\n",
      "Epoch 228, loss = 29.0658, time: 16.3667\n",
      "Epoch 229, loss = 28.5986, time: 16.0706\n",
      "Epoch 230, loss = 31.4705, time: 15.7835\n",
      "Epoch 231, loss = 29.1270, time: 15.7695\n",
      "Epoch 232, loss = 27.9568, time: 15.7635\n",
      "Epoch 233, loss = 29.6121, time: 16.9738\n",
      "Epoch 234, loss = 29.1912, time: 16.2606\n",
      "Epoch 235, loss = 28.2998, time: 15.9776\n",
      "Epoch 236, loss = 28.0775, time: 15.9386\n",
      "Epoch 237, loss = 30.5355, time: 16.0996\n",
      "Epoch 238, loss = 29.3976, time: 16.0276\n",
      "Epoch 239, loss = 29.0232, time: 16.2556\n",
      "Epoch 240, loss = 30.2952, time: 16.5287\n",
      "Epoch 241, loss = 31.9825, time: 16.6617\n",
      "Epoch 242, loss = 30.2333, time: 15.7555\n",
      "Epoch 243, loss = 28.8466, time: 15.8175\n",
      "Epoch 244, loss = 27.7048, time: 15.9636\n",
      "Epoch 245, loss = 31.3497, time: 16.2096\n",
      "Epoch 246, loss = 27.4414, time: 16.2066\n",
      "Epoch 247, loss = 30.1060, time: 15.8205\n",
      "Epoch 248, loss = 29.7765, time: 15.7755\n",
      "Epoch 249, loss = 28.5461, time: 15.9776\n",
      "Epoch 250, loss = 28.3096, time: 15.8245\n",
      "Epoch 251, loss = 31.5686, time: 15.8765\n",
      "Epoch 252, loss = 29.7633, time: 15.6975\n",
      "Epoch 253, loss = 30.1019, time: 15.7315\n",
      "Epoch 254, loss = 31.3842, time: 15.7545\n",
      "Epoch 255, loss = 27.3349, time: 16.0796\n",
      "Epoch 256, loss = 28.8252, time: 15.9706\n",
      "Epoch 257, loss = 30.8531, time: 15.7995\n",
      "Epoch 258, loss = 27.8111, time: 16.2626\n",
      "Epoch 259, loss = 29.7580, time: 16.5217\n",
      "Epoch 260, loss = 27.4445, time: 16.2536\n",
      "Epoch 261, loss = 28.6106, time: 15.8245\n",
      "Epoch 262, loss = 29.2078, time: 16.5207\n",
      "Epoch 263, loss = 28.5090, time: 16.0836\n",
      "Epoch 264, loss = 29.8614, time: 15.9015\n",
      "Epoch 265, loss = 27.0259, time: 16.6557\n",
      "Epoch 266, loss = 30.2374, time: 16.2896\n",
      "Epoch 267, loss = 26.2288, time: 16.3126\n",
      "Epoch 268, loss = 31.0538, time: 16.5627\n",
      "Epoch 269, loss = 30.5520, time: 17.6409\n",
      "Epoch 270, loss = 27.7906, time: 16.9048\n",
      "Epoch 271, loss = 29.5141, time: 15.8925\n",
      "Epoch 272, loss = 26.8082, time: 15.8825\n",
      "Epoch 273, loss = 28.3314, time: 16.0796\n",
      "Epoch 274, loss = 30.6231, time: 16.2236\n",
      "Epoch 275, loss = 29.2817, time: 16.3316\n",
      "Epoch 276, loss = 29.0840, time: 15.8181\n",
      "Epoch 277, loss = 28.9666, time: 15.7755\n",
      "Epoch 278, loss = 28.3682, time: 16.0806\n",
      "Epoch 279, loss = 29.2533, time: 15.7135\n",
      "Epoch 280, loss = 27.9832, time: 15.8895\n",
      "Epoch 281, loss = 28.7395, time: 16.0116\n",
      "Epoch 282, loss = 30.1207, time: 16.1386\n",
      "Epoch 283, loss = 29.7464, time: 16.2116\n",
      "Epoch 284, loss = 29.4060, time: 16.2486\n",
      "Epoch 285, loss = 29.3025, time: 16.8293\n",
      "Epoch 286, loss = 27.6795, time: 16.5527\n",
      "Epoch 287, loss = 29.9801, time: 16.5007\n",
      "Epoch 288, loss = 29.0698, time: 15.9901\n",
      "Epoch 289, loss = 26.7144, time: 17.2879\n",
      "Epoch 290, loss = 27.1036, time: 17.7060\n",
      "Epoch 291, loss = 28.7212, time: 15.7295\n",
      "Epoch 292, loss = 26.2645, time: 15.6665\n",
      "Epoch 293, loss = 27.1248, time: 15.9035\n",
      "Epoch 294, loss = 28.3971, time: 16.6877\n",
      "Epoch 295, loss = 26.0813, time: 16.9818\n",
      "Epoch 296, loss = 26.9740, time: 16.7737\n",
      "Epoch 297, loss = 29.9566, time: 16.5847\n",
      "Epoch 298, loss = 26.9866, time: 16.6967\n",
      "Epoch 299, loss = 27.9261, time: 16.5777\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.4982, val.acc = 0.6030\n",
      "Epoch 1, loss = 1.1502, val.acc = 0.6468\n",
      "Epoch 2, loss = 1.0501, val.acc = 0.6650\n",
      "Epoch 3, loss = 0.9890, val.acc = 0.6766\n",
      "Epoch 4, loss = 0.9445, val.acc = 0.6884\n",
      "Epoch 5, loss = 0.9095, val.acc = 0.6968\n",
      "Epoch 6, loss = 0.8805, val.acc = 0.7028\n",
      "Epoch 7, loss = 0.8557, val.acc = 0.7066\n",
      "Epoch 8, loss = 0.8340, val.acc = 0.7126\n",
      "Epoch 9, loss = 0.8148, val.acc = 0.7182\n",
      "Epoch 10, loss = 0.7975, val.acc = 0.7210\n",
      "Epoch 11, loss = 0.7817, val.acc = 0.7242\n",
      "Epoch 12, loss = 0.7673, val.acc = 0.7276\n",
      "Epoch 13, loss = 0.7539, val.acc = 0.7300\n",
      "Epoch 14, loss = 0.7414, val.acc = 0.7316\n",
      "Epoch 15, loss = 0.7298, val.acc = 0.7344\n",
      "Epoch 16, loss = 0.7189, val.acc = 0.7366\n",
      "Epoch 17, loss = 0.7085, val.acc = 0.7382\n",
      "Epoch 18, loss = 0.6988, val.acc = 0.7402\n",
      "Epoch 19, loss = 0.6895, val.acc = 0.7408\n",
      "Epoch 20, loss = 0.6806, val.acc = 0.7426\n",
      "Epoch 21, loss = 0.6722, val.acc = 0.7436\n",
      "Epoch 22, loss = 0.6641, val.acc = 0.7436\n",
      "Epoch 23, loss = 0.6564, val.acc = 0.7448\n",
      "Epoch 24, loss = 0.6489, val.acc = 0.7450\n",
      "Epoch 25, loss = 0.6418, val.acc = 0.7476\n",
      "Epoch 26, loss = 0.6348, val.acc = 0.7472\n",
      "Epoch 27, loss = 0.6282, val.acc = 0.7482\n",
      "Epoch 28, loss = 0.6217, val.acc = 0.7486\n",
      "Epoch 29, loss = 0.6155, val.acc = 0.7498\n",
      "Epoch 30, loss = 0.6094, val.acc = 0.7510\n",
      "Epoch 31, loss = 0.6035, val.acc = 0.7518\n",
      "Epoch 32, loss = 0.5978, val.acc = 0.7532\n",
      "Epoch 33, loss = 0.5923, val.acc = 0.7538\n",
      "Epoch 34, loss = 0.5869, val.acc = 0.7548\n",
      "Epoch 35, loss = 0.5816, val.acc = 0.7538\n",
      "Epoch 36, loss = 0.5765, val.acc = 0.7544\n",
      "Epoch 37, loss = 0.5715, val.acc = 0.7542\n",
      "Epoch 38, loss = 0.5667, val.acc = 0.7542\n",
      "Epoch 39, loss = 0.5619, val.acc = 0.7548\n",
      "Epoch 40, loss = 0.5573, val.acc = 0.7556\n",
      "Epoch 41, loss = 0.5527, val.acc = 0.7550\n",
      "Epoch 42, loss = 0.5483, val.acc = 0.7554\n",
      "Epoch 43, loss = 0.5440, val.acc = 0.7552\n",
      "Epoch 44, loss = 0.5397, val.acc = 0.7556\n",
      "Epoch 45, loss = 0.5355, val.acc = 0.7558\n",
      "Epoch 46, loss = 0.5315, val.acc = 0.7560\n",
      "Epoch 47, loss = 0.5275, val.acc = 0.7554\n",
      "Epoch 48, loss = 0.5235, val.acc = 0.7558\n",
      "Epoch 49, loss = 0.5197, val.acc = 0.7554\n",
      "Epoch 50, loss = 0.5159, val.acc = 0.7552\n",
      "Epoch 51, loss = 0.5122, val.acc = 0.7554\n",
      "Epoch 52, loss = 0.5086, val.acc = 0.7556\n",
      "Epoch 53, loss = 0.5050, val.acc = 0.7554\n",
      "Epoch 54, loss = 0.5015, val.acc = 0.7548\n",
      "Epoch 55, loss = 0.4980, val.acc = 0.7552\n",
      "Epoch 56, loss = 0.4946, val.acc = 0.7554\n",
      "Epoch 57, loss = 0.4913, val.acc = 0.7554\n",
      "Epoch 58, loss = 0.4880, val.acc = 0.7550\n",
      "Epoch 59, loss = 0.4848, val.acc = 0.7554\n",
      "Epoch 60, loss = 0.4816, val.acc = 0.7556\n",
      "Epoch 61, loss = 0.4784, val.acc = 0.7550\n",
      "Epoch 62, loss = 0.4754, val.acc = 0.7550\n",
      "Epoch 63, loss = 0.4723, val.acc = 0.7552\n",
      "Epoch 64, loss = 0.4693, val.acc = 0.7552\n",
      "Epoch 65, loss = 0.4664, val.acc = 0.7554\n",
      "Epoch 66, loss = 0.4635, val.acc = 0.7552\n",
      "Epoch 67, loss = 0.4606, val.acc = 0.7548\n",
      "Epoch 68, loss = 0.4578, val.acc = 0.7554\n",
      "Epoch 69, loss = 0.4550, val.acc = 0.7552\n",
      "Epoch 70, loss = 0.4523, val.acc = 0.7546\n",
      "Epoch 71, loss = 0.4496, val.acc = 0.7542\n",
      "Epoch 72, loss = 0.4469, val.acc = 0.7544\n",
      "Epoch 73, loss = 0.4443, val.acc = 0.7548\n",
      "Epoch 74, loss = 0.4417, val.acc = 0.7540\n",
      "Epoch 75, loss = 0.4391, val.acc = 0.7542\n",
      "Epoch 76, loss = 0.4366, val.acc = 0.7536\n",
      "Epoch 77, loss = 0.4341, val.acc = 0.7538\n",
      "Epoch 78, loss = 0.4316, val.acc = 0.7534\n",
      "Epoch 79, loss = 0.4292, val.acc = 0.7536\n",
      "Epoch 80, loss = 0.4268, val.acc = 0.7532\n",
      "Epoch 81, loss = 0.4244, val.acc = 0.7528\n",
      "Epoch 82, loss = 0.4221, val.acc = 0.7528\n",
      "Epoch 83, loss = 0.4197, val.acc = 0.7528\n",
      "Epoch 84, loss = 0.4174, val.acc = 0.7532\n",
      "Epoch 85, loss = 0.4152, val.acc = 0.7526\n",
      "Epoch 86, loss = 0.4129, val.acc = 0.7524\n",
      "Epoch 87, loss = 0.4107, val.acc = 0.7518\n",
      "Epoch 88, loss = 0.4085, val.acc = 0.7516\n",
      "Epoch 89, loss = 0.4064, val.acc = 0.7512\n",
      "Epoch 90, loss = 0.4043, val.acc = 0.7512\n",
      "Epoch 91, loss = 0.4021, val.acc = 0.7510\n",
      "Epoch 92, loss = 0.4000, val.acc = 0.7508\n",
      "Epoch 93, loss = 0.3980, val.acc = 0.7512\n",
      "Epoch 94, loss = 0.3959, val.acc = 0.7510\n",
      "Epoch 95, loss = 0.3939, val.acc = 0.7510\n",
      "Epoch 96, loss = 0.3919, val.acc = 0.7508\n",
      "Epoch 97, loss = 0.3899, val.acc = 0.7506\n",
      "Epoch 98, loss = 0.3880, val.acc = 0.7508\n",
      "Epoch 99, loss = 0.3860, val.acc = 0.7510\n",
      "Rep: 1, te.acc = 0.7322\n",
      "\n",
      "Rep 2\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead4): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BarlowTwinsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 79.4148, time: 15.9125\n",
      "Epoch 1, loss = 59.0271, time: 15.8165\n",
      "Epoch 2, loss = 57.6406, time: 16.2836\n",
      "Epoch 3, loss = 53.4218, time: 16.6227\n",
      "Epoch 4, loss = 52.9991, time: 17.0318\n",
      "Epoch 5, loss = 53.2532, time: 16.6917\n",
      "Epoch 6, loss = 52.3774, time: 16.7127\n",
      "Epoch 7, loss = 52.3639, time: 16.2386\n",
      "Epoch 8, loss = 51.4747, time: 16.1716\n",
      "Epoch 9, loss = 52.8582, time: 16.1136\n",
      "Epoch 10, loss = 51.7500, time: 16.8708\n",
      "Epoch 11, loss = 48.2121, time: 16.6377\n",
      "Epoch 12, loss = 50.3812, time: 17.1178\n",
      "Epoch 13, loss = 48.7410, time: 15.9316\n",
      "Epoch 14, loss = 47.9692, time: 15.9326\n",
      "Epoch 15, loss = 49.4184, time: 15.7135\n",
      "Epoch 16, loss = 49.1250, time: 15.7275\n",
      "Epoch 17, loss = 47.4617, time: 15.8195\n",
      "Epoch 18, loss = 45.8101, time: 16.0776\n",
      "Epoch 19, loss = 44.0936, time: 17.8600\n",
      "Epoch 20, loss = 45.4391, time: 17.5989\n",
      "Epoch 21, loss = 45.5158, time: 15.8885\n",
      "Epoch 22, loss = 44.5911, time: 15.8015\n",
      "Epoch 23, loss = 42.6975, time: 15.7945\n",
      "Epoch 24, loss = 44.5739, time: 15.7945\n",
      "Epoch 25, loss = 44.5413, time: 17.2468\n",
      "Epoch 26, loss = 45.4687, time: 16.0686\n",
      "Epoch 27, loss = 44.1521, time: 16.4727\n",
      "Epoch 28, loss = 43.9360, time: 15.8765\n",
      "Epoch 29, loss = 43.1851, time: 15.5755\n",
      "Epoch 30, loss = 45.2234, time: 16.2296\n",
      "Epoch 31, loss = 45.0070, time: 17.2258\n",
      "Epoch 32, loss = 44.3088, time: 16.5387\n",
      "Epoch 33, loss = 41.0268, time: 15.8885\n",
      "Epoch 34, loss = 42.1270, time: 16.1926\n",
      "Epoch 35, loss = 40.7427, time: 16.0506\n",
      "Epoch 36, loss = 39.9797, time: 15.9566\n",
      "Epoch 37, loss = 40.6623, time: 15.9336\n",
      "Epoch 38, loss = 41.6989, time: 16.1636\n",
      "Epoch 39, loss = 40.5750, time: 16.2486\n",
      "Epoch 40, loss = 44.1351, time: 16.3567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_84412/2636605875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBarlowTwinsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBTlambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_unsupervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_criterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[1;34m(pars, criterion, clf_criterion, optimizer)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m                 print('Epoch %d, loss = %.4f, time: %0.4f' %\n",
      "\u001b[0;32m    246\u001b[0m                       (e, running_loss, end_time))\n",
      "\u001b[1;32m--> 247\u001b[1;33m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_tar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    249\u001b[0m                 print('Epoch %d, loss = %.4f, val.acc = %.4f' %\n",
      "\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data, fix, model, pars, ep_loss, ep_acc, criterion, optimizer)\u001b[0m\n",
      "\u001b[0;32m    129\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    130\u001b[0m                     y = torch.from_numpy(\n",
      "\u001b[1;32m--> 131\u001b[1;33m                         train_tar[j:j+pars.batch_size]).to(device=device, dtype=torch.long)\n",
      "\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    133\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mget_BYOL_transforms\u001b[1;34m(is_first)\u001b[0m\n",
      "\u001b[0;32m     29\u001b[0m         \u001b[0mTF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomSolarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     30\u001b[0m     )\n",
      "\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mscripted_transforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscripted_transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_script.py\u001b[0m in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n",
      "\u001b[0;32m   1255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1256\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1257\u001b[1;33m         return torch.jit._recursive.create_script_module(\n",
      "\u001b[0m\u001b[0;32m   1258\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1259\u001b[0m         )\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n",
      "\u001b[0;32m    448\u001b[0m     \u001b[0mconcrete_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_module_concrete_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    449\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 450\u001b[1;33m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    451\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_check.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, nn_module)\u001b[0m\n",
      "\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musing_deprecated_ast\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 65\u001b[1;33m         \u001b[0msource_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m         \u001b[1;31m# This AST only contains the `__init__` method of the nn.Module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsource\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m   1022\u001b[0m     \u001b[1;32mor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mAn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1023\u001b[0m     OSError is raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[1;32m-> 1024\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1025\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m   1004\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[0;32m   1005\u001b[0m     \u001b[0mobject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1006\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1008\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m    815\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n",
      "\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 817\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    818\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    819\u001b[0m         \u001b[1;31m# Invalidate cache if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n",
      "\u001b[0;32m    704\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n",
      "\u001b[0;32m    705\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 706\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    707\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    708\u001b[0m     \u001b[1;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = BarlowTwinsLoss(pars.batch_size, pars.lam, pars.device)\n",
    "train_unsupervised(pars, criterion=criterion, clf_criterion=None, optimizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "train_unsupervised: True\n",
      "NUM_LAYER: 5\n",
      "\n",
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lam_1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (unflatten): Unflatten(dim=8192, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (unflatten): Unflatten(dim=8192, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 8192)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_111796/4172230237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_unsupervised_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_unsupervised_ae\u001b[1;34m(pars, criterion, clf_criterion, optimizer)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         train_model_ae(data, fix, model, decoder, pars, head_loss,\n\u001b[0m\u001b[0;32m    396\u001b[0m                     None, criterion, optimizer)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_model_ae\u001b[1;34m(data, fix, model, decoder, pars, ep_loss, ep_acc, criterion_re, criterion_sim, optimizer)\u001b[0m\n\u001b[0;32m    221\u001b[0m                     \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mx_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mloss_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_re\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\flatten.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflattened_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36munflatten\u001b[1;34m(self, dim, sizes)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m             \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munzip_namedshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 8192)"
     ]
    }
   ],
   "source": [
    "train_unsupervised_ae(pars)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd034ed54560f0698c2946b7ca675e493afbd7ee3c0ecf162ae3deac3cf4477b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
