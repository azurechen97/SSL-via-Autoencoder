{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "datadirs = ''\n",
    "sys.path.insert(1, datadirs)\n",
    "savepath = datadirs+'save/'\n",
    "datapath = datadirs+'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visdom\n",
    "# python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pars import PARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "from setup_net import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.9\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.9\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.1703, time: 18.6848\n",
      "reconstruction loss = 0.0681, similarity loss: 1.0902\n",
      "Epoch 1, loss = 0.1428, time: 18.2625\n",
      "reconstruction loss = 0.0673, similarity loss: 0.8222\n",
      "Epoch 2, loss = 0.1355, time: 18.4255\n",
      "reconstruction loss = 0.0701, similarity loss: 0.7243\n",
      "Epoch 3, loss = 0.1306, time: 18.3827\n",
      "reconstruction loss = 0.0695, similarity loss: 0.6806\n",
      "Epoch 4, loss = 0.1388, time: 18.5344\n",
      "reconstruction loss = 0.0664, similarity loss: 0.7904\n",
      "Epoch 5, loss = 0.1266, time: 18.5743\n",
      "reconstruction loss = 0.0665, similarity loss: 0.6675\n",
      "Epoch 6, loss = 0.1285, time: 17.2996\n",
      "reconstruction loss = 0.0660, similarity loss: 0.6915\n",
      "Epoch 7, loss = 0.1429, time: 17.3744\n",
      "reconstruction loss = 0.0675, similarity loss: 0.8221\n",
      "Epoch 8, loss = 0.1298, time: 17.3177\n",
      "reconstruction loss = 0.0685, similarity loss: 0.6814\n",
      "Epoch 9, loss = 0.1310, time: 17.2740\n",
      "reconstruction loss = 0.0686, similarity loss: 0.6933\n",
      "Epoch 10, loss = 0.1270, time: 17.2516\n",
      "reconstruction loss = 0.0676, similarity loss: 0.6619\n",
      "Epoch 11, loss = 0.1254, time: 17.1979\n",
      "reconstruction loss = 0.0687, similarity loss: 0.6357\n",
      "Epoch 12, loss = 0.1334, time: 17.6165\n",
      "reconstruction loss = 0.0656, similarity loss: 0.7436\n",
      "Epoch 13, loss = 0.1174, time: 17.8322\n",
      "reconstruction loss = 0.0662, similarity loss: 0.5779\n",
      "Epoch 14, loss = 0.1277, time: 17.3203\n",
      "reconstruction loss = 0.0676, similarity loss: 0.6683\n",
      "Epoch 15, loss = 0.1073, time: 17.2114\n",
      "reconstruction loss = 0.0677, similarity loss: 0.4640\n",
      "Epoch 16, loss = 0.1196, time: 17.1964\n",
      "reconstruction loss = 0.0660, similarity loss: 0.6014\n",
      "Epoch 17, loss = 0.1194, time: 17.2570\n",
      "reconstruction loss = 0.0652, similarity loss: 0.6078\n",
      "Epoch 18, loss = 0.1228, time: 17.3787\n",
      "reconstruction loss = 0.0654, similarity loss: 0.6396\n",
      "Epoch 19, loss = 0.1232, time: 17.4198\n",
      "reconstruction loss = 0.0672, similarity loss: 0.6264\n",
      "Epoch 20, loss = 0.1146, time: 17.3642\n",
      "reconstruction loss = 0.0660, similarity loss: 0.5521\n",
      "Epoch 21, loss = 0.1157, time: 17.3275\n",
      "reconstruction loss = 0.0667, similarity loss: 0.5575\n",
      "Epoch 22, loss = 0.1139, time: 17.3764\n",
      "reconstruction loss = 0.0683, similarity loss: 0.5239\n",
      "Epoch 23, loss = 0.1103, time: 17.2444\n",
      "reconstruction loss = 0.0685, similarity loss: 0.4862\n",
      "Epoch 24, loss = 0.1008, time: 17.2410\n",
      "reconstruction loss = 0.0666, similarity loss: 0.4083\n",
      "Epoch 25, loss = 0.0978, time: 17.3816\n",
      "reconstruction loss = 0.0675, similarity loss: 0.3705\n",
      "Epoch 26, loss = 0.1052, time: 17.1948\n",
      "reconstruction loss = 0.0678, similarity loss: 0.4415\n",
      "Epoch 27, loss = 0.0949, time: 17.7122\n",
      "reconstruction loss = 0.0684, similarity loss: 0.3333\n",
      "Epoch 28, loss = 0.0952, time: 17.8125\n",
      "reconstruction loss = 0.0667, similarity loss: 0.3515\n",
      "Epoch 29, loss = 0.0984, time: 18.6258\n",
      "reconstruction loss = 0.0658, similarity loss: 0.3927\n",
      "Epoch 30, loss = 0.0929, time: 17.6310\n",
      "reconstruction loss = 0.0649, similarity loss: 0.3447\n",
      "Epoch 31, loss = 0.0965, time: 17.5785\n",
      "reconstruction loss = 0.0674, similarity loss: 0.3591\n",
      "Epoch 32, loss = 0.0968, time: 17.3149\n",
      "reconstruction loss = 0.0675, similarity loss: 0.3604\n",
      "Epoch 33, loss = 0.0963, time: 17.2577\n",
      "reconstruction loss = 0.0667, similarity loss: 0.3627\n",
      "Epoch 34, loss = 0.0921, time: 17.4030\n",
      "reconstruction loss = 0.0672, similarity loss: 0.3162\n",
      "Epoch 35, loss = 0.0908, time: 17.4990\n",
      "reconstruction loss = 0.0633, similarity loss: 0.3378\n",
      "Epoch 36, loss = 0.0919, time: 17.3895\n",
      "reconstruction loss = 0.0657, similarity loss: 0.3279\n",
      "Epoch 37, loss = 0.0928, time: 17.2082\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3425\n",
      "Epoch 38, loss = 0.0907, time: 17.2901\n",
      "reconstruction loss = 0.0645, similarity loss: 0.3266\n",
      "Epoch 39, loss = 0.0904, time: 17.2880\n",
      "reconstruction loss = 0.0623, similarity loss: 0.3436\n",
      "Epoch 40, loss = 0.0904, time: 17.3264\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3187\n",
      "Epoch 41, loss = 0.0851, time: 17.2569\n",
      "reconstruction loss = 0.0610, similarity loss: 0.3021\n",
      "Epoch 42, loss = 0.0945, time: 17.2998\n",
      "reconstruction loss = 0.0650, similarity loss: 0.3601\n",
      "Epoch 43, loss = 0.0907, time: 17.2655\n",
      "reconstruction loss = 0.0646, similarity loss: 0.3259\n",
      "Epoch 44, loss = 0.0867, time: 17.7053\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3030\n",
      "Epoch 45, loss = 0.0900, time: 17.5273\n",
      "reconstruction loss = 0.0631, similarity loss: 0.3316\n",
      "Epoch 46, loss = 0.0906, time: 17.5114\n",
      "reconstruction loss = 0.0646, similarity loss: 0.3250\n",
      "Epoch 47, loss = 0.0862, time: 17.2761\n",
      "reconstruction loss = 0.0634, similarity loss: 0.2917\n",
      "Epoch 48, loss = 0.0870, time: 17.3438\n",
      "reconstruction loss = 0.0644, similarity loss: 0.2910\n",
      "Epoch 49, loss = 0.0915, time: 17.3394\n",
      "reconstruction loss = 0.0661, similarity loss: 0.3194\n",
      "Epoch 50, loss = 0.0916, time: 17.2205\n",
      "reconstruction loss = 0.0632, similarity loss: 0.3471\n",
      "Epoch 51, loss = 0.0938, time: 17.2233\n",
      "reconstruction loss = 0.0652, similarity loss: 0.3504\n",
      "Epoch 52, loss = 0.0867, time: 17.3852\n",
      "reconstruction loss = 0.0640, similarity loss: 0.2912\n",
      "Epoch 53, loss = 0.0906, time: 17.4452\n",
      "reconstruction loss = 0.0634, similarity loss: 0.3349\n",
      "Epoch 54, loss = 0.0864, time: 17.2559\n",
      "reconstruction loss = 0.0652, similarity loss: 0.2779\n",
      "Epoch 55, loss = 0.0877, time: 17.3393\n",
      "reconstruction loss = 0.0636, similarity loss: 0.3050\n",
      "Epoch 56, loss = 0.0898, time: 17.4335\n",
      "reconstruction loss = 0.0657, similarity loss: 0.3066\n",
      "Epoch 57, loss = 0.0878, time: 17.3598\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3139\n",
      "Epoch 58, loss = 0.0902, time: 17.3680\n",
      "reconstruction loss = 0.0638, similarity loss: 0.3284\n",
      "Epoch 59, loss = 0.0830, time: 17.2782\n",
      "reconstruction loss = 0.0632, similarity loss: 0.2611\n",
      "Epoch 60, loss = 0.0946, time: 17.2823\n",
      "reconstruction loss = 0.0672, similarity loss: 0.3407\n",
      "Epoch 61, loss = 0.0861, time: 17.4135\n",
      "reconstruction loss = 0.0651, similarity loss: 0.2746\n",
      "Epoch 62, loss = 0.0846, time: 17.2769\n",
      "reconstruction loss = 0.0663, similarity loss: 0.2493\n",
      "Epoch 63, loss = 0.0868, time: 17.3815\n",
      "reconstruction loss = 0.0627, similarity loss: 0.3038\n",
      "Epoch 64, loss = 0.0812, time: 17.2099\n",
      "reconstruction loss = 0.0633, similarity loss: 0.2422\n",
      "Epoch 65, loss = 0.0871, time: 17.3848\n",
      "reconstruction loss = 0.0640, similarity loss: 0.2943\n",
      "Epoch 66, loss = 0.0911, time: 17.1051\n",
      "reconstruction loss = 0.0638, similarity loss: 0.3373\n",
      "Epoch 67, loss = 0.0765, time: 17.1823\n",
      "reconstruction loss = 0.0417, similarity loss: 0.3902\n",
      "Epoch 68, loss = 0.0678, time: 17.2673\n",
      "reconstruction loss = 0.0381, similarity loss: 0.3345\n",
      "Epoch 69, loss = 0.0650, time: 17.2493\n",
      "reconstruction loss = 0.0374, similarity loss: 0.3130\n",
      "Epoch 70, loss = 0.0623, time: 17.2742\n",
      "reconstruction loss = 0.0368, similarity loss: 0.2927\n",
      "Epoch 71, loss = 0.0574, time: 17.5128\n",
      "reconstruction loss = 0.0306, similarity loss: 0.2989\n",
      "Epoch 72, loss = 0.0626, time: 17.7839\n",
      "reconstruction loss = 0.0336, similarity loss: 0.3234\n",
      "Epoch 73, loss = 0.0536, time: 17.8566\n",
      "reconstruction loss = 0.0282, similarity loss: 0.2827\n",
      "Epoch 74, loss = 0.0568, time: 17.7566\n",
      "reconstruction loss = 0.0272, similarity loss: 0.3235\n",
      "Epoch 75, loss = 0.0588, time: 17.7354\n",
      "reconstruction loss = 0.0283, similarity loss: 0.3332\n",
      "Epoch 76, loss = 0.0595, time: 17.3011\n",
      "reconstruction loss = 0.0280, similarity loss: 0.3432\n",
      "Epoch 77, loss = 0.0575, time: 17.2640\n",
      "reconstruction loss = 0.0275, similarity loss: 0.3278\n",
      "Epoch 78, loss = 0.0534, time: 17.2528\n",
      "reconstruction loss = 0.0271, similarity loss: 0.2898\n",
      "Epoch 79, loss = 0.0508, time: 17.2211\n",
      "reconstruction loss = 0.0260, similarity loss: 0.2736\n",
      "Epoch 80, loss = 0.0529, time: 17.4135\n",
      "reconstruction loss = 0.0246, similarity loss: 0.3072\n",
      "Epoch 81, loss = 0.0501, time: 17.4401\n",
      "reconstruction loss = 0.0238, similarity loss: 0.2869\n",
      "Epoch 82, loss = 0.0474, time: 17.2678\n",
      "reconstruction loss = 0.0223, similarity loss: 0.2728\n",
      "Epoch 83, loss = 0.0461, time: 17.2363\n",
      "reconstruction loss = 0.0206, similarity loss: 0.2756\n",
      "Epoch 84, loss = 0.0590, time: 17.2989\n",
      "reconstruction loss = 0.0286, similarity loss: 0.3323\n",
      "Epoch 85, loss = 0.1063, time: 17.3594\n",
      "reconstruction loss = 0.0570, similarity loss: 0.5494\n",
      "Epoch 86, loss = 0.0708, time: 17.2517\n",
      "reconstruction loss = 0.0326, similarity loss: 0.4150\n",
      "Epoch 87, loss = 0.0587, time: 17.2997\n",
      "reconstruction loss = 0.0270, similarity loss: 0.3440\n",
      "Epoch 88, loss = 0.0520, time: 17.2833\n",
      "reconstruction loss = 0.0246, similarity loss: 0.2992\n",
      "Epoch 89, loss = 0.0550, time: 17.3615\n",
      "reconstruction loss = 0.0270, similarity loss: 0.3069\n",
      "Epoch 90, loss = 0.0580, time: 17.2424\n",
      "reconstruction loss = 0.0285, similarity loss: 0.3235\n",
      "Epoch 91, loss = 0.0532, time: 17.2679\n",
      "reconstruction loss = 0.0277, similarity loss: 0.2826\n",
      "Epoch 92, loss = 0.1303, time: 17.2991\n",
      "reconstruction loss = 0.0973, similarity loss: 0.4276\n",
      "Epoch 93, loss = 0.1805, time: 17.3146\n",
      "reconstruction loss = 0.1337, similarity loss: 0.6026\n",
      "Epoch 94, loss = 0.1689, time: 17.2991\n",
      "reconstruction loss = 0.1209, similarity loss: 0.6011\n",
      "Epoch 95, loss = 0.1676, time: 17.2709\n",
      "reconstruction loss = 0.1157, similarity loss: 0.6343\n",
      "Epoch 96, loss = 0.1579, time: 17.2521\n",
      "reconstruction loss = 0.1157, similarity loss: 0.5378\n",
      "Epoch 97, loss = 0.1555, time: 17.0964\n",
      "reconstruction loss = 0.1130, similarity loss: 0.5379\n",
      "Epoch 98, loss = 0.1568, time: 17.0647\n",
      "reconstruction loss = 0.1116, similarity loss: 0.5640\n",
      "Epoch 99, loss = 0.1462, time: 17.2989\n",
      "reconstruction loss = 0.1067, similarity loss: 0.5015\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_0_9')\n",
    "find_lr_ae(pars, lr0=1e-6, lr1=1e-2, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_62960/3857999388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8097\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'find_lr_0_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfind_lr_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mfind_lr_ae\u001b[1;34m(pars, lr0, lr1, n_epochs, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(datapath, dataset, num_train)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Cifar100'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIFAR100\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtrain_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mtrain_tar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Cifar10'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_0_1')\n",
    "find_lr_ae(pars, lr0=1e-8, lr1=1e-3, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: -1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = -1\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-08\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.6240, time: 9.8681\n",
      "reconstruction loss = 0.0693, similarity loss: 1.1786\n",
      "Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "Epoch 1, loss = 0.6432, time: 9.4241\n",
      "reconstruction loss = 0.0718, similarity loss: 1.2145\n",
      "Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "Epoch 2, loss = 0.6363, time: 9.4796\n",
      "reconstruction loss = 0.0699, similarity loss: 1.2027\n",
      "Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_70848/616924429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8097\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'find_lr_mtl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfind_lr_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mfind_lr_ae\u001b[1;34m(pars, lr0, lr1, n_epochs, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[0mfind_lr_model_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mcriterion_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_sim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_lr_model_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_re\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_sim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mfind_lr_model_ae\u001b[1;34m(data, fix, model, decoder, pars, ep_loss, lr0, lr1, n_epochs, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                 \u001b[0mx_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[0mloss_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097, env='find_lr_mtl')\n",
    "find_lr_ae(pars, lr0=1e-8, lr1=1, n_epochs=100, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.9\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.9\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_300_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_0.9\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.1172, time: 28.5337\n",
      "reconstruction loss = 0.0534, similarity loss: 0.6910\n",
      "Epoch 1, loss = 0.0926, time: 17.6372\n",
      "reconstruction loss = 0.0415, similarity loss: 0.5526\n",
      "Epoch 2, loss = 0.0994, time: 17.2684\n",
      "reconstruction loss = 0.0358, similarity loss: 0.6714\n",
      "Epoch 3, loss = 0.0949, time: 17.3492\n",
      "reconstruction loss = 0.0303, similarity loss: 0.6766\n",
      "Epoch 4, loss = 0.0800, time: 17.1734\n",
      "reconstruction loss = 0.0269, similarity loss: 0.5580\n",
      "Epoch 5, loss = 0.0760, time: 17.2967\n",
      "reconstruction loss = 0.0238, similarity loss: 0.5458\n",
      "Epoch 6, loss = 0.0692, time: 17.3288\n",
      "reconstruction loss = 0.0230, similarity loss: 0.4855\n",
      "Epoch 7, loss = 0.0622, time: 17.2713\n",
      "reconstruction loss = 0.0242, similarity loss: 0.4047\n",
      "Epoch 8, loss = 0.0649, time: 17.2999\n",
      "reconstruction loss = 0.0232, similarity loss: 0.4400\n",
      "Epoch 9, loss = 0.0472, time: 17.2005\n",
      "reconstruction loss = 0.0213, similarity loss: 0.2810\n",
      "Epoch 10, loss = 0.0569, time: 17.2709\n",
      "reconstruction loss = 0.0211, similarity loss: 0.3796\n",
      "Epoch 11, loss = 0.0579, time: 17.3432\n",
      "reconstruction loss = 0.0226, similarity loss: 0.3759\n",
      "Epoch 12, loss = 0.0499, time: 17.2774\n",
      "reconstruction loss = 0.0177, similarity loss: 0.3399\n",
      "Epoch 13, loss = 0.0509, time: 17.3725\n",
      "reconstruction loss = 0.0198, similarity loss: 0.3313\n",
      "Epoch 14, loss = 0.0458, time: 17.2109\n",
      "reconstruction loss = 0.0183, similarity loss: 0.2932\n",
      "Epoch 15, loss = 0.0522, time: 17.9521\n",
      "reconstruction loss = 0.0193, similarity loss: 0.3481\n",
      "Epoch 16, loss = 0.0513, time: 18.0037\n",
      "reconstruction loss = 0.0204, similarity loss: 0.3289\n",
      "Epoch 17, loss = 0.0464, time: 18.3841\n",
      "reconstruction loss = 0.0168, similarity loss: 0.3119\n",
      "Epoch 18, loss = 0.0451, time: 18.1356\n",
      "reconstruction loss = 0.0168, similarity loss: 0.2997\n",
      "Epoch 19, loss = 0.0522, time: 18.1149\n",
      "reconstruction loss = 0.0187, similarity loss: 0.3544\n",
      "Epoch 20, loss = 0.0526, time: 18.3402\n",
      "reconstruction loss = 0.0190, similarity loss: 0.3546\n",
      "Epoch 21, loss = 0.0444, time: 17.9874\n",
      "reconstruction loss = 0.0161, similarity loss: 0.2986\n",
      "Epoch 22, loss = 0.0500, time: 18.3058\n",
      "reconstruction loss = 0.0176, similarity loss: 0.3417\n",
      "Epoch 23, loss = 0.0522, time: 18.1204\n",
      "reconstruction loss = 0.0182, similarity loss: 0.3587\n",
      "Epoch 24, loss = 0.0459, time: 18.0508\n",
      "reconstruction loss = 0.0173, similarity loss: 0.3036\n",
      "Epoch 25, loss = 0.0396, time: 18.1180\n",
      "reconstruction loss = 0.0149, similarity loss: 0.2621\n",
      "Epoch 26, loss = 0.0398, time: 18.1026\n",
      "reconstruction loss = 0.0158, similarity loss: 0.2561\n",
      "Epoch 27, loss = 0.0392, time: 17.9521\n",
      "reconstruction loss = 0.0154, similarity loss: 0.2533\n",
      "Epoch 28, loss = 0.0444, time: 18.0265\n",
      "reconstruction loss = 0.0194, similarity loss: 0.2689\n",
      "Epoch 29, loss = 0.0450, time: 18.0361\n",
      "reconstruction loss = 0.0145, similarity loss: 0.3194\n",
      "Epoch 30, loss = 0.0461, time: 18.2239\n",
      "reconstruction loss = 0.0157, similarity loss: 0.3203\n",
      "Epoch 31, loss = 0.0439, time: 18.0604\n",
      "reconstruction loss = 0.0161, similarity loss: 0.2941\n",
      "Epoch 32, loss = 0.0391, time: 18.0120\n",
      "reconstruction loss = 0.0158, similarity loss: 0.2494\n",
      "Epoch 33, loss = 0.0432, time: 18.0322\n",
      "reconstruction loss = 0.0151, similarity loss: 0.2961\n",
      "Epoch 34, loss = 0.0438, time: 18.1416\n",
      "reconstruction loss = 0.0153, similarity loss: 0.3003\n",
      "Epoch 35, loss = 0.0415, time: 18.1431\n",
      "reconstruction loss = 0.0156, similarity loss: 0.2748\n",
      "Epoch 36, loss = 0.0413, time: 18.1130\n",
      "reconstruction loss = 0.0140, similarity loss: 0.2868\n",
      "Epoch 37, loss = 0.0414, time: 18.1214\n",
      "reconstruction loss = 0.0170, similarity loss: 0.2614\n",
      "Epoch 38, loss = 0.0416, time: 18.0906\n",
      "reconstruction loss = 0.0124, similarity loss: 0.3044\n",
      "Epoch 39, loss = 0.0409, time: 17.9517\n",
      "reconstruction loss = 0.0147, similarity loss: 0.2763\n",
      "Epoch 40, loss = 0.0456, time: 18.1681\n",
      "reconstruction loss = 0.0137, similarity loss: 0.3325\n",
      "Epoch 41, loss = 0.0387, time: 18.2226\n",
      "reconstruction loss = 0.0132, similarity loss: 0.2687\n",
      "Epoch 42, loss = 0.0408, time: 18.1295\n",
      "reconstruction loss = 0.0149, similarity loss: 0.2740\n",
      "Epoch 43, loss = 0.0388, time: 18.2785\n",
      "reconstruction loss = 0.0131, similarity loss: 0.2702\n",
      "Epoch 44, loss = 0.0381, time: 18.0288\n",
      "reconstruction loss = 0.0134, similarity loss: 0.2602\n",
      "Epoch 45, loss = 0.0403, time: 18.0117\n",
      "reconstruction loss = 0.0151, similarity loss: 0.2673\n",
      "Epoch 46, loss = 0.0385, time: 18.0492\n",
      "reconstruction loss = 0.0128, similarity loss: 0.2699\n",
      "Epoch 47, loss = 0.0396, time: 18.0775\n",
      "reconstruction loss = 0.0137, similarity loss: 0.2721\n",
      "Epoch 48, loss = 0.0446, time: 17.5277\n",
      "reconstruction loss = 0.0151, similarity loss: 0.3105\n",
      "Epoch 49, loss = 0.0383, time: 17.0839\n",
      "reconstruction loss = 0.0129, similarity loss: 0.2666\n",
      "Epoch 50, loss = 0.0365, time: 17.5330\n",
      "reconstruction loss = 0.0123, similarity loss: 0.2541\n",
      "Epoch 51, loss = 0.0395, time: 17.1198\n",
      "reconstruction loss = 0.0148, similarity loss: 0.2615\n",
      "Epoch 52, loss = 0.0379, time: 16.6595\n",
      "reconstruction loss = 0.0118, similarity loss: 0.2724\n",
      "Epoch 53, loss = 0.0405, time: 16.4591\n",
      "reconstruction loss = 0.0150, similarity loss: 0.2698\n",
      "Epoch 54, loss = 0.0358, time: 16.4824\n",
      "reconstruction loss = 0.0134, similarity loss: 0.2373\n",
      "Epoch 55, loss = 0.0368, time: 16.6390\n",
      "reconstruction loss = 0.0119, similarity loss: 0.2608\n",
      "Epoch 56, loss = 0.0392, time: 16.6566\n",
      "reconstruction loss = 0.0128, similarity loss: 0.2772\n",
      "Epoch 57, loss = 0.0416, time: 16.5309\n",
      "reconstruction loss = 0.0124, similarity loss: 0.3043\n",
      "Epoch 58, loss = 0.0374, time: 16.6587\n",
      "reconstruction loss = 0.0120, similarity loss: 0.2652\n",
      "Epoch 59, loss = 0.0332, time: 16.5817\n",
      "reconstruction loss = 0.0122, similarity loss: 0.2228\n",
      "Epoch 60, loss = 0.0410, time: 16.6852\n",
      "reconstruction loss = 0.0144, similarity loss: 0.2798\n",
      "Epoch 61, loss = 0.0408, time: 16.8946\n",
      "reconstruction loss = 0.0131, similarity loss: 0.2898\n",
      "Epoch 62, loss = 0.0365, time: 16.4795\n",
      "reconstruction loss = 0.0122, similarity loss: 0.2550\n",
      "Epoch 63, loss = 0.0362, time: 16.6896\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2599\n",
      "Epoch 64, loss = 0.0351, time: 16.5711\n",
      "reconstruction loss = 0.0138, similarity loss: 0.2273\n",
      "Epoch 65, loss = 0.0374, time: 16.6457\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2765\n",
      "Epoch 66, loss = 0.0383, time: 16.5716\n",
      "reconstruction loss = 0.0112, similarity loss: 0.2829\n",
      "Epoch 67, loss = 0.0375, time: 16.6716\n",
      "reconstruction loss = 0.0128, similarity loss: 0.2598\n",
      "Epoch 68, loss = 0.0358, time: 16.7074\n",
      "reconstruction loss = 0.0103, similarity loss: 0.2653\n",
      "Epoch 69, loss = 0.0348, time: 16.6029\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2479\n",
      "Epoch 70, loss = 0.0332, time: 16.5869\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2327\n",
      "Epoch 71, loss = 0.0346, time: 16.5491\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2549\n",
      "Epoch 72, loss = 0.0336, time: 16.6988\n",
      "reconstruction loss = 0.0121, similarity loss: 0.2266\n",
      "Epoch 73, loss = 0.0340, time: 16.6976\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2406\n",
      "Epoch 74, loss = 0.0352, time: 16.6179\n",
      "reconstruction loss = 0.0106, similarity loss: 0.2570\n",
      "Epoch 75, loss = 0.0363, time: 16.6468\n",
      "reconstruction loss = 0.0110, similarity loss: 0.2647\n",
      "Epoch 76, loss = 0.0390, time: 17.2860\n",
      "reconstruction loss = 0.0125, similarity loss: 0.2776\n",
      "Epoch 77, loss = 0.0365, time: 17.4247\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2631\n",
      "Epoch 78, loss = 0.0384, time: 17.6116\n",
      "reconstruction loss = 0.0114, similarity loss: 0.2815\n",
      "Epoch 79, loss = 0.0346, time: 17.4160\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2482\n",
      "Epoch 80, loss = 0.0358, time: 17.4791\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2563\n",
      "Epoch 81, loss = 0.0369, time: 17.3572\n",
      "reconstruction loss = 0.0103, similarity loss: 0.2768\n",
      "Epoch 82, loss = 0.0363, time: 17.4113\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2723\n",
      "Epoch 83, loss = 0.0319, time: 17.5431\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2219\n",
      "Epoch 84, loss = 0.0333, time: 17.3778\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2329\n",
      "Epoch 85, loss = 0.0357, time: 17.4548\n",
      "reconstruction loss = 0.0105, similarity loss: 0.2621\n",
      "Epoch 86, loss = 0.0362, time: 17.3646\n",
      "reconstruction loss = 0.0117, similarity loss: 0.2566\n",
      "Epoch 87, loss = 0.0341, time: 17.4449\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2509\n",
      "Epoch 88, loss = 0.0376, time: 17.4960\n",
      "reconstruction loss = 0.0117, similarity loss: 0.2704\n",
      "Epoch 89, loss = 0.0391, time: 17.3812\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2967\n",
      "Epoch 90, loss = 0.0317, time: 17.3233\n",
      "reconstruction loss = 0.0109, similarity loss: 0.2185\n",
      "Epoch 91, loss = 0.0327, time: 17.3891\n",
      "reconstruction loss = 0.0096, similarity loss: 0.2407\n",
      "Epoch 92, loss = 0.0331, time: 17.4241\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2496\n",
      "Epoch 93, loss = 0.0353, time: 17.3331\n",
      "reconstruction loss = 0.0113, similarity loss: 0.2517\n",
      "Epoch 94, loss = 0.0363, time: 17.4642\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2654\n",
      "Epoch 95, loss = 0.0365, time: 17.3336\n",
      "reconstruction loss = 0.0112, similarity loss: 0.2646\n",
      "Epoch 96, loss = 0.0363, time: 17.4265\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2839\n",
      "Epoch 97, loss = 0.0378, time: 17.5367\n",
      "reconstruction loss = 0.0125, similarity loss: 0.2646\n",
      "Epoch 98, loss = 0.0379, time: 17.2806\n",
      "reconstruction loss = 0.0123, similarity loss: 0.2679\n",
      "Epoch 99, loss = 0.0355, time: 17.3793\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2614\n",
      "Epoch 100, loss = 0.0336, time: 17.4626\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2479\n",
      "Epoch 101, loss = 0.0366, time: 17.4423\n",
      "reconstruction loss = 0.0114, similarity loss: 0.2629\n",
      "Epoch 102, loss = 0.0368, time: 17.2596\n",
      "reconstruction loss = 0.0096, similarity loss: 0.2814\n",
      "Epoch 103, loss = 0.0298, time: 17.4645\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2077\n",
      "Epoch 104, loss = 0.0359, time: 17.4385\n",
      "reconstruction loss = 0.0104, similarity loss: 0.2652\n",
      "Epoch 105, loss = 0.0307, time: 17.3594\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2268\n",
      "Epoch 106, loss = 0.0387, time: 17.4102\n",
      "reconstruction loss = 0.0116, similarity loss: 0.2820\n",
      "Epoch 107, loss = 0.0340, time: 17.3182\n",
      "reconstruction loss = 0.0101, similarity loss: 0.2493\n",
      "Epoch 108, loss = 0.0364, time: 17.3957\n",
      "reconstruction loss = 0.0103, similarity loss: 0.2706\n",
      "Epoch 109, loss = 0.0352, time: 17.1881\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2644\n",
      "Epoch 110, loss = 0.0307, time: 16.9895\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2260\n",
      "Epoch 111, loss = 0.0356, time: 16.7004\n",
      "reconstruction loss = 0.0106, similarity loss: 0.2606\n",
      "Epoch 112, loss = 0.0318, time: 16.7521\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2301\n",
      "Epoch 113, loss = 0.0321, time: 16.6038\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2328\n",
      "Epoch 114, loss = 0.0367, time: 16.8048\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2847\n",
      "Epoch 115, loss = 0.0338, time: 16.7864\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2492\n",
      "Epoch 116, loss = 0.0353, time: 16.7263\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2530\n",
      "Epoch 117, loss = 0.0349, time: 16.6410\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2685\n",
      "Epoch 118, loss = 0.0316, time: 16.6362\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2339\n",
      "Epoch 119, loss = 0.0338, time: 16.5844\n",
      "reconstruction loss = 0.0099, similarity loss: 0.2488\n",
      "Epoch 120, loss = 0.0309, time: 16.6215\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2282\n",
      "Epoch 121, loss = 0.0311, time: 16.9131\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2232\n",
      "Epoch 122, loss = 0.0304, time: 16.5803\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2190\n",
      "Epoch 123, loss = 0.0338, time: 16.6033\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2505\n",
      "Epoch 124, loss = 0.0327, time: 16.5433\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2494\n",
      "Epoch 125, loss = 0.0335, time: 16.8794\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2552\n",
      "Epoch 126, loss = 0.0361, time: 16.5479\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2740\n",
      "Epoch 127, loss = 0.0292, time: 16.5326\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2143\n",
      "Epoch 128, loss = 0.0334, time: 16.9503\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2546\n",
      "Epoch 129, loss = 0.0304, time: 17.3932\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2217\n",
      "Epoch 130, loss = 0.0333, time: 17.3855\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2532\n",
      "Epoch 131, loss = 0.0334, time: 17.2986\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2557\n",
      "Epoch 132, loss = 0.0305, time: 17.3973\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2168\n",
      "Epoch 133, loss = 0.0309, time: 17.3678\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2349\n",
      "Epoch 134, loss = 0.0320, time: 17.4693\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2429\n",
      "Epoch 135, loss = 0.0344, time: 17.3457\n",
      "reconstruction loss = 0.0094, similarity loss: 0.2601\n",
      "Epoch 136, loss = 0.0341, time: 17.4679\n",
      "reconstruction loss = 0.0111, similarity loss: 0.2406\n",
      "Epoch 137, loss = 0.0304, time: 17.4353\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2292\n",
      "Epoch 138, loss = 0.0323, time: 17.3239\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2528\n",
      "Epoch 139, loss = 0.0340, time: 17.2936\n",
      "reconstruction loss = 0.0108, similarity loss: 0.2423\n",
      "Epoch 140, loss = 0.0302, time: 17.3779\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2282\n",
      "Epoch 141, loss = 0.0334, time: 17.3782\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2536\n",
      "Epoch 142, loss = 0.0320, time: 17.2886\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2425\n",
      "Epoch 143, loss = 0.0297, time: 17.4882\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2243\n",
      "Epoch 144, loss = 0.0325, time: 17.4995\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2481\n",
      "Epoch 145, loss = 0.0321, time: 17.3653\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2431\n",
      "Epoch 146, loss = 0.0398, time: 17.4919\n",
      "reconstruction loss = 0.0155, similarity loss: 0.2590\n",
      "Epoch 147, loss = 0.0345, time: 17.3733\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2635\n",
      "Epoch 148, loss = 0.0293, time: 17.3094\n",
      "reconstruction loss = 0.0086, similarity loss: 0.2162\n",
      "Epoch 149, loss = 0.0323, time: 17.4321\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2489\n",
      "Epoch 150, loss = 0.0313, time: 17.5468\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2306\n",
      "Epoch 151, loss = 0.0301, time: 17.3356\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2342\n",
      "Epoch 152, loss = 0.0295, time: 17.3731\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2326\n",
      "Epoch 153, loss = 0.0352, time: 17.3968\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2752\n",
      "Epoch 154, loss = 0.0322, time: 17.2314\n",
      "reconstruction loss = 0.0105, similarity loss: 0.2273\n",
      "Epoch 155, loss = 0.0320, time: 17.4089\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2474\n",
      "Epoch 156, loss = 0.0341, time: 17.3849\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2693\n",
      "Epoch 157, loss = 0.0352, time: 17.3712\n",
      "reconstruction loss = 0.0102, similarity loss: 0.2598\n",
      "Epoch 158, loss = 0.0302, time: 17.3681\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2296\n",
      "Epoch 159, loss = 0.0301, time: 17.3715\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2130\n",
      "Epoch 160, loss = 0.0309, time: 17.3279\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2281\n",
      "Epoch 161, loss = 0.0313, time: 17.3450\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2394\n",
      "Epoch 162, loss = 0.0301, time: 17.3651\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2305\n",
      "Epoch 163, loss = 0.0320, time: 16.7946\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2626\n",
      "Epoch 164, loss = 0.0338, time: 16.7346\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2567\n",
      "Epoch 165, loss = 0.0287, time: 16.6200\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2267\n",
      "Epoch 166, loss = 0.0287, time: 16.6585\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2154\n",
      "Epoch 167, loss = 0.0341, time: 16.6271\n",
      "reconstruction loss = 0.0097, similarity loss: 0.2541\n",
      "Epoch 168, loss = 0.0301, time: 16.4872\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2277\n",
      "Epoch 169, loss = 0.0306, time: 16.6203\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2392\n",
      "Epoch 170, loss = 0.0293, time: 16.5316\n",
      "reconstruction loss = 0.0075, similarity loss: 0.2249\n",
      "Epoch 171, loss = 0.0304, time: 16.5961\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2295\n",
      "Epoch 172, loss = 0.0295, time: 16.6492\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2223\n",
      "Epoch 173, loss = 0.0291, time: 16.5339\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2198\n",
      "Epoch 174, loss = 0.0343, time: 16.7364\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2687\n",
      "Epoch 175, loss = 0.0278, time: 16.4962\n",
      "reconstruction loss = 0.0079, similarity loss: 0.2068\n",
      "Epoch 176, loss = 0.0326, time: 16.5198\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2631\n",
      "Epoch 177, loss = 0.0269, time: 16.5573\n",
      "reconstruction loss = 0.0079, similarity loss: 0.1978\n",
      "Epoch 178, loss = 0.0334, time: 16.8001\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2555\n",
      "Epoch 179, loss = 0.0290, time: 16.4700\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2205\n",
      "Epoch 180, loss = 0.0293, time: 16.9421\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2206\n",
      "Epoch 181, loss = 0.0281, time: 17.3538\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2219\n",
      "Epoch 182, loss = 0.0302, time: 17.2376\n",
      "reconstruction loss = 0.0091, similarity loss: 0.2200\n",
      "Epoch 183, loss = 0.0318, time: 17.2650\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2537\n",
      "Epoch 184, loss = 0.0318, time: 17.4557\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2559\n",
      "Epoch 185, loss = 0.0323, time: 17.3802\n",
      "reconstruction loss = 0.0090, similarity loss: 0.2421\n",
      "Epoch 186, loss = 0.0300, time: 17.3141\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2373\n",
      "Epoch 187, loss = 0.0319, time: 17.4157\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2495\n",
      "Epoch 188, loss = 0.0293, time: 17.4560\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2240\n",
      "Epoch 189, loss = 0.0293, time: 17.3462\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2286\n",
      "Epoch 190, loss = 0.0305, time: 17.3003\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2397\n",
      "Epoch 191, loss = 0.0321, time: 17.4046\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2522\n",
      "Epoch 192, loss = 0.0313, time: 17.2999\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2500\n",
      "Epoch 193, loss = 0.0261, time: 17.1424\n",
      "reconstruction loss = 0.0071, similarity loss: 0.1965\n",
      "Epoch 194, loss = 0.0311, time: 17.3871\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2344\n",
      "Epoch 195, loss = 0.0314, time: 17.3679\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2503\n",
      "Epoch 196, loss = 0.0324, time: 17.2802\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2474\n",
      "Epoch 197, loss = 0.0301, time: 17.3828\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2258\n",
      "Epoch 198, loss = 0.0321, time: 17.2698\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2440\n",
      "Epoch 199, loss = 0.0315, time: 17.5424\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2424\n",
      "Epoch 200, loss = 0.0265, time: 17.3349\n",
      "reconstruction loss = 0.0077, similarity loss: 0.1956\n",
      "Epoch 201, loss = 0.0302, time: 17.2424\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2295\n",
      "Epoch 202, loss = 0.0281, time: 17.3117\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2171\n",
      "Epoch 203, loss = 0.0288, time: 17.2735\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2242\n",
      "Epoch 204, loss = 0.0292, time: 17.3705\n",
      "reconstruction loss = 0.0081, similarity loss: 0.2187\n",
      "Epoch 205, loss = 0.0320, time: 17.3387\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2623\n",
      "Epoch 206, loss = 0.0276, time: 17.4100\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2146\n",
      "Epoch 207, loss = 0.0306, time: 17.2457\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2394\n",
      "Epoch 208, loss = 0.0307, time: 17.3170\n",
      "reconstruction loss = 0.0082, similarity loss: 0.2338\n",
      "Epoch 209, loss = 0.0299, time: 17.4939\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2346\n",
      "Epoch 210, loss = 0.0280, time: 17.2782\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2246\n",
      "Epoch 211, loss = 0.0303, time: 17.2797\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2364\n",
      "Epoch 212, loss = 0.0280, time: 17.3698\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2203\n",
      "Epoch 213, loss = 0.0296, time: 17.2978\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2271\n",
      "Epoch 214, loss = 0.0271, time: 17.3452\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2121\n",
      "Epoch 215, loss = 0.0310, time: 16.6196\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2437\n",
      "Epoch 216, loss = 0.0290, time: 16.7370\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2190\n",
      "Epoch 217, loss = 0.0292, time: 16.6848\n",
      "reconstruction loss = 0.0083, similarity loss: 0.2179\n",
      "Epoch 218, loss = 0.0317, time: 16.6237\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2536\n",
      "Epoch 219, loss = 0.0296, time: 16.8479\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2248\n",
      "Epoch 220, loss = 0.0314, time: 16.7067\n",
      "reconstruction loss = 0.0085, similarity loss: 0.2369\n",
      "Epoch 221, loss = 0.0289, time: 16.6864\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2323\n",
      "Epoch 222, loss = 0.0288, time: 16.6257\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2315\n",
      "Epoch 223, loss = 0.0330, time: 16.9052\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2506\n",
      "Epoch 224, loss = 0.0302, time: 17.5692\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2367\n",
      "Epoch 225, loss = 0.0285, time: 17.3158\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2202\n",
      "Epoch 226, loss = 0.0299, time: 17.3713\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2379\n",
      "Epoch 227, loss = 0.0318, time: 17.6614\n",
      "reconstruction loss = 0.0098, similarity loss: 0.2295\n",
      "Epoch 228, loss = 0.0307, time: 17.4656\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2370\n",
      "Epoch 229, loss = 0.0256, time: 17.2192\n",
      "reconstruction loss = 0.0074, similarity loss: 0.1887\n",
      "Epoch 230, loss = 0.0290, time: 17.3781\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2343\n",
      "Epoch 231, loss = 0.0316, time: 17.5282\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2298\n",
      "Epoch 232, loss = 0.0258, time: 17.3311\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2000\n",
      "Epoch 233, loss = 0.0342, time: 17.3961\n",
      "reconstruction loss = 0.0080, similarity loss: 0.2703\n",
      "Epoch 234, loss = 0.0251, time: 17.3472\n",
      "reconstruction loss = 0.0063, similarity loss: 0.1943\n",
      "Epoch 235, loss = 0.0290, time: 17.3164\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2323\n",
      "Epoch 236, loss = 0.0318, time: 17.3570\n",
      "reconstruction loss = 0.0072, similarity loss: 0.2539\n",
      "Epoch 237, loss = 0.0290, time: 17.4427\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2336\n",
      "Epoch 238, loss = 0.0262, time: 17.4079\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2068\n",
      "Epoch 239, loss = 0.0299, time: 17.3827\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2395\n",
      "Epoch 240, loss = 0.0351, time: 17.4095\n",
      "reconstruction loss = 0.0120, similarity loss: 0.2434\n",
      "Epoch 241, loss = 0.0277, time: 17.6566\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2197\n",
      "Epoch 242, loss = 0.0325, time: 17.4234\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2631\n",
      "Epoch 243, loss = 0.0292, time: 17.7347\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2392\n",
      "Epoch 244, loss = 0.0272, time: 17.4240\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2097\n",
      "Epoch 245, loss = 0.0273, time: 17.2765\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2193\n",
      "Epoch 246, loss = 0.0291, time: 17.2649\n",
      "reconstruction loss = 0.0084, similarity loss: 0.2158\n",
      "Epoch 247, loss = 0.0283, time: 17.3297\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2144\n",
      "Epoch 248, loss = 0.0304, time: 17.3350\n",
      "reconstruction loss = 0.0089, similarity loss: 0.2239\n",
      "Epoch 249, loss = 0.0285, time: 17.2760\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2296\n",
      "Epoch 250, loss = 0.0288, time: 17.2437\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2271\n",
      "Epoch 251, loss = 0.0299, time: 17.3190\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2133\n",
      "Epoch 252, loss = 0.0286, time: 17.4225\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2242\n",
      "Epoch 253, loss = 0.0282, time: 17.2510\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2136\n",
      "Epoch 254, loss = 0.0320, time: 17.3213\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2587\n",
      "Epoch 255, loss = 0.0292, time: 17.4434\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2365\n",
      "Epoch 256, loss = 0.0283, time: 17.4185\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2269\n",
      "Epoch 257, loss = 0.0292, time: 17.2502\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2268\n",
      "Epoch 258, loss = 0.0288, time: 16.7219\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2378\n",
      "Epoch 259, loss = 0.0299, time: 16.7466\n",
      "reconstruction loss = 0.0074, similarity loss: 0.2330\n",
      "Epoch 260, loss = 0.0324, time: 16.6816\n",
      "reconstruction loss = 0.0059, similarity loss: 0.2707\n",
      "Epoch 261, loss = 0.0331, time: 16.6434\n",
      "reconstruction loss = 0.0095, similarity loss: 0.2450\n",
      "Epoch 262, loss = 0.0295, time: 16.7139\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2381\n",
      "Epoch 263, loss = 0.0307, time: 16.6944\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2406\n",
      "Epoch 264, loss = 0.0286, time: 16.7175\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2271\n",
      "Epoch 265, loss = 0.0298, time: 16.6227\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2290\n",
      "Epoch 266, loss = 0.0260, time: 16.6456\n",
      "reconstruction loss = 0.0062, similarity loss: 0.2042\n",
      "Epoch 267, loss = 0.0307, time: 16.6722\n",
      "reconstruction loss = 0.0064, similarity loss: 0.2492\n",
      "Epoch 268, loss = 0.0297, time: 16.6164\n",
      "reconstruction loss = 0.0070, similarity loss: 0.2334\n",
      "Epoch 269, loss = 0.0287, time: 16.5471\n",
      "reconstruction loss = 0.0068, similarity loss: 0.2255\n",
      "Epoch 270, loss = 0.0300, time: 16.5241\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2391\n",
      "Epoch 271, loss = 0.0248, time: 16.4199\n",
      "reconstruction loss = 0.0069, similarity loss: 0.1862\n",
      "Epoch 272, loss = 0.0247, time: 16.7427\n",
      "reconstruction loss = 0.0062, similarity loss: 0.1916\n",
      "Epoch 273, loss = 0.0268, time: 16.6893\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2081\n",
      "Epoch 274, loss = 0.0310, time: 16.6488\n",
      "reconstruction loss = 0.0078, similarity loss: 0.2401\n",
      "Epoch 275, loss = 0.0281, time: 16.6698\n",
      "reconstruction loss = 0.0066, similarity loss: 0.2214\n",
      "Epoch 276, loss = 0.0306, time: 16.6527\n",
      "reconstruction loss = 0.0076, similarity loss: 0.2373\n",
      "Epoch 277, loss = 0.0300, time: 16.5636\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2479\n",
      "Epoch 278, loss = 0.0284, time: 16.5474\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2204\n",
      "Epoch 279, loss = 0.0263, time: 16.6525\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2027\n",
      "Epoch 280, loss = 0.0281, time: 16.7467\n",
      "reconstruction loss = 0.0056, similarity loss: 0.2308\n",
      "Epoch 281, loss = 0.0280, time: 16.4748\n",
      "reconstruction loss = 0.0061, similarity loss: 0.2251\n",
      "Epoch 282, loss = 0.0309, time: 16.6024\n",
      "reconstruction loss = 0.0075, similarity loss: 0.2412\n",
      "Epoch 283, loss = 0.0305, time: 16.5565\n",
      "reconstruction loss = 0.0073, similarity loss: 0.2391\n",
      "Epoch 284, loss = 0.0312, time: 16.7036\n",
      "reconstruction loss = 0.0077, similarity loss: 0.2424\n",
      "Epoch 285, loss = 0.0271, time: 16.6516\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2124\n",
      "Epoch 286, loss = 0.0238, time: 17.2387\n",
      "reconstruction loss = 0.0057, similarity loss: 0.1866\n",
      "Epoch 287, loss = 0.0276, time: 17.4228\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2139\n",
      "Epoch 288, loss = 0.0268, time: 17.3107\n",
      "reconstruction loss = 0.0055, similarity loss: 0.2180\n",
      "Epoch 289, loss = 0.0277, time: 17.3074\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2200\n",
      "Epoch 290, loss = 0.0324, time: 17.9407\n",
      "reconstruction loss = 0.0065, similarity loss: 0.2656\n",
      "Epoch 291, loss = 0.0287, time: 17.3461\n",
      "reconstruction loss = 0.0067, similarity loss: 0.2262\n",
      "Epoch 292, loss = 0.0284, time: 17.4985\n",
      "reconstruction loss = 0.0060, similarity loss: 0.2301\n",
      "Epoch 293, loss = 0.0299, time: 17.5513\n",
      "reconstruction loss = 0.0087, similarity loss: 0.2211\n",
      "Epoch 294, loss = 0.0288, time: 17.5170\n",
      "reconstruction loss = 0.0088, similarity loss: 0.2084\n",
      "Epoch 295, loss = 0.0261, time: 17.2869\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2037\n",
      "Epoch 296, loss = 0.0278, time: 17.4146\n",
      "reconstruction loss = 0.0063, similarity loss: 0.2218\n",
      "Epoch 297, loss = 0.0331, time: 17.2304\n",
      "reconstruction loss = 0.0071, similarity loss: 0.2665\n",
      "Epoch 298, loss = 0.0289, time: 17.3317\n",
      "reconstruction loss = 0.0058, similarity loss: 0.2362\n",
      "Epoch 299, loss = 0.0301, time: 17.2119\n",
      "reconstruction loss = 0.0069, similarity loss: 0.2398\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.4009, val.acc = 0.6234\n",
      "Epoch 1, loss = 1.0562, val.acc = 0.6482\n",
      "Epoch 2, loss = 0.9524, val.acc = 0.6568\n",
      "Epoch 3, loss = 0.8814, val.acc = 0.6636\n",
      "Epoch 4, loss = 0.8267, val.acc = 0.6652\n",
      "Epoch 5, loss = 0.7823, val.acc = 0.6686\n",
      "Epoch 6, loss = 0.7451, val.acc = 0.6680\n",
      "Epoch 7, loss = 0.7131, val.acc = 0.6688\n",
      "Epoch 8, loss = 0.6853, val.acc = 0.6700\n",
      "Epoch 9, loss = 0.6609, val.acc = 0.6688\n",
      "Epoch 10, loss = 0.6395, val.acc = 0.6706\n",
      "Epoch 11, loss = 0.6209, val.acc = 0.6672\n",
      "Epoch 12, loss = 0.6050, val.acc = 0.6674\n",
      "Epoch 13, loss = 0.5920, val.acc = 0.6620\n",
      "Epoch 14, loss = 0.5811, val.acc = 0.6614\n",
      "Epoch 15, loss = 0.5693, val.acc = 0.6620\n",
      "Epoch 16, loss = 0.5552, val.acc = 0.6600\n",
      "Epoch 17, loss = 0.5415, val.acc = 0.6584\n",
      "Epoch 18, loss = 0.5262, val.acc = 0.6612\n",
      "Epoch 19, loss = 0.5116, val.acc = 0.6648\n",
      "Epoch 20, loss = 0.5005, val.acc = 0.6588\n",
      "Epoch 21, loss = 0.4920, val.acc = 0.6568\n",
      "Epoch 22, loss = 0.4850, val.acc = 0.6552\n",
      "Epoch 23, loss = 0.4784, val.acc = 0.6504\n",
      "Epoch 24, loss = 0.4725, val.acc = 0.6474\n",
      "Epoch 25, loss = 0.4683, val.acc = 0.6430\n",
      "Epoch 26, loss = 0.4667, val.acc = 0.6402\n",
      "Epoch 27, loss = 0.4671, val.acc = 0.6310\n",
      "Epoch 28, loss = 0.4657, val.acc = 0.6260\n",
      "Epoch 29, loss = 0.4579, val.acc = 0.6242\n",
      "Epoch 30, loss = 0.4462, val.acc = 0.6290\n",
      "Epoch 31, loss = 0.4375, val.acc = 0.6316\n",
      "Epoch 32, loss = 0.4288, val.acc = 0.6310\n",
      "Epoch 33, loss = 0.4185, val.acc = 0.6246\n",
      "Epoch 34, loss = 0.4108, val.acc = 0.6258\n",
      "Epoch 35, loss = 0.4054, val.acc = 0.6292\n",
      "Epoch 36, loss = 0.4010, val.acc = 0.6296\n",
      "Epoch 37, loss = 0.3963, val.acc = 0.6352\n",
      "Epoch 38, loss = 0.3910, val.acc = 0.6366\n",
      "Epoch 39, loss = 0.3849, val.acc = 0.6400\n",
      "Epoch 40, loss = 0.3766, val.acc = 0.6428\n",
      "Epoch 41, loss = 0.3685, val.acc = 0.6462\n",
      "Epoch 42, loss = 0.3618, val.acc = 0.6516\n",
      "Epoch 43, loss = 0.3565, val.acc = 0.6490\n",
      "Epoch 44, loss = 0.3524, val.acc = 0.6466\n",
      "Epoch 45, loss = 0.3496, val.acc = 0.6458\n",
      "Epoch 46, loss = 0.3478, val.acc = 0.6446\n",
      "Epoch 47, loss = 0.3470, val.acc = 0.6442\n",
      "Epoch 48, loss = 0.3469, val.acc = 0.6434\n",
      "Epoch 49, loss = 0.3464, val.acc = 0.6342\n",
      "Epoch 50, loss = 0.3453, val.acc = 0.6318\n",
      "Epoch 51, loss = 0.3434, val.acc = 0.6284\n",
      "Epoch 52, loss = 0.3414, val.acc = 0.6308\n",
      "Epoch 53, loss = 0.3406, val.acc = 0.6326\n",
      "Epoch 54, loss = 0.3415, val.acc = 0.6336\n",
      "Epoch 55, loss = 0.3435, val.acc = 0.6292\n",
      "Epoch 56, loss = 0.3468, val.acc = 0.6236\n",
      "Epoch 57, loss = 0.3522, val.acc = 0.6200\n",
      "Epoch 58, loss = 0.3535, val.acc = 0.6202\n",
      "Epoch 59, loss = 0.3440, val.acc = 0.6192\n",
      "Epoch 60, loss = 0.3354, val.acc = 0.6248\n",
      "Epoch 61, loss = 0.3292, val.acc = 0.6308\n",
      "Epoch 62, loss = 0.3259, val.acc = 0.6298\n",
      "Epoch 63, loss = 0.3260, val.acc = 0.6236\n",
      "Epoch 64, loss = 0.3266, val.acc = 0.6226\n",
      "Epoch 65, loss = 0.3232, val.acc = 0.6298\n",
      "Epoch 66, loss = 0.3218, val.acc = 0.6272\n",
      "Epoch 67, loss = 0.3202, val.acc = 0.6220\n",
      "Epoch 68, loss = 0.3181, val.acc = 0.6152\n",
      "Epoch 69, loss = 0.3151, val.acc = 0.6088\n",
      "Epoch 70, loss = 0.3100, val.acc = 0.6092\n",
      "Epoch 71, loss = 0.3025, val.acc = 0.6112\n",
      "Epoch 72, loss = 0.2944, val.acc = 0.6214\n",
      "Epoch 73, loss = 0.2873, val.acc = 0.6290\n",
      "Epoch 74, loss = 0.2815, val.acc = 0.6292\n",
      "Epoch 75, loss = 0.2772, val.acc = 0.6246\n",
      "Epoch 76, loss = 0.2746, val.acc = 0.6222\n",
      "Epoch 77, loss = 0.2735, val.acc = 0.6200\n",
      "Epoch 78, loss = 0.2740, val.acc = 0.6164\n",
      "Epoch 79, loss = 0.2760, val.acc = 0.6108\n",
      "Epoch 80, loss = 0.2791, val.acc = 0.6106\n",
      "Epoch 81, loss = 0.2827, val.acc = 0.6150\n",
      "Epoch 82, loss = 0.2850, val.acc = 0.6260\n",
      "Epoch 83, loss = 0.2834, val.acc = 0.6268\n",
      "Epoch 84, loss = 0.2802, val.acc = 0.6262\n",
      "Epoch 85, loss = 0.2779, val.acc = 0.6272\n",
      "Epoch 86, loss = 0.2786, val.acc = 0.6224\n",
      "Epoch 87, loss = 0.2843, val.acc = 0.6210\n",
      "Epoch 88, loss = 0.2900, val.acc = 0.6238\n",
      "Epoch 89, loss = 0.2790, val.acc = 0.6244\n",
      "Epoch 90, loss = 0.2664, val.acc = 0.6280\n",
      "Epoch 91, loss = 0.2534, val.acc = 0.6266\n",
      "Epoch 92, loss = 0.2413, val.acc = 0.6222\n",
      "Epoch 93, loss = 0.2311, val.acc = 0.6178\n",
      "Epoch 94, loss = 0.2233, val.acc = 0.6196\n",
      "Epoch 95, loss = 0.2174, val.acc = 0.6190\n",
      "Epoch 96, loss = 0.2130, val.acc = 0.6190\n",
      "Epoch 97, loss = 0.2095, val.acc = 0.6184\n",
      "Epoch 98, loss = 0.2066, val.acc = 0.6170\n",
      "Epoch 99, loss = 0.2041, val.acc = 0.6138\n",
      "Rep: 1, te.acc = 0.6092\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6092]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_0_9')\n",
    "train_unsupervised_ae(pars,vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 1\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 1 # proportion of the reconstruction loss\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_300_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.0399, time: 17.8459\n",
      "reconstruction loss = 0.0399, similarity loss: 0.8950\n",
      "Epoch 1, loss = 0.0187, time: 17.3239\n",
      "reconstruction loss = 0.0187, similarity loss: 0.9881\n",
      "Epoch 2, loss = 0.0141, time: 17.3449\n",
      "reconstruction loss = 0.0141, similarity loss: 1.1286\n",
      "Epoch 3, loss = 0.0120, time: 17.5210\n",
      "reconstruction loss = 0.0120, similarity loss: 1.1333\n",
      "Epoch 4, loss = 0.0102, time: 17.2451\n",
      "reconstruction loss = 0.0102, similarity loss: 1.2009\n",
      "Epoch 5, loss = 0.0093, time: 17.0872\n",
      "reconstruction loss = 0.0093, similarity loss: 1.0367\n",
      "Epoch 6, loss = 0.0080, time: 17.5294\n",
      "reconstruction loss = 0.0080, similarity loss: 1.2226\n",
      "Epoch 7, loss = 0.0076, time: 17.3231\n",
      "reconstruction loss = 0.0076, similarity loss: 1.1003\n",
      "Epoch 8, loss = 0.0066, time: 17.2358\n",
      "reconstruction loss = 0.0066, similarity loss: 1.1119\n",
      "Epoch 9, loss = 0.0065, time: 17.3416\n",
      "reconstruction loss = 0.0065, similarity loss: 1.2006\n",
      "Epoch 10, loss = 0.0062, time: 17.2196\n",
      "reconstruction loss = 0.0062, similarity loss: 1.0657\n",
      "Epoch 11, loss = 0.0057, time: 16.7959\n",
      "reconstruction loss = 0.0057, similarity loss: 1.1471\n",
      "Epoch 12, loss = 0.0057, time: 16.6634\n",
      "reconstruction loss = 0.0057, similarity loss: 1.0895\n",
      "Epoch 13, loss = 0.0053, time: 16.4148\n",
      "reconstruction loss = 0.0053, similarity loss: 1.0082\n",
      "Epoch 14, loss = 0.0050, time: 16.6231\n",
      "reconstruction loss = 0.0050, similarity loss: 1.2168\n",
      "Epoch 15, loss = 0.0051, time: 16.5434\n",
      "reconstruction loss = 0.0051, similarity loss: 1.0768\n",
      "Epoch 16, loss = 0.0051, time: 16.6293\n",
      "reconstruction loss = 0.0051, similarity loss: 1.0583\n",
      "Epoch 17, loss = 0.0048, time: 16.6552\n",
      "reconstruction loss = 0.0048, similarity loss: 1.0411\n",
      "Epoch 18, loss = 0.0050, time: 16.7223\n",
      "reconstruction loss = 0.0050, similarity loss: 1.1647\n",
      "Epoch 19, loss = 0.0052, time: 16.6532\n",
      "reconstruction loss = 0.0052, similarity loss: 1.0009\n",
      "Epoch 20, loss = 0.0041, time: 16.6284\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0313\n",
      "Epoch 21, loss = 0.0042, time: 16.6654\n",
      "reconstruction loss = 0.0042, similarity loss: 1.0516\n",
      "Epoch 22, loss = 0.0046, time: 16.6945\n",
      "reconstruction loss = 0.0046, similarity loss: 1.0168\n",
      "Epoch 23, loss = 0.0044, time: 16.7376\n",
      "reconstruction loss = 0.0044, similarity loss: 1.0275\n",
      "Epoch 24, loss = 0.0047, time: 16.7535\n",
      "reconstruction loss = 0.0047, similarity loss: 1.0687\n",
      "Epoch 25, loss = 0.0045, time: 16.6364\n",
      "reconstruction loss = 0.0045, similarity loss: 1.0267\n",
      "Epoch 26, loss = 0.0043, time: 16.5587\n",
      "reconstruction loss = 0.0043, similarity loss: 1.0521\n",
      "Epoch 27, loss = 0.0040, time: 16.6575\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0460\n",
      "Epoch 28, loss = 0.0044, time: 16.7161\n",
      "reconstruction loss = 0.0044, similarity loss: 1.1032\n",
      "Epoch 29, loss = 0.0042, time: 17.0776\n",
      "reconstruction loss = 0.0042, similarity loss: 1.2149\n",
      "Epoch 30, loss = 0.0041, time: 17.1628\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0341\n",
      "Epoch 31, loss = 0.0041, time: 17.4046\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0937\n",
      "Epoch 32, loss = 0.0042, time: 17.4747\n",
      "reconstruction loss = 0.0042, similarity loss: 1.0724\n",
      "Epoch 33, loss = 0.0043, time: 17.2743\n",
      "reconstruction loss = 0.0043, similarity loss: 1.0897\n",
      "Epoch 34, loss = 0.0040, time: 17.3299\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0710\n",
      "Epoch 35, loss = 0.0038, time: 17.1704\n",
      "reconstruction loss = 0.0038, similarity loss: 1.0087\n",
      "Epoch 36, loss = 0.0042, time: 17.3312\n",
      "reconstruction loss = 0.0042, similarity loss: 1.0689\n",
      "Epoch 37, loss = 0.0037, time: 17.3280\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0888\n",
      "Epoch 38, loss = 0.0041, time: 17.2970\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0693\n",
      "Epoch 39, loss = 0.0039, time: 17.3542\n",
      "reconstruction loss = 0.0039, similarity loss: 1.0976\n",
      "Epoch 40, loss = 0.0039, time: 17.4407\n",
      "reconstruction loss = 0.0039, similarity loss: 1.0889\n",
      "Epoch 41, loss = 0.0040, time: 17.3270\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1152\n",
      "Epoch 42, loss = 0.0036, time: 17.2782\n",
      "reconstruction loss = 0.0036, similarity loss: 1.0677\n",
      "Epoch 43, loss = 0.0041, time: 17.2752\n",
      "reconstruction loss = 0.0041, similarity loss: 1.1443\n",
      "Epoch 44, loss = 0.0040, time: 17.2888\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0659\n",
      "Epoch 45, loss = 0.0040, time: 17.3178\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1185\n",
      "Epoch 46, loss = 0.0039, time: 17.3647\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1176\n",
      "Epoch 47, loss = 0.0042, time: 17.1956\n",
      "reconstruction loss = 0.0042, similarity loss: 1.0737\n",
      "Epoch 48, loss = 0.0041, time: 17.2935\n",
      "reconstruction loss = 0.0041, similarity loss: 1.1108\n",
      "Epoch 49, loss = 0.0041, time: 17.3528\n",
      "reconstruction loss = 0.0041, similarity loss: 1.1110\n",
      "Epoch 50, loss = 0.0039, time: 17.4294\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1654\n",
      "Epoch 51, loss = 0.0041, time: 18.1352\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0749\n",
      "Epoch 52, loss = 0.0039, time: 17.7117\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1576\n",
      "Epoch 53, loss = 0.0043, time: 17.2837\n",
      "reconstruction loss = 0.0043, similarity loss: 1.0652\n",
      "Epoch 54, loss = 0.0039, time: 17.2007\n",
      "reconstruction loss = 0.0039, similarity loss: 1.0635\n",
      "Epoch 55, loss = 0.0041, time: 17.2686\n",
      "reconstruction loss = 0.0041, similarity loss: 1.0413\n",
      "Epoch 56, loss = 0.0039, time: 17.2500\n",
      "reconstruction loss = 0.0039, similarity loss: 1.0724\n",
      "Epoch 57, loss = 0.0037, time: 17.1104\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0534\n",
      "Epoch 58, loss = 0.0039, time: 17.2740\n",
      "reconstruction loss = 0.0039, similarity loss: 1.0880\n",
      "Epoch 59, loss = 0.0040, time: 17.4011\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0537\n",
      "Epoch 60, loss = 0.0037, time: 17.2225\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1119\n",
      "Epoch 61, loss = 0.0038, time: 17.2118\n",
      "reconstruction loss = 0.0038, similarity loss: 1.0817\n",
      "Epoch 62, loss = 0.0037, time: 17.3567\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0788\n",
      "Epoch 63, loss = 0.0043, time: 17.2122\n",
      "reconstruction loss = 0.0043, similarity loss: 1.1259\n",
      "Epoch 64, loss = 0.0038, time: 16.5559\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1097\n",
      "Epoch 65, loss = 0.0036, time: 16.5638\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1029\n",
      "Epoch 66, loss = 0.0038, time: 16.5752\n",
      "reconstruction loss = 0.0038, similarity loss: 1.0584\n",
      "Epoch 67, loss = 0.0037, time: 16.8276\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0994\n",
      "Epoch 68, loss = 0.0037, time: 16.5699\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1383\n",
      "Epoch 69, loss = 0.0035, time: 16.6600\n",
      "reconstruction loss = 0.0035, similarity loss: 1.0593\n",
      "Epoch 70, loss = 0.0038, time: 16.6195\n",
      "reconstruction loss = 0.0038, similarity loss: 1.0275\n",
      "Epoch 71, loss = 0.0040, time: 16.5165\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1522\n",
      "Epoch 72, loss = 0.0036, time: 16.5555\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1205\n",
      "Epoch 73, loss = 0.0037, time: 16.5611\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1379\n",
      "Epoch 74, loss = 0.0038, time: 16.6330\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2301\n",
      "Epoch 75, loss = 0.0041, time: 16.6807\n",
      "reconstruction loss = 0.0041, similarity loss: 1.1025\n",
      "Epoch 76, loss = 0.0040, time: 16.7911\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0436\n",
      "Epoch 77, loss = 0.0037, time: 16.6154\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0566\n",
      "Epoch 78, loss = 0.0038, time: 16.8532\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1782\n",
      "Epoch 79, loss = 0.0040, time: 16.6175\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1580\n",
      "Epoch 80, loss = 0.0040, time: 16.6967\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1175\n",
      "Epoch 81, loss = 0.0036, time: 16.8126\n",
      "reconstruction loss = 0.0036, similarity loss: 1.0979\n",
      "Epoch 82, loss = 0.0037, time: 17.0964\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0833\n",
      "Epoch 83, loss = 0.0038, time: 17.3741\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1734\n",
      "Epoch 84, loss = 0.0037, time: 17.3452\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1646\n",
      "Epoch 85, loss = 0.0037, time: 17.3433\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1346\n",
      "Epoch 86, loss = 0.0037, time: 17.1420\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2060\n",
      "Epoch 87, loss = 0.0039, time: 17.3531\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1889\n",
      "Epoch 88, loss = 0.0036, time: 17.2587\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1961\n",
      "Epoch 89, loss = 0.0036, time: 17.3026\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1445\n",
      "Epoch 90, loss = 0.0036, time: 17.6332\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2055\n",
      "Epoch 91, loss = 0.0037, time: 17.4011\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0905\n",
      "Epoch 92, loss = 0.0040, time: 17.1148\n",
      "reconstruction loss = 0.0040, similarity loss: 1.0610\n",
      "Epoch 93, loss = 0.0033, time: 17.4349\n",
      "reconstruction loss = 0.0033, similarity loss: 1.2234\n",
      "Epoch 94, loss = 0.0036, time: 17.4044\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1948\n",
      "Epoch 95, loss = 0.0036, time: 17.3059\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1894\n",
      "Epoch 96, loss = 0.0038, time: 17.5998\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1020\n",
      "Epoch 97, loss = 0.0035, time: 17.4901\n",
      "reconstruction loss = 0.0035, similarity loss: 1.0895\n",
      "Epoch 98, loss = 0.0037, time: 17.3884\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1447\n",
      "Epoch 99, loss = 0.0034, time: 17.3429\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2346\n",
      "Epoch 100, loss = 0.0036, time: 17.4181\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1059\n",
      "Epoch 101, loss = 0.0036, time: 17.4404\n",
      "reconstruction loss = 0.0036, similarity loss: 1.0805\n",
      "Epoch 102, loss = 0.0037, time: 17.3137\n",
      "reconstruction loss = 0.0037, similarity loss: 1.0883\n",
      "Epoch 103, loss = 0.0034, time: 17.3434\n",
      "reconstruction loss = 0.0034, similarity loss: 1.1280\n",
      "Epoch 104, loss = 0.0035, time: 17.4071\n",
      "reconstruction loss = 0.0035, similarity loss: 1.1267\n",
      "Epoch 105, loss = 0.0040, time: 17.3123\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1568\n",
      "Epoch 106, loss = 0.0037, time: 17.4256\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2022\n",
      "Epoch 107, loss = 0.0039, time: 17.2939\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1391\n",
      "Epoch 108, loss = 0.0034, time: 17.2352\n",
      "reconstruction loss = 0.0034, similarity loss: 1.1622\n",
      "Epoch 109, loss = 0.0034, time: 17.2395\n",
      "reconstruction loss = 0.0034, similarity loss: 1.1698\n",
      "Epoch 110, loss = 0.0046, time: 17.2173\n",
      "reconstruction loss = 0.0046, similarity loss: 1.0852\n",
      "Epoch 111, loss = 0.0038, time: 17.2392\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2197\n",
      "Epoch 112, loss = 0.0037, time: 17.4849\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1746\n",
      "Epoch 113, loss = 0.0033, time: 17.2104\n",
      "reconstruction loss = 0.0033, similarity loss: 1.0487\n",
      "Epoch 114, loss = 0.0035, time: 17.3229\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2119\n",
      "Epoch 115, loss = 0.0035, time: 17.4534\n",
      "reconstruction loss = 0.0035, similarity loss: 1.1720\n",
      "Epoch 116, loss = 0.0037, time: 16.9948\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1198\n",
      "Epoch 117, loss = 0.0042, time: 16.6169\n",
      "reconstruction loss = 0.0042, similarity loss: 1.2035\n",
      "Epoch 118, loss = 0.0036, time: 16.6386\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2136\n",
      "Epoch 119, loss = 0.0038, time: 16.6670\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2297\n",
      "Epoch 120, loss = 0.0034, time: 16.8368\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2574\n",
      "Epoch 121, loss = 0.0039, time: 16.8816\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1481\n",
      "Epoch 122, loss = 0.0036, time: 16.7247\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2304\n",
      "Epoch 123, loss = 0.0031, time: 16.6916\n",
      "reconstruction loss = 0.0031, similarity loss: 1.1902\n",
      "Epoch 124, loss = 0.0038, time: 16.5733\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1076\n",
      "Epoch 125, loss = 0.0037, time: 16.7495\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1483\n",
      "Epoch 126, loss = 0.0039, time: 16.5555\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1060\n",
      "Epoch 127, loss = 0.0037, time: 16.6306\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2343\n",
      "Epoch 128, loss = 0.0037, time: 16.5619\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2450\n",
      "Epoch 129, loss = 0.0035, time: 16.7729\n",
      "reconstruction loss = 0.0035, similarity loss: 1.1039\n",
      "Epoch 130, loss = 0.0037, time: 16.8111\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1152\n",
      "Epoch 131, loss = 0.0032, time: 16.7006\n",
      "reconstruction loss = 0.0032, similarity loss: 1.2358\n",
      "Epoch 132, loss = 0.0037, time: 16.6393\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1108\n",
      "Epoch 133, loss = 0.0036, time: 16.6129\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1932\n",
      "Epoch 134, loss = 0.0034, time: 17.4842\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3123\n",
      "Epoch 135, loss = 0.0039, time: 17.2675\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1857\n",
      "Epoch 136, loss = 0.0036, time: 17.3468\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1413\n",
      "Epoch 137, loss = 0.0038, time: 17.3178\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2057\n",
      "Epoch 138, loss = 0.0036, time: 17.2828\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1520\n",
      "Epoch 139, loss = 0.0040, time: 17.3178\n",
      "reconstruction loss = 0.0040, similarity loss: 1.2108\n",
      "Epoch 140, loss = 0.0034, time: 17.3662\n",
      "reconstruction loss = 0.0034, similarity loss: 1.1695\n",
      "Epoch 141, loss = 0.0036, time: 17.2915\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2047\n",
      "Epoch 142, loss = 0.0037, time: 17.1946\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2061\n",
      "Epoch 143, loss = 0.0040, time: 17.2801\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1698\n",
      "Epoch 144, loss = 0.0040, time: 17.4963\n",
      "reconstruction loss = 0.0040, similarity loss: 1.1630\n",
      "Epoch 145, loss = 0.0037, time: 17.1734\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1271\n",
      "Epoch 146, loss = 0.0037, time: 17.3930\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2391\n",
      "Epoch 147, loss = 0.0037, time: 17.2587\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2472\n",
      "Epoch 148, loss = 0.0038, time: 17.3216\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2243\n",
      "Epoch 149, loss = 0.0034, time: 17.3944\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2701\n",
      "Epoch 150, loss = 0.0039, time: 17.3273\n",
      "reconstruction loss = 0.0039, similarity loss: 1.1295\n",
      "Epoch 151, loss = 0.0037, time: 17.2075\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2553\n",
      "Epoch 152, loss = 0.0037, time: 17.2827\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1726\n",
      "Epoch 153, loss = 0.0038, time: 17.3811\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2799\n",
      "Epoch 154, loss = 0.0040, time: 17.3231\n",
      "reconstruction loss = 0.0040, similarity loss: 1.2459\n",
      "Epoch 155, loss = 0.0036, time: 17.3713\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2770\n",
      "Epoch 156, loss = 0.0036, time: 17.1902\n",
      "reconstruction loss = 0.0036, similarity loss: 1.1627\n",
      "Epoch 157, loss = 0.0036, time: 17.3804\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2095\n",
      "Epoch 158, loss = 0.0036, time: 17.2428\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3131\n",
      "Epoch 159, loss = 0.0035, time: 17.2271\n",
      "reconstruction loss = 0.0035, similarity loss: 1.1742\n",
      "Epoch 160, loss = 0.0034, time: 17.2970\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2112\n",
      "Epoch 161, loss = 0.0036, time: 17.3238\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2484\n",
      "Epoch 162, loss = 0.0034, time: 17.3929\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3145\n",
      "Epoch 163, loss = 0.0036, time: 17.2723\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2748\n",
      "Epoch 164, loss = 0.0038, time: 17.2185\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1556\n",
      "Epoch 165, loss = 0.0037, time: 17.3426\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3093\n",
      "Epoch 166, loss = 0.0038, time: 17.2105\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2106\n",
      "Epoch 167, loss = 0.0037, time: 17.1981\n",
      "reconstruction loss = 0.0037, similarity loss: 1.1805\n",
      "Epoch 168, loss = 0.0042, time: 17.3709\n",
      "reconstruction loss = 0.0042, similarity loss: 1.2235\n",
      "Epoch 169, loss = 0.0035, time: 17.3476\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2833\n",
      "Epoch 170, loss = 0.0037, time: 17.3722\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2926\n",
      "Epoch 171, loss = 0.0037, time: 17.2979\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2014\n",
      "Epoch 172, loss = 0.0034, time: 17.3250\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2299\n",
      "Epoch 173, loss = 0.0036, time: 17.3205\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2479\n",
      "Epoch 174, loss = 0.0038, time: 17.3238\n",
      "reconstruction loss = 0.0038, similarity loss: 1.1420\n",
      "Epoch 175, loss = 0.0036, time: 17.3518\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2259\n",
      "Epoch 176, loss = 0.0039, time: 17.4253\n",
      "reconstruction loss = 0.0039, similarity loss: 1.2474\n",
      "Epoch 177, loss = 0.0035, time: 17.2614\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2314\n",
      "Epoch 178, loss = 0.0037, time: 17.3144\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2001\n",
      "Epoch 179, loss = 0.0040, time: 17.3794\n",
      "reconstruction loss = 0.0040, similarity loss: 1.3198\n",
      "Epoch 180, loss = 0.0035, time: 17.2921\n",
      "reconstruction loss = 0.0035, similarity loss: 1.1645\n",
      "Epoch 181, loss = 0.0035, time: 17.5636\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2862\n",
      "Epoch 182, loss = 0.0037, time: 17.3583\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2279\n",
      "Epoch 183, loss = 0.0035, time: 17.4451\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2120\n",
      "Epoch 184, loss = 0.0037, time: 17.6441\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2808\n",
      "Epoch 185, loss = 0.0034, time: 17.4214\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2193\n",
      "Epoch 186, loss = 0.0035, time: 17.5955\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2411\n",
      "Epoch 187, loss = 0.0036, time: 17.4794\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2812\n",
      "Epoch 188, loss = 0.0036, time: 17.1770\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2572\n",
      "Epoch 189, loss = 0.0034, time: 16.6666\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2923\n",
      "Epoch 190, loss = 0.0034, time: 17.1629\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2254\n",
      "Epoch 191, loss = 0.0035, time: 17.1599\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2446\n",
      "Epoch 192, loss = 0.0035, time: 17.1744\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3651\n",
      "Epoch 193, loss = 0.0037, time: 17.1509\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3126\n",
      "Epoch 194, loss = 0.0035, time: 16.7597\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2922\n",
      "Epoch 195, loss = 0.0038, time: 16.5628\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2378\n",
      "Epoch 196, loss = 0.0038, time: 16.6632\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2813\n",
      "Epoch 197, loss = 0.0036, time: 16.6931\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2438\n",
      "Epoch 198, loss = 0.0035, time: 16.6304\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2180\n",
      "Epoch 199, loss = 0.0039, time: 16.6011\n",
      "reconstruction loss = 0.0039, similarity loss: 1.2538\n",
      "Epoch 200, loss = 0.0038, time: 16.7102\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3156\n",
      "Epoch 201, loss = 0.0034, time: 16.6571\n",
      "reconstruction loss = 0.0034, similarity loss: 1.4191\n",
      "Epoch 202, loss = 0.0037, time: 16.6488\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3060\n",
      "Epoch 203, loss = 0.0033, time: 16.7204\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3118\n",
      "Epoch 204, loss = 0.0036, time: 16.5966\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3338\n",
      "Epoch 205, loss = 0.0036, time: 16.4782\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3345\n",
      "Epoch 206, loss = 0.0032, time: 16.7222\n",
      "reconstruction loss = 0.0032, similarity loss: 1.3365\n",
      "Epoch 207, loss = 0.0034, time: 16.7514\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2642\n",
      "Epoch 208, loss = 0.0035, time: 16.8113\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3649\n",
      "Epoch 209, loss = 0.0038, time: 16.7637\n",
      "reconstruction loss = 0.0038, similarity loss: 1.2671\n",
      "Epoch 210, loss = 0.0037, time: 16.8065\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3826\n",
      "Epoch 211, loss = 0.0038, time: 16.6597\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3681\n",
      "Epoch 212, loss = 0.0035, time: 16.6468\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2801\n",
      "Epoch 213, loss = 0.0034, time: 16.6929\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2866\n",
      "Epoch 214, loss = 0.0035, time: 16.5663\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3672\n",
      "Epoch 215, loss = 0.0039, time: 16.6710\n",
      "reconstruction loss = 0.0039, similarity loss: 1.2608\n",
      "Epoch 216, loss = 0.0035, time: 16.6376\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2351\n",
      "Epoch 217, loss = 0.0039, time: 16.5244\n",
      "reconstruction loss = 0.0039, similarity loss: 1.2657\n",
      "Epoch 218, loss = 0.0036, time: 16.7970\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2855\n",
      "Epoch 219, loss = 0.0035, time: 16.7281\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3570\n",
      "Epoch 220, loss = 0.0040, time: 16.6915\n",
      "reconstruction loss = 0.0040, similarity loss: 1.2876\n",
      "Epoch 221, loss = 0.0037, time: 16.7098\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3222\n",
      "Epoch 222, loss = 0.0036, time: 16.6470\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3971\n",
      "Epoch 223, loss = 0.0036, time: 16.6081\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2607\n",
      "Epoch 224, loss = 0.0036, time: 16.7468\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3991\n",
      "Epoch 225, loss = 0.0036, time: 16.5846\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2904\n",
      "Epoch 226, loss = 0.0035, time: 16.6560\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2736\n",
      "Epoch 227, loss = 0.0034, time: 16.7852\n",
      "reconstruction loss = 0.0034, similarity loss: 1.4523\n",
      "Epoch 228, loss = 0.0035, time: 16.7614\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2462\n",
      "Epoch 229, loss = 0.0036, time: 16.6267\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3393\n",
      "Epoch 230, loss = 0.0038, time: 16.6803\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3428\n",
      "Epoch 231, loss = 0.0036, time: 16.7499\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3990\n",
      "Epoch 232, loss = 0.0038, time: 16.6081\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3711\n",
      "Epoch 233, loss = 0.0035, time: 16.6632\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3518\n",
      "Epoch 234, loss = 0.0037, time: 16.6730\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3175\n",
      "Epoch 235, loss = 0.0036, time: 16.7675\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2397\n",
      "Epoch 236, loss = 0.0041, time: 16.5984\n",
      "reconstruction loss = 0.0041, similarity loss: 1.2446\n",
      "Epoch 237, loss = 0.0035, time: 16.7894\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3794\n",
      "Epoch 238, loss = 0.0037, time: 16.6997\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2671\n",
      "Epoch 239, loss = 0.0037, time: 16.5250\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2839\n",
      "Epoch 240, loss = 0.0035, time: 17.2803\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3216\n",
      "Epoch 241, loss = 0.0035, time: 17.2977\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3686\n",
      "Epoch 242, loss = 0.0037, time: 17.2965\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3149\n",
      "Epoch 243, loss = 0.0037, time: 17.3324\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2954\n",
      "Epoch 244, loss = 0.0035, time: 17.3213\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3634\n",
      "Epoch 245, loss = 0.0035, time: 17.3192\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3887\n",
      "Epoch 246, loss = 0.0037, time: 17.1966\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3027\n",
      "Epoch 247, loss = 0.0037, time: 17.3337\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3206\n",
      "Epoch 248, loss = 0.0036, time: 17.3323\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3903\n",
      "Epoch 249, loss = 0.0037, time: 17.3981\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3771\n",
      "Epoch 250, loss = 0.0033, time: 17.3938\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3943\n",
      "Epoch 251, loss = 0.0036, time: 17.2952\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3902\n",
      "Epoch 252, loss = 0.0035, time: 17.2592\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2716\n",
      "Epoch 253, loss = 0.0034, time: 17.1635\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3018\n",
      "Epoch 254, loss = 0.0034, time: 17.2826\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3717\n",
      "Epoch 255, loss = 0.0037, time: 17.2165\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3164\n",
      "Epoch 256, loss = 0.0037, time: 17.3641\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2901\n",
      "Epoch 257, loss = 0.0039, time: 17.2926\n",
      "reconstruction loss = 0.0039, similarity loss: 1.3268\n",
      "Epoch 258, loss = 0.0032, time: 17.1650\n",
      "reconstruction loss = 0.0032, similarity loss: 1.3215\n",
      "Epoch 259, loss = 0.0034, time: 17.3045\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3621\n",
      "Epoch 260, loss = 0.0031, time: 17.3819\n",
      "reconstruction loss = 0.0031, similarity loss: 1.3604\n",
      "Epoch 261, loss = 0.0033, time: 18.0270\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3447\n",
      "Epoch 262, loss = 0.0038, time: 17.3767\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3636\n",
      "Epoch 263, loss = 0.0037, time: 17.3541\n",
      "reconstruction loss = 0.0037, similarity loss: 1.2807\n",
      "Epoch 264, loss = 0.0035, time: 17.4095\n",
      "reconstruction loss = 0.0035, similarity loss: 1.2998\n",
      "Epoch 265, loss = 0.0035, time: 17.3326\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3442\n",
      "Epoch 266, loss = 0.0034, time: 17.4178\n",
      "reconstruction loss = 0.0034, similarity loss: 1.4231\n",
      "Epoch 267, loss = 0.0034, time: 17.2697\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2099\n",
      "Epoch 268, loss = 0.0033, time: 17.3100\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3842\n",
      "Epoch 269, loss = 0.0035, time: 17.3194\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3219\n",
      "Epoch 270, loss = 0.0036, time: 17.4639\n",
      "reconstruction loss = 0.0036, similarity loss: 1.2327\n",
      "Epoch 271, loss = 0.0037, time: 17.5459\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3855\n",
      "Epoch 272, loss = 0.0034, time: 17.3237\n",
      "reconstruction loss = 0.0034, similarity loss: 1.4086\n",
      "Epoch 273, loss = 0.0037, time: 17.4210\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3760\n",
      "Epoch 274, loss = 0.0037, time: 17.1296\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3270\n",
      "Epoch 275, loss = 0.0033, time: 16.7827\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3462\n",
      "Epoch 276, loss = 0.0036, time: 16.6573\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3172\n",
      "Epoch 277, loss = 0.0033, time: 16.6083\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3290\n",
      "Epoch 278, loss = 0.0035, time: 16.7240\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3746\n",
      "Epoch 279, loss = 0.0034, time: 16.6286\n",
      "reconstruction loss = 0.0034, similarity loss: 1.3220\n",
      "Epoch 280, loss = 0.0032, time: 16.7674\n",
      "reconstruction loss = 0.0032, similarity loss: 1.3815\n",
      "Epoch 281, loss = 0.0036, time: 16.6417\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3344\n",
      "Epoch 282, loss = 0.0036, time: 16.7165\n",
      "reconstruction loss = 0.0036, similarity loss: 1.4028\n",
      "Epoch 283, loss = 0.0036, time: 16.5949\n",
      "reconstruction loss = 0.0036, similarity loss: 1.3781\n",
      "Epoch 284, loss = 0.0035, time: 16.7245\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3339\n",
      "Epoch 285, loss = 0.0038, time: 16.6951\n",
      "reconstruction loss = 0.0038, similarity loss: 1.3108\n",
      "Epoch 286, loss = 0.0035, time: 16.6296\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3447\n",
      "Epoch 287, loss = 0.0035, time: 16.7320\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3507\n",
      "Epoch 288, loss = 0.0035, time: 16.6489\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3471\n",
      "Epoch 289, loss = 0.0040, time: 16.4617\n",
      "reconstruction loss = 0.0040, similarity loss: 1.3025\n",
      "Epoch 290, loss = 0.0037, time: 16.5905\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3357\n",
      "Epoch 291, loss = 0.0034, time: 16.5447\n",
      "reconstruction loss = 0.0034, similarity loss: 1.2936\n",
      "Epoch 292, loss = 0.0033, time: 16.7450\n",
      "reconstruction loss = 0.0033, similarity loss: 1.3838\n",
      "Epoch 293, loss = 0.0037, time: 17.3570\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3386\n",
      "Epoch 294, loss = 0.0035, time: 17.4441\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3780\n",
      "Epoch 295, loss = 0.0037, time: 17.2916\n",
      "reconstruction loss = 0.0037, similarity loss: 1.4055\n",
      "Epoch 296, loss = 0.0035, time: 17.2126\n",
      "reconstruction loss = 0.0035, similarity loss: 1.4395\n",
      "Epoch 297, loss = 0.0037, time: 17.1509\n",
      "reconstruction loss = 0.0037, similarity loss: 1.3111\n",
      "Epoch 298, loss = 0.0032, time: 17.3768\n",
      "reconstruction loss = 0.0032, similarity loss: 1.3981\n",
      "Epoch 299, loss = 0.0035, time: 17.1779\n",
      "reconstruction loss = 0.0035, similarity loss: 1.3247\n",
      "Train Classifier\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 1.7704, val.acc = 0.4608\n",
      "Epoch 1, loss = 1.5195, val.acc = 0.5006\n",
      "Epoch 2, loss = 1.4289, val.acc = 0.5214\n",
      "Epoch 3, loss = 1.3717, val.acc = 0.5360\n",
      "Epoch 4, loss = 1.3300, val.acc = 0.5456\n",
      "Epoch 5, loss = 1.2974, val.acc = 0.5564\n",
      "Epoch 6, loss = 1.2705, val.acc = 0.5624\n",
      "Epoch 7, loss = 1.2476, val.acc = 0.5674\n",
      "Epoch 8, loss = 1.2277, val.acc = 0.5718\n",
      "Epoch 9, loss = 1.2100, val.acc = 0.5766\n",
      "Epoch 10, loss = 1.1942, val.acc = 0.5794\n",
      "Epoch 11, loss = 1.1797, val.acc = 0.5826\n",
      "Epoch 12, loss = 1.1665, val.acc = 0.5850\n",
      "Epoch 13, loss = 1.1542, val.acc = 0.5882\n",
      "Epoch 14, loss = 1.1428, val.acc = 0.5894\n",
      "Epoch 15, loss = 1.1322, val.acc = 0.5928\n",
      "Epoch 16, loss = 1.1221, val.acc = 0.5954\n",
      "Epoch 17, loss = 1.1127, val.acc = 0.5962\n",
      "Epoch 18, loss = 1.1037, val.acc = 0.5998\n",
      "Epoch 19, loss = 1.0952, val.acc = 0.6010\n",
      "Epoch 20, loss = 1.0871, val.acc = 0.6026\n",
      "Epoch 21, loss = 1.0793, val.acc = 0.6042\n",
      "Epoch 22, loss = 1.0719, val.acc = 0.6062\n",
      "Epoch 23, loss = 1.0647, val.acc = 0.6082\n",
      "Epoch 24, loss = 1.0579, val.acc = 0.6102\n",
      "Epoch 25, loss = 1.0513, val.acc = 0.6110\n",
      "Epoch 26, loss = 1.0449, val.acc = 0.6122\n",
      "Epoch 27, loss = 1.0387, val.acc = 0.6122\n",
      "Epoch 28, loss = 1.0328, val.acc = 0.6134\n",
      "Epoch 29, loss = 1.0270, val.acc = 0.6140\n",
      "Epoch 30, loss = 1.0214, val.acc = 0.6144\n",
      "Epoch 31, loss = 1.0159, val.acc = 0.6166\n",
      "Epoch 32, loss = 1.0106, val.acc = 0.6166\n",
      "Epoch 33, loss = 1.0055, val.acc = 0.6172\n",
      "Epoch 34, loss = 1.0004, val.acc = 0.6186\n",
      "Epoch 35, loss = 0.9956, val.acc = 0.6180\n",
      "Epoch 36, loss = 0.9908, val.acc = 0.6184\n",
      "Epoch 37, loss = 0.9861, val.acc = 0.6194\n",
      "Epoch 38, loss = 0.9816, val.acc = 0.6202\n",
      "Epoch 39, loss = 0.9771, val.acc = 0.6198\n",
      "Epoch 40, loss = 0.9728, val.acc = 0.6206\n",
      "Epoch 41, loss = 0.9685, val.acc = 0.6204\n",
      "Epoch 42, loss = 0.9644, val.acc = 0.6212\n",
      "Epoch 43, loss = 0.9603, val.acc = 0.6216\n",
      "Epoch 44, loss = 0.9563, val.acc = 0.6222\n",
      "Epoch 45, loss = 0.9523, val.acc = 0.6226\n",
      "Epoch 46, loss = 0.9485, val.acc = 0.6230\n",
      "Epoch 47, loss = 0.9447, val.acc = 0.6236\n",
      "Epoch 48, loss = 0.9410, val.acc = 0.6242\n",
      "Epoch 49, loss = 0.9373, val.acc = 0.6246\n",
      "Epoch 50, loss = 0.9338, val.acc = 0.6250\n",
      "Epoch 51, loss = 0.9302, val.acc = 0.6244\n",
      "Epoch 52, loss = 0.9268, val.acc = 0.6244\n",
      "Epoch 53, loss = 0.9234, val.acc = 0.6238\n",
      "Epoch 54, loss = 0.9200, val.acc = 0.6244\n",
      "Epoch 55, loss = 0.9167, val.acc = 0.6252\n",
      "Epoch 56, loss = 0.9135, val.acc = 0.6252\n",
      "Epoch 57, loss = 0.9103, val.acc = 0.6260\n",
      "Epoch 58, loss = 0.9071, val.acc = 0.6262\n",
      "Epoch 59, loss = 0.9040, val.acc = 0.6260\n",
      "Epoch 60, loss = 0.9009, val.acc = 0.6266\n",
      "Epoch 61, loss = 0.8979, val.acc = 0.6266\n",
      "Epoch 62, loss = 0.8949, val.acc = 0.6264\n",
      "Epoch 63, loss = 0.8920, val.acc = 0.6264\n",
      "Epoch 64, loss = 0.8891, val.acc = 0.6270\n",
      "Epoch 65, loss = 0.8863, val.acc = 0.6276\n",
      "Epoch 66, loss = 0.8834, val.acc = 0.6280\n",
      "Epoch 67, loss = 0.8807, val.acc = 0.6286\n",
      "Epoch 68, loss = 0.8779, val.acc = 0.6288\n",
      "Epoch 69, loss = 0.8752, val.acc = 0.6288\n",
      "Epoch 70, loss = 0.8725, val.acc = 0.6288\n",
      "Epoch 71, loss = 0.8699, val.acc = 0.6290\n",
      "Epoch 72, loss = 0.8672, val.acc = 0.6288\n",
      "Epoch 73, loss = 0.8647, val.acc = 0.6292\n",
      "Epoch 74, loss = 0.8621, val.acc = 0.6290\n",
      "Epoch 75, loss = 0.8596, val.acc = 0.6292\n",
      "Epoch 76, loss = 0.8571, val.acc = 0.6288\n",
      "Epoch 77, loss = 0.8546, val.acc = 0.6292\n",
      "Epoch 78, loss = 0.8522, val.acc = 0.6302\n",
      "Epoch 79, loss = 0.8498, val.acc = 0.6302\n",
      "Epoch 80, loss = 0.8474, val.acc = 0.6310\n",
      "Epoch 81, loss = 0.8450, val.acc = 0.6304\n",
      "Epoch 82, loss = 0.8427, val.acc = 0.6308\n",
      "Epoch 83, loss = 0.8404, val.acc = 0.6312\n",
      "Epoch 84, loss = 0.8381, val.acc = 0.6314\n",
      "Epoch 85, loss = 0.8358, val.acc = 0.6316\n",
      "Epoch 86, loss = 0.8336, val.acc = 0.6322\n",
      "Epoch 87, loss = 0.8313, val.acc = 0.6322\n",
      "Epoch 88, loss = 0.8291, val.acc = 0.6322\n",
      "Epoch 89, loss = 0.8270, val.acc = 0.6326\n",
      "Epoch 90, loss = 0.8248, val.acc = 0.6334\n",
      "Epoch 91, loss = 0.8227, val.acc = 0.6332\n",
      "Epoch 92, loss = 0.8206, val.acc = 0.6336\n",
      "Epoch 93, loss = 0.8185, val.acc = 0.6328\n",
      "Epoch 94, loss = 0.8164, val.acc = 0.6330\n",
      "Epoch 95, loss = 0.8143, val.acc = 0.6330\n",
      "Epoch 96, loss = 0.8123, val.acc = 0.6336\n",
      "Epoch 97, loss = 0.8103, val.acc = 0.6338\n",
      "Epoch 98, loss = 0.8083, val.acc = 0.6338\n",
      "Epoch 99, loss = 0.8063, val.acc = 0.6342\n",
      "Rep: 1, te.acc = 0.6213\n",
      "\n",
      "All reps test.acc:\n",
      "[0.6213]\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_1')\n",
    "train_unsupervised_ae(pars,vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0005\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.0005\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0005_Epochs_300_CLF_Cifar10_Adam_LR_0.0005_Epochs_100_lam_0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.4773, time: 17.7552\n",
      "reconstruction loss = 0.1189, similarity loss: 0.4773\n",
      "Epoch 1, loss = 0.5853, time: 17.9633\n",
      "reconstruction loss = 0.1104, similarity loss: 0.5853\n",
      "Epoch 2, loss = 0.6260, time: 17.4745\n",
      "reconstruction loss = 0.2080, similarity loss: 0.6260\n",
      "Epoch 3, loss = 0.5938, time: 17.4455\n",
      "reconstruction loss = 0.3094, similarity loss: 0.5938\n",
      "Epoch 4, loss = 0.5153, time: 17.3215\n",
      "reconstruction loss = 0.3120, similarity loss: 0.5153\n",
      "Epoch 5, loss = 0.6587, time: 17.4755\n",
      "reconstruction loss = 0.3112, similarity loss: 0.6587\n",
      "Epoch 6, loss = 0.5721, time: 17.1480\n",
      "reconstruction loss = 0.2992, similarity loss: 0.5721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_62960/1791833225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8097\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lam_0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_unsupervised_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_unsupervised_ae\u001b[1;34m(pars, criterion_re, criterion_sim, clf_criterion, optimizer, vis)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[0mpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         \u001b[0mtrain_model_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_sim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Classifier'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mtrain_model_ae\u001b[1;34m(data, fix, model, decoder, pars, ep_loss, criterion_re, criterion_sim, optimizer, vis)\u001b[0m\n\u001b[0;32m    246\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m                 \u001b[0mtransform1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_BYOL_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m                 \u001b[0mtransform2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_BYOL_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UCHI\\unsupervised\\utils.py\u001b[0m in \u001b[0;36mget_BYOL_transforms\u001b[1;34m(is_first)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mTF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomSolarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_first\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     )\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mscripted_transforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscripted_transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_script.py\u001b[0m in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[0;32m   1258\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         )\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m     \u001b[0mscript_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;31m# Compile methods if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_script.py\u001b[0m in \u001b[0;36m_construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \"\"\"\n\u001b[0;32m    586\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[0minit_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[1;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m                 \u001b[0mscripted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mcpp_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscripted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \"\"\"\n\u001b[0;32m    462\u001b[0m     \u001b[0mcpp_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_module_with_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcrete_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m     \u001b[0mmethod_stubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m     \u001b[0mproperty_stubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_property_stubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[0mhook_stubs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_hook_stubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hook_stubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36minfer_methods_to_compile\u001b[1;34m(nn_module)\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[0mstubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muniqued_methods\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[0mstubs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_stub_from_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moverload_stubs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstubs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mmake_stub_from_method\u001b[1;34m(nn_module, method_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# In this case, the actual function object will have the name `_forward`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# even though we requested a stub for `forward`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmake_stub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py\u001b[0m in \u001b[0;36mmake_stub\u001b[1;34m(func, name)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_stub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mrcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_jit_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"RecursiveScriptModule\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mScriptMethodStub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrcb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\frontend.py\u001b[0m in \u001b[0;36mget_jit_def\u001b[1;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mself_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \"\"\"\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[0mparsed_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[0mtype_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_type_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[0mfn_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_sources.py\u001b[0m in \u001b[0;36mparse_def\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0msourcelines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_lineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_source_lines_and_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mErrorReport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0msourcelines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_source_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourcelines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourcelines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_sources.py\u001b[0m in \u001b[0;36mget_source_lines_and_file\u001b[1;34m(obj, error_msg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# in case getsourcefile throws\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0msourcelines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_lineno\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsourcelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    704\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n\u001b[0;32m    705\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_0')\n",
    "train_unsupervised_ae(pars,vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.9\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.5\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lam_0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.7050, time: 25.1046\n",
      "reconstruction loss = 0.0685, similarity loss: 0.7050\n",
      "Epoch 1, loss = 0.6958, time: 18.0448\n",
      "reconstruction loss = 0.0700, similarity loss: 0.6958\n",
      "Epoch 2, loss = 0.6823, time: 17.8828\n",
      "reconstruction loss = 0.0725, similarity loss: 0.6823\n",
      "Epoch 3, loss = 0.7897, time: 18.3050\n",
      "reconstruction loss = 0.0745, similarity loss: 0.7897\n",
      "Epoch 4, loss = 0.5174, time: 17.5966\n",
      "reconstruction loss = 0.0694, similarity loss: 0.5174\n",
      "Epoch 5, loss = 0.4368, time: 17.1870\n",
      "reconstruction loss = 0.0692, similarity loss: 0.4368\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_0_5')\n",
    "train_unsupervised_ae(pars,vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: CONV6\n",
      "nonlinear: hardtanh\n",
      "batch_size: 500\n",
      "headsize: 64\n",
      "dataset: Cifar100\n",
      "loss: SimCLR\n",
      "OPT: Adam\n",
      "LR: 0.0001\n",
      "epochs: 300\n",
      "clf_dataset: Cifar10\n",
      "clf_loss: CE\n",
      "clf_opt: Adam\n",
      "clf_lr: 0.001\n",
      "clf_epochs: 100\n",
      "repeat: 1\n",
      "device: cuda:0\n",
      "datapath: data/\n",
      "savepath: save/\n",
      "loadnet: None\n",
      "loadclf: None\n",
      "lam: 0.9\n",
      "auxnonlinear: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 5e-4\n",
    "pars.clf_lr = 5e-4\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.lam = 0.75\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/CONV6/AE/\n",
      "hardtanh_Cifar100_Adam_LR_0.0001_Epochs_300_CLF_Cifar10_Adam_LR_0.001_Epochs_100_lam_0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Rep 1\n",
      "Sequential(\n",
      "  (layer0): Sequential(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (aux): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxhead): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Sequential()\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (layer0): Sequential(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (conv): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (auxhead): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (auxdecoder): Sequential(\n",
      "    (fc): Linear(in_features=64, out_features=8192, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(8, 32, 32))\n",
      "    (deconv): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "TwinMSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0, loss = 0.7050, time: 25.1046\n",
      "reconstruction loss = 0.0685, similarity loss: 0.7050\n",
      "Epoch 1, loss = 0.6958, time: 18.0448\n",
      "reconstruction loss = 0.0700, similarity loss: 0.6958\n",
      "Epoch 2, loss = 0.6823, time: 17.8828\n",
      "reconstruction loss = 0.0725, similarity loss: 0.6823\n",
      "Epoch 3, loss = 0.7897, time: 18.3050\n",
      "reconstruction loss = 0.0745, similarity loss: 0.7897\n",
      "Epoch 4, loss = 0.5174, time: 17.5966\n",
      "reconstruction loss = 0.0694, similarity loss: 0.5174\n",
      "Epoch 5, loss = 0.4368, time: 17.1870\n",
      "reconstruction loss = 0.0692, similarity loss: 0.4368\n"
     ]
    }
   ],
   "source": [
    "vis = visdom.Visdom(port=8097,env='lam_0_75')\n",
    "train_unsupervised_ae(pars,vis=vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average reconstruction loss\n",
    "- reconstruction result\n",
    "- zero weight on BT\n",
    "- lambda from small to large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = PARS(device, datapath, savepath)\n",
    "pars.architecture = 'CONV6'\n",
    "pars.LR = 0.0001\n",
    "pars.clf_lr = 0.001\n",
    "pars.epochs = 300\n",
    "pars.clf_epochs = 100\n",
    "pars.nonlinear = 'hardtanh'\n",
    "pars.repeat = 1\n",
    "pars.loss = \"BarlowTwins\"\n",
    "pars.lam = 0.5\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BarlowTwinsLoss(pars.batch_size, pars.lam, pars.device)\n",
    "train_unsupervised(pars, criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd034ed54560f0698c2946b7ca675e493afbd7ee3c0ecf162ae3deac3cf4477b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
